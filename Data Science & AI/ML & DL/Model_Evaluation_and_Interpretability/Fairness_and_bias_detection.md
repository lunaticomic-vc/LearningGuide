# Fairness_and_bias_detection

## 1. Activate Prior Knowledge

- Какво разбирате под „справедливост“ в контекста на изкуствения интелект (ИИ) и софтуерните системи?
- Можете ли да дадете пример за ситуация, в която алгоритъм може да прояви пристрастие (bias)?
- Как мислите, защо е важно да откриваме и коригираме пристрастия в ИИ системите?

## 2. Overview

Fairness (справедливостта) и bias detection (откриването на пристрастия) са ключови аспекти в разработката и внедряването на изкуствени интелектуални системи. Целта им е да гарантират, че алгоритмите вземат решения, които не дискриминират или не вредят на определени групи хора. Това е особено важно в приложения като кредитиране, подбор на персонал, здравеопазване и правосъдие, където неправилните решения могат да имат сериозни социални последици.

В по-широк контекст, fairness и bias detection са част от етичната рамка на софтуерната инженерия и машинното обучение. Те помагат да се осигури прозрачност и доверие в автоматизираните системи, като същевременно поддържат законови и морални стандарти. Без тези механизми, ИИ системите могат да възпроизвеждат или дори усилват съществуващи социални неравенства.

Процесът на откриване на пристрастия включва идентифициране на източници на несправедливост в данните, алгоритмите или резултатите, а справедливостта се стреми към корекция и баланс на тези отклонения. Това изисква интердисциплинарен подход, включващ статистика, етика, право и технически умения.

## 3. Key Concepts

- **Fairness (Справедливост)** – Качеството на алгоритъм да третира всички групи и индивиди без дискриминация. Може да се разбира като баланс между различни метрики, например равенство на възможностите или равенство на резултатите.
- **Bias (Пристрастие)** – Систематична грешка или отклонение в данните или модела, която води до несправедливо третиране на определени групи. Аналогия: като да използваме неправилна карта, която изкривява реалността.
- **Protected Attributes (Защитени характеристики)** – Характеристики като раса, пол, възраст, които законово не трябва да бъдат причина за дискриминация.
- **Disparate Impact (Различно въздействие)** – Когато алгоритъм има непропорционално негативен ефект върху определена група, дори без явна дискриминация.
- **Fairness Metrics (Метрики за справедливост)** – Количествени показатели, които измерват степента на справедливост, например Demographic Parity, Equal Opportunity, Equalized Odds.
- **Bias Detection Techniques (Техники за откриване на пристрастия)** – Методи като анализ на разпределения, визуализации, статистически тестове и интерпретируеми модели, които помагат да се идентифицират пристрастия.
- **Mitigation Strategies (Стратегии за смекчаване)** – Подходи като препроцесиране на данни, промяна на алгоритъма или постпроцесиране на резултатите, които намаляват или премахват пристрастията.

## 4. Step-by-step Learning Path

1. **Запознаване с основите на справедливостта и пристрастията**
   - Фокус: Разберете защо справедливостта е важна в ИИ.
   - Практическа задача: Прочетете и обобщете статия за социалните ефекти на пристрастията в ИИ.
   - Въпроси: Какво е пристрастие? Защо е опасно в автоматизирани решения?

2. **Изучаване на защитени характеристики и правни рамки**
   - Фокус: Идентифицирайте кои характеристики са защитени и какво означава дискриминация.
   - Практическа задача: Създайте списък с примери за защитени характеристики във вашата страна.
   - Въпроси: Какво е protected attribute? Какво е disparate impact?

3. **Запознаване с метрики за справедливост**
   - Фокус: Научете основните метрики и кога се използват.
   - Практическа задача: Изчислете Demographic Parity и Equal Opportunity на примерен набор от данни.
   - Въпроси: Какво измерва Demographic Parity? Как се различава от Equal Opportunity?

4. **Откриване на пристрастия в данни и модели**
   - Фокус: Използвайте техники за анализ на данни и визуализации.
   - Практическа задача: Анализирайте набор от данни за наличие на пристрастия спрямо пол или раса.
   - Въпроси: Какви техники могат да открият пристрастия? Какво показва визуализацията на разпределения?

5. **Прилагане на стратегии за смекчаване**
   - Фокус: Научете как да коригирате пристрастията.
   - Практическа задача: Използвайте препроцесиране на данни или постпроцесиране на резултати, за да подобрите справедливостта на модел.
   - Въпроси: Какви са основните методи за смекчаване? Кога е подходящо да се използва постпроцесиране?

6. **Оценка и мониторинг на справедливостта в реални системи**
   - Фокус: Разберете важността на постоянния контрол.
   - Практическа задача: Създайте план за мониторинг на справедливостта в съществуващ проект.
   - Въпроси: Защо е важно да следим справедливостта след внедряване? Какви индикатори да наблюдаваме?

## 5. Examples

### Пример 1: Откриване на пристрастия в кредитен скоринг

```python
import pandas as pd
from sklearn.metrics import confusion_matrix

# Зареждане на данни с колони: 'race', 'approved' (1/0), 'predicted_approval' (1/0)
data = pd.read_csv('credit_data.csv')

# Изчисляване на одобрения по раса
approval_rates = data.groupby('race')['predicted_approval'].mean()
print(approval_rates)

# Проверка за Demographic Parity
# Ако одобренията са значително различни между раси, има пристрастие
```

### Пример 2: Използване на Equal Opportunity метрика

```python
# True Positive Rate (TPR) за различни групи
def true_positive_rate(df, group_col, label_col, pred_col, group_value):
    group = df[df[group_col] == group_value]
    tp = ((group[label_col] == 1) & (group[pred_col] == 1)).sum()
    fn = ((group[label_col] == 1) & (group[pred_col] == 0)).sum()
    return tp / (tp + fn)

tpr_group1 = true_positive_rate(data, 'gender', 'approved', 'predicted_approval', 'female')
tpr_group2 = true_positive_rate(data, 'gender', 'approved', 'predicted_approval', 'male')

print(f"TPR Female: {tpr_group1}, TPR Male: {tpr_group2}")
```

### Пример 3: Корекция на пристрастия чрез препроцесиране

```python
from aif360.datasets import BinaryLabelDataset
from aif360.algorithms.preprocessing import Reweighing

dataset = BinaryLabelDataset(df=data, label_names=['approved'], protected_attribute_names=['race'])

rw = Reweighing(unprivileged_groups=[{'race': 0}], privileged_groups=[{'race': 1}])
dataset_transf = rw.fit_transform(dataset)

# След това dataset_transf се използва за обучение на модел с по-малко пристрастия
```

## 6. Common Pitfalls

- **Игнориране на защитени характеристики** – Пропускането на анализ по отношение на защитени атрибути води до скрити пристрастия.
- **Използване на неподходящи метрики** – Неправилният избор на метрика може да създаде илюзия за справедливост.
- **Прекомерна оптимизация за справедливост** – Прекалено фокусиране върху една метрика може да влоши други аспекти на модела (напр. точност).
- **Недостатъчен мониторинг след внедряване** – Справедливостта може да се влоши с времето, ако не се следи постоянно.
- **Пренебрегване на контекста** – Справедливостта е социално и културно обусловена, затова универсални решения не винаги са приложими.
- **Смесване на корелация с причинност** – Пристрастията могат да са резултат от скрити фактори, които не се откриват само с корелационен анализ.

## 7. Short Retrieval Quiz

1. Какво означава терминът „bias“ в контекста на ИИ?
2. Кои са примерни защитени характеристики?
3. Каква е разликата между Demographic Parity и Equal Opportunity?
4. Какви са основните подходи за смекчаване на пристрастия?
5. Защо е важно да се следи справедливостта и след внедряване на модел?
6. Какво представлява disparate impact?
7. Коя техника може да се използва за препроцесиране на данни с цел намаляване на пристрастия?

## 8. Quick Recap

- Справедливостта гарантира, че ИИ системите не дискриминират определени групи.
- Пристрастията са систематични грешки, които водят до несправедливи резултати.
- Защитените характеристики са ключови за откриване на дискриминация.
- Метриките за справедливост измерват различни аспекти на равнопоставеност.
- Откриването и смекчаването на пристрастия изискват технически и етически умения.
- Постоянният мониторинг е необходим за поддържане на справедливостта в реални системи.
- Контекстът и социалните фактори са важни при прилагането на решения за справедливост.

## 9. Spaced Review Plan

| Време след учене | Прегледен въпрос/Задача                                  |
|-------------------|---------------------------------------------------------|
| 1 ден             | Опишете с прости думи какво е fairness и bias.          |
| 3 дни             | Избройте и обяснете две метрики за справедливост.       |
| 1 седмица         | Анализирайте примерен набор от данни за наличие на bias. |
| 1 месец           | Създайте кратък план за мониторинг на справедливостта в проект. |