# Vision_language_models

## 1. Activate Prior Knowledge
- Какво представляват мултимодалните AI системи и какви предизвикателства решават?
- Как бихте описали връзката между визуална информация и естествен език в контекста на машинното обучение?
- Къде в съвременните приложения на изкуствения интелект виждате нужда от модели, които разбират едновременно изображения и текст?

## 2. Overview
Vision-language models (VLMs) са специализирани изкуствени интелект системи, които интегрират визуална и езикова информация, за да разбират и генерират смислени отговори, базирани на комбинирани мултимодални данни. Те се използват за задачи като описване на изображения, визуален въпрос-отговор, търсене на изображения по текстово описание и много други.

Тези модели се намират на пресечната точка между компютърното зрение и обработката на естествен език, като обединяват представяния на изображения и текст в общо семантично пространство. Това позволява на системите да "разбират" съдържанието на изображения в контекста на езикови запитвания и обратното.

Значението на VLMs се увеличава с растящия обем на мултимодални данни и нуждата от интелигентни системи, които могат да взаимодействат с хора по естествен начин, използвайки както визуални, така и езикови сигнали.

## 3. Key Concepts
- **Multimodal Learning** – Обучение, което използва повече от един тип данни (например изображения и текст), за да създаде по-богати и свързани представяния. Представете си го като учене чрез комбиниране на различни сетива – зрение и слух.
- **Embedding Space** – Математическо пространство, в което изображения и текст се преобразуват в числови вектори, така че семантично сходните обекти са близо един до друг. Аналогия: карта, където близките точки означават сходни значения.
- **Cross-modal Attention** – Механизъм, който позволява на модела да фокусира вниманието си върху релевантни части от изображение и текст едновременно, подобно на начина, по който човек обръща внимание на определени детайли при четене и гледане.
- **Pretraining and Fine-tuning** – Първоначално обучение върху големи набори от данни за общи знания, последвано от адаптация към специфични задачи. Мислете за това като първоначално образование и последваща специализация.
- **Vision Transformer (ViT)** – Модел, който прилага архитектурата на трансформърите към изображения, разделяйки ги на малки парчета (patches) и обработвайки ги подобно на думи в текст.

## 4. Step-by-step Learning Path
1. **Запознайте се с основите на компютърното зрение и NLP**
   - Фокус: Разберете как се представят изображения и текст в цифров вид.
   - Задача: Прегледайте и визуализирайте embeddings на изображения и текст с помощта на предварително обучени модели.
   - Въпроси: Какво е embedding? Защо е важно да имаме общо пространство за изображения и текст?

2. **Изучете архитектурата на трансформърите**
   - Фокус: Разберете как работят трансформърите и защо са подходящи за мултимодални задачи.
   - Задача: Имплементирайте прост трансформър за текстова задача с помощта на библиотека като PyTorch или TensorFlow.
   - Въпроси: Как трансформърът обработва последователности? Какво е self-attention?

3. **Разгледайте Vision Transformer (ViT) и мултимодални архитектури**
   - Фокус: Научете как изображенията се обработват чрез трансформъри и как се комбинират с текст.
   - Задача: Използвайте предварително обучен ViT модел за класификация на изображения.
   - Въпроси: Какво представляват image patches? Как се комбинират визуалните и езиковите данни?

4. **Практикувайте с готови vision-language модели**
   - Фокус: Работете с модели като CLIP, ALIGN или BLIP.
   - Задача: Направете търсене на изображения по текстово описание с CLIP.
   - Въпроси: Как CLIP съпоставя изображения и текст? Какво е zero-shot learning?

5. **Финно настройване и създаване на собствени приложения**
   - Фокус: Адаптирайте модел към специфична задача (напр. визуален въпрос-отговор).
   - Задача: Фино настройте модел върху малък набор от данни и тествайте.
   - Въпроси: Какво е transfer learning? Как да избегнем overfitting?

## 5. Examples
### Пример 1: Търсене на изображения по текст с CLIP
```python
import torch
import clip
from PIL import Image

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

image = preprocess(Image.open("cat.jpg")).unsqueeze(0).to(device)
text = clip.tokenize(["a photo of a cat", "a photo of a dog"]).to(device)

with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)
    logits_per_image, logits_per_text = model(image, text)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()

print("Probabilities:", probs)
```

### Пример 2: Описание на изображение с BLIP
```python
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

image = Image.open("dog.jpg")
inputs = processor(image, return_tensors="pt")

out = model.generate(**inputs)
caption = processor.decode(out[0], skip_special_tokens=True)
print(caption)
```

## 6. Common Pitfalls
- **Игнориране на качеството на данните** – Мултимодалните модели са чувствителни към шум и несъответствия между изображения и текст. Винаги проверявайте и почиствайте данните.
- **Прекомерно фино настройване върху малки набори** – Може да доведе до overfitting и загуба на обща способност на модела.
- **Недостатъчно разбиране на embedding пространството** – Без ясна представа как се съпоставят визуални и текстови вектори, трудно се интерпретират резултатите.
- **Пренебрегване на изчислителните ресурси** – Моделите са големи и изискват оптимизации и подходящ хардуер.
- **Липса на оценка на мултимодалната производителност** – Използвайте подходящи метрики за комбинирани задачи, а не само за отделни модалности.

## 7. Short Retrieval Quiz
1. Какво е мултимодално обучение?
2. Какво представлява embedding пространството в контекста на VLM?
3. Как трансформърите използват механизма на внимание?
4. Какви са основните задачи, които решават vision-language модели?
5. Какво е zero-shot learning и как се използва в CLIP?
6. Защо е важно финото настройване да се прави внимателно?
7. Какво представлява Vision Transformer?

## 8. Quick Recap
- Vision-language models комбинират визуални и текстови данни за по-добро разбиране и генериране.
- Те използват embedding пространства, където изображения и текст се съпоставят семантично.
- Трансформър архитектурата е ключова за обработката и интеграцията на мултимодални данни.
- Модели като CLIP и BLIP демонстрират практическо приложение на VLM в търсене и описание на изображения.
- Качеството на данните и внимателното фино настройване са критични за успеха на тези модели.
- Разбирането на механизми като cross-modal attention помага за по-добра интерпретация и разработка.
- Изчислителните ресурси и правилните метрики са важни за ефективна работа и оценка.

## 9. Spaced Review Plan

| Време след изучаване | Промпт за преговор                                 |
|----------------------|---------------------------------------------------|
| 1 ден                | Обяснете с прости думи какво представляват VLM.  |
| 3 дни                | Избройте и опишете ключовите концепции във VLM.  |
| 1 седмица            | Опишете архитектурата на трансформърите и ViT.   |
| 1 месец              | Приложете VLM за конкретна задача и анализирайте резултатите. |