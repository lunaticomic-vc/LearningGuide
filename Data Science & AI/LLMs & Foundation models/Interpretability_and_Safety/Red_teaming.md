# Red_teaming

## 1. Activate Prior Knowledge

- Какво разбирате под „тестване на сигурността“ или „етичен хакеринг“ в контекста на софтуерни системи и изкуствен интелект?
- Какви са основните рискове и уязвимости, които могат да възникнат при внедряване на AI системи в реална среда?
- Как бихте дефинирали ролята на „червения отбор“ (red team) спрямо „синия отбор“ (blue team) в киберсигурността и софтуерното инженерство?

## 2. Overview

Red teaming е систематичен подход за симулиране на атаки или опити за компрометиране на система с цел откриване на уязвимости, които традиционните тестове може да пропуснат. В контекста на AI и софтуерното инженерство, red teaming се използва за оценка на устойчивостта на системите спрямо злонамерени действия, грешки или непредвидени сценарии.

Тази практика е част от по-широкия процес на сигурност и качество, като допълва автоматизираните тестове и стандартните проверки. Red teaming не само тества техническата защита, но и организационните процеси, реакцията на екипа и готовността за инциденти.

Значението на red teaming се увеличава с нарастването на сложността на AI системите, където традиционните методи не винаги могат да предвидят всички възможни атаки или експлоатиране на слабости. Той осигурява по-реалистична и практическа оценка, която помага да се подобри сигурността и надеждността на системите.

## 3. Key Concepts

- **Red Team** – екип от специалисти, които симулират атаки и опити за проникване в система, за да открият уязвимости и да подобрят защитата ѝ. Можем да го сравним с „противников отбор“ в спорт, който тества силите на „синия отбор“.
- **Blue Team** – екип, който защитава системата, открива и реагира на атаки. Те са „отборът по защита“ в киберсигурността.
- **Purple Team** – сътрудничество между red и blue teams за по-добро разбиране и подобряване на сигурността.
- **Threat Modeling** – процес на идентифициране, оценка и приоритизиране на потенциални заплахи към системата.
- **Adversarial Testing** – специфичен вид red teaming, при който се създават и прилагат злонамерени входни данни, за да се тестват AI модели.
- **Attack Surface** – всички точки на достъп или взаимодействие със системата, които могат да бъдат използвани за атака.
- **Exploit** – конкретен метод или код, който използва уязвимост в системата за постигане на нежелан ефект.
- **Social Engineering** – техники за манипулиране на хора с цел получаване на достъп или информация, често част от red teaming.

## 4. Step-by-step Learning Path

1. **Запознаване с основите на киберсигурността и red teaming**
   - Фокус: Основни принципи на сигурността, роли на red и blue teams.
   - Задача: Прочетете и обобщете статия за разликите между red и blue teams.
   - Въпроси: Каква е основната цел на red team? Какво прави blue team?

2. **Изучаване на threat modeling и attack surface analysis**
   - Фокус: Идентифициране на потенциални заплахи и уязвимости.
   - Задача: Създайте threat model за проста AI система (например чатбот).
   - Въпроси: Какви са основните компоненти на threat model? Какво е attack surface?

3. **Практическо прилагане на adversarial testing върху AI модели**
   - Фокус: Създаване на злонамерени входни данни и анализ на реакцията на модела.
   - Задача: Използвайте библиотека като CleverHans или Adversarial Robustness Toolbox, за да генерирате adversarial примери.
   - Въпроси: Какво представлява adversarial example? Защо е важно да се тества AI с такива примери?

4. **Разработка на сценарии за социално инженерство**
   - Фокус: Разбиране на човешкия фактор в сигурността.
   - Задача: Проектирайте кратък сценарий за фишинг атака, насочена към служители.
   - Въпроси: Как социалното инженерство може да компрометира система? Какви са защитните мерки?

5. **Извършване на симулирана red team атака**
   - Фокус: Интегриране на наученото в цялостен тест.
   - Задача: Проведете малък red team тест върху демонстрационна система, документирайте уязвимостите.
   - Въпроси: Кои уязвимости открихте? Как бихте препоръчали да се подобри защитата?

## 5. Examples

### Пример 1: Adversarial attack върху AI модел за класификация на изображения

```python
import tensorflow as tf
import numpy as np
from cleverhans.tf2.attacks import fast_gradient_method

# Зареждане на предварително обучен модел
model = tf.keras.applications.MobileNetV2(weights='imagenet')

# Зареждане на примерна снимка
image = tf.keras.preprocessing.image.load_img('cat.jpg', target_size=(224, 224))
image = tf.keras.preprocessing.image.img_to_array(image)
image = np.expand_dims(image, axis=0)
image = tf.keras.applications.mobilenet_v2.preprocess_input(image)

# Генериране на adversarial пример
adv_image = fast_gradient_method(model, image, eps=0.03, norm=np.inf)

# Предсказване преди и след атаката
preds_orig = model.predict(image)
preds_adv = model.predict(adv_image)
print("Original prediction:", tf.keras.applications.mobilenet_v2.decode_predictions(preds_orig, top=1))
print("Adversarial prediction:", tf.keras.applications.mobilenet_v2.decode_predictions(preds_adv, top=1))
```

### Пример 2: Сценарий за социално инженерство

- Изпращане на фишинг имейл, който изглежда като официално съобщение от IT отдела, с линк към фалшива страница за вход.
- Цел: Получаване на потребителски имена и пароли.
- Мярка за защита: Обучение на служителите и двуфакторна автентикация.

### Пример 3: Threat modeling на AI чатбот

- Идентифициране на входните точки: уеб интерфейс, API.
- Потенциални заплахи: injection атаки, манипулиране на отговорите, изтичане на лични данни.
- Препоръки: валидиране на входни данни, мониторинг и логване на аномалии.

## 6. Common Pitfalls

- **Пренебрегване на човешкия фактор** – много red team тестове забравят социалното инженерство, което често е най-лесният път за компрометиране.
- **Ограничаване само до технически уязвимости** – red teaming трябва да включва и организационни и процесни слабости.
- **Липса на ясна комуникация с blue team** – без сътрудничество, уроците от red team остават неизползвани.
- **Недостатъчно реалистични сценарии** – твърде теоретичните атаки не отразяват истинските заплахи.
- **Прекалено доверие в автоматизирани инструменти** – ръчната проверка и креативността са незаменими.

## 7. Short Retrieval Quiz

1. Каква е основната цел на red teaming?
2. Какво представлява adversarial example в AI?
3. Коя е ролята на blue team?
4. Какво е attack surface?
5. Защо е важно да се включва социално инженерство в red teaming?
6. Какво представлява threat modeling?
7. Какъв е смисълът на purple team?

## 8. Quick Recap

- Red teaming е симулиране на атаки за откриване на уязвимости в системи.
- В AI контекста, red teaming включва adversarial testing и оценка на човешкия фактор.
- Основните роли са red team (атакуващи) и blue team (защитници).
- Threat modeling и анализ на attack surface са ключови за подготовката.
- Социалното инженерство е критичен компонент на реалистичните атаки.
- Ефективната комуникация между екипите подобрява сигурността.
- Избягвайте прекалена зависимост от автоматизация и непълни сценарии.

## 9. Spaced Review Plan

| Време след учене | Прегледна задача                                      |
|------------------|------------------------------------------------------|
| 1 ден            | Отговорете на quiz въпросите без помощ              |
| 3 дни            | Прегледайте threat modeling и adversarial testing   |
| 1 седмица        | Проведете малък red team тест върху учебна система  |
| 1 месец          | Обсъдете с колеги или напишете кратък доклад за red teaming практики |