# Tokenization

## 1. Activate Prior Knowledge
- Какво представлява текстът като данни в компютърните системи и защо е необходимо да го преобразуваме?
- Как смятате, че машините „разбират“ думите в изреченията, когато обработват естествен език?
- В кои части на AI системите или софтуерното инженерство може да срещнете процес, който разделя текст на по-малки части?

## 2. Overview
Tokenization е процесът на разделяне на текст на по-малки единици, наречени токени. Тези токени могат да бъдат думи, части от думи или дори символи, в зависимост от целта и приложението. Този процес е фундаментален за обработката на естествен език (NLP) и други AI системи, защото позволява на алгоритмите да работят с разбираеми и структурирани данни.

В по-широк контекст, tokenization е първата стъпка в много NLP задачи като машинен превод, анализ на настроения, търсене и разпознаване на говор. Без правилна токенизация, последващите стъпки като анализ на синтаксис или семантика биха били неефективни или неточни.

Този процес също така влияе върху производителността и точността на моделите, тъй като различните техники за токенизация могат да променят начина, по който текстът се интерпретира и обработва. Затова разбирането на tokenization е ключово за всеки, който иска да разработва или използва AI системи, базирани на текст.

## 3. Key Concepts
- **Token** – Най-малката единица текст, която има смисъл за дадена задача. Представете си токен като „парче пъзел“, което заедно с други парчета изгражда цялата картина.
- **Tokenizer** – Софтуерен инструмент или алгоритъм, който разделя текст на токени. Може да бъде сравнен с нож, който реже дълъг хляб на филийки.
- **Whitespace Tokenization** – Най-простият метод, който разделя текста на токени, използвайки интервали и нови редове като разделители.
- **Subword Tokenization** – Метод, който разделя думите на по-малки части (например суфикси или префикси), полезен при работа с редки или нови думи.
- **Vocabulary** – Списък с всички токени, които моделът или системата разпознава и обработва. Може да се мисли като речник, който системата „знае“.
- **Out-of-Vocabulary (OOV) Tokens** – Токени, които не са част от речника, често предизвикват проблеми при обработката.
- **Byte Pair Encoding (BPE)** – Популярен алгоритъм за subword tokenization, който балансира между думи и по-малки части, оптимизирайки размера на речника и покритието.

## 4. Step-by-step Learning Path
1. **Разберете основната идея на токенизацията**  
   - Фокус: Какво е токен и защо е важен.  
   - Задача: Вземете кратък текст и го разделете ръчно на думи (токени).  
   - Въпроси: Какво е токен? Защо не можем да работим директно с целия текст?

2. **Изучете различните методи за токенизация**  
   - Фокус: Whitespace, punctuation, subword tokenization.  
   - Задача: Използвайте библиотека като NLTK или SpaCy, за да токенизирате един и същ текст по различни начини.  
   - Въпроси: Как се различават резултатите? Кога subword токенизация е по-подходяща?

3. **Практикувайте с реални инструменти и библиотеки**  
   - Фокус: Инсталирайте и използвайте tokenizer от Hugging Face Transformers.  
   - Задача: Токенизирайте няколко изречения и разгледайте изхода (token ids, attention masks).  
   - Въпроси: Какво представляват token ids? Защо са необходими?

4. **Изследвайте влиянието на токенизацията върху модели**  
   - Фокус: Как токенизацията влияе на обучението и производителността на NLP модели.  
   - Задача: Сравнете резултатите на модел, обучен с различни токенизатори върху една и съща задача.  
   - Въпроси: Как токенизацията може да подобри или влоши резултатите?

5. **Разгледайте проблемите с OOV токените и решенията им**  
   - Фокус: Как се справят моделите с непознати думи.  
   - Задача: Тествайте модели с текст, съдържащ нови думи и анализирайте поведението.  
   - Въпроси: Какво е OOV? Как subword tokenization помага при OOV?

## 5. Examples
### Пример 1: Whitespace Tokenization на български текст
```python
text = "Това е пример за токенизация."
tokens = text.split()
print(tokens)
# Изход: ['Това', 'е', 'пример', 'за', 'токенизация.']
```

### Пример 2: Токенизация с NLTK (английски)
```python
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

text = "Tokenization is the first step in NLP."
tokens = word_tokenize(text)
print(tokens)
# Изход: ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '.']
```

### Пример 3: Subword Tokenization с Hugging Face BPE Tokenizer
```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
text = "unaffable"
tokens = tokenizer.tokenize(text)
print(tokens)
# Изход: ['un', '##aff', '##able']
```

## 6. Common Pitfalls
- **Игнориране на пунктуацията** – Простото разделяне по интервали често оставя пунктуацията прикрепена към думите, което може да обърка модела. Използвайте по-сложни токенизатори.
- **Недооценяване на OOV проблема** – Ако речникът е твърде малък, много думи ще бъдат непознати, което води до загуба на информация.
- **Неправилна обработка на езикови особености** – Например, при български език склоненията и съчетанията могат да изискват специална обработка.
- **Прекомерно разчленяване** – Субдумизацията може да доведе до твърде много токени, което увеличава изчислителната сложност.
- **Пропускане на нормализация** – Липсата на нормализация (като малки/големи букви, премахване на диакритични знаци) може да доведе до излишно разширяване на речника.

## 7. Short Retrieval Quiz
1. Какво представлява токенът в контекста на обработката на естествен език?  
2. Коя е основната цел на процеса tokenization?  
3. Какво е разликата между whitespace tokenization и subword tokenization?  
4. Защо OOV токените са проблем и как subword tokenization помага?  
5. Какво представлява vocabulary в NLP системите?  
6. Какви са рисковете от използване на твърде малък речник?  
7. Какво е Byte Pair Encoding (BPE) накратко?

## 8. Quick Recap
- Tokenization разделя текст на по-малки, смислени единици – токени.  
- Токенизацията е първата и критична стъпка в NLP и AI системи, базирани на текст.  
- Съществуват различни методи: whitespace, punctuation, subword tokenization.  
- Subword tokenization решава проблема с непознатите думи (OOV).  
- Правилната токенизация подобрява точността и ефективността на модели за обработка на език.  
- Използването на подходящ tokenizer и речник е ключово за успешни NLP проекти.  
- Често срещани грешки включват игнориране на пунктуация и лоша обработка на езикови особености.

## 9. Spaced Review Plan

| Време след изучаване | Промпт за преговор                                  |
|----------------------|----------------------------------------------------|
| 1 ден                | Обяснете с прости думи какво е tokenization.       |
| 3 дни                | Избройте и сравнете основните методи за токенизация.|
| 1 седмица            | Опишете как subword tokenization решава проблема с OOV.|
| 1 месец              | Дайте пример за приложение на tokenization в реален NLP проект. |