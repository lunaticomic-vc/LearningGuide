# SVM_kNN_Naive_Bayes

## 1. Activate Prior Knowledge
- Какво представляват алгоритмите за класификация и къде се използват в съвременните AI системи?
- Как бихте обяснили разликата между модели, базирани на вероятности, и такива, базирани на разстояния?
- Какви са предизвикателствата при работа с големи и сложни набори от данни в контекста на машинното обучение?

## 2. Overview
Support Vector Machines (SVM), k-Nearest Neighbors (kNN) и Naive Bayes са три фундаментални алгоритъма за класификация, широко използвани в машинното обучение и изкуствения интелект. Всеки от тях предлага различен подход за разделяне на данните в категории, което ги прави подходящи за различни типове задачи и набори от данни.

SVM се фокусира върху намирането на оптимална граница (хиперплан), която максимално разделя класовете, като се стреми да минимизира грешките при класификация. kNN е интуитивен и базиран на близостта между точките в пространството на характеристиките, като класифицира нови примери според мнозинството от най-близките им съседи. Naive Bayes използва вероятностен модел, базиран на теоремата на Байес, като приема условна независимост между характеристиките, което го прави изключително бърз и ефективен за големи и високоизмерни данни.

Тези алгоритми са ключови в изграждането на системи за разпознаване на образи, филтриране на спам, медицинска диагностика и много други области. Разбирането на техните принципи и ограничения е от съществено значение за проектиране на надеждни и ефективни AI решения.

## 3. Key Concepts
- **Support Vector Machine (SVM)** – Модел, който намира най-добрата граница (хиперплан) за разделяне на класове, като максимизира разстоянието (margin) между най-близките точки от различните класове (support vectors). Мислете за SVM като за опит да поставите ограда между два вида плодове в градина, така че да има максимално разстояние между тях.
- **k-Nearest Neighbors (kNN)** – Небайесов, базиран на разстояния алгоритъм, който класифицира нови точки според мнозинството от k-те най-близки съседи. Представете си, че сте в нов град и питате няколко местни жители (съседите) за най-добрия ресторант – мнението на повечето ще ви насочи.
- **Naive Bayes** – Вероятностен класификатор, който използва теоремата на Байес с предположението, че всички характеристики са независими една от друга. Това е като да оценявате вероятността за дадено събитие, като разглеждате всяка причина поотделно, без да се влияят взаимно.
- **Margin** – Разстоянието между хиперплана на SVM и най-близките точки от всеки клас. По-голям margin означава по-добра обобщаваща способност.
- **Conditional Independence** – Предположението в Naive Bayes, че характеристиките са независими при даден клас, което опростява изчисленията.
- **Distance Metric** – Мярка за разстояние между точки, например Евклидово разстояние, използвано в kNN за определяне на близост.
- **Overfitting** – Когато моделът се адаптира твърде много към тренировъчните данни и губи способността си да обобщава върху нови примери.

## 4. Step-by-step Learning Path
1. **Разберете основите на класификацията и типовете данни**
   - Фокус: Какво е класификация и как се представят данните.
   - Задача: Изберете прост набор от данни (напр. Iris) и разгледайте класовете и характеристиките.
   - Въпроси: Какво означава "клас" в машинното обучение? Какви типове характеристики съществуват?

2. **Изучете принципа на работа на SVM**
   - Фокус: Хиперплан, margin, support vectors.
   - Задача: Визуализирайте SVM върху двумерен набор от данни с помощта на библиотека като scikit-learn.
   - Въпроси: Какво представляват support vectors? Защо margin е важен?

3. **Практикувайте kNN и разстоянията**
   - Фокус: Избор на k, метрики за разстояние.
   - Задача: Имплементирайте kNN от нулата за малък набор от данни и експериментирайте с различни стойности на k.
   - Въпроси: Как влияе стойността на k на резултатите? Коя метрика за разстояние използвахте?

4. **Разберете Naive Bayes и вероятностите**
   - Фокус: Теорема на Байес, условна независимост.
   - Задача: Използвайте Naive Bayes за текстова класификация (напр. спам/не-спам).
   - Въпроси: Какво означава условна независимост? Защо Naive Bayes е бърз?

5. **Сравнете и комбинирайте алгоритмите**
   - Фокус: Кога кой алгоритъм е подходящ.
   - Задача: Изберете реален набор от данни и приложете трите алгоритъма, сравнете точността и времето за обучение.
   - Въпроси: Кой алгоритъм работи най-добре при висока размерност? Кой е най-лесен за интерпретация?

## 5. Examples

### Пример 1: SVM с scikit-learn (Python)
```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)

svm = SVC(kernel='linear')
svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)

print("SVM Accuracy:", accuracy_score(y_test, y_pred))
```

### Пример 2: kNN имплементация от нулата
```python
import numpy as np
from collections import Counter

def euclidean_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

def knn_predict(X_train, y_train, x_test, k=3):
    distances = [euclidean_distance(x_test, x) for x in X_train]
    k_indices = np.argsort(distances)[:k]
    k_nearest_labels = [y_train[i] for i in k_indices]
    most_common = Counter(k_nearest_labels).most_common(1)
    return most_common[0][0]
```

### Пример 3: Naive Bayes за текстова класификация (scikit-learn)
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

texts = ["spam message", "important email", "spam offer", "meeting schedule"]
labels = [1, 0, 1, 0]  # 1 = spam, 0 = not spam

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)

nb = MultinomialNB()
nb.fit(X_train, y_train)
y_pred = nb.predict(X_test)

print("Naive Bayes Accuracy:", accuracy_score(y_test, y_pred))
```

## 6. Common Pitfalls
- **SVM с неправилен избор на kernel** – Изборът на kernel функция (линеен, RBF и др.) е критичен. Неподходящ kernel може да доведе до лоша класификация.
- **kNN с неподходящо k** – Малки стойности на k водят до шум и overfitting, големи – до underfitting. Винаги тествайте няколко стойности.
- **Naive Bayes и условната независимост** – В реални данни характеристиките често не са независими, което може да намали точността на модела.
- **Не нормализиране на данните за kNN и SVM** – Разстоянията са чувствителни към мащаба на характеристиките, затова нормализацията е задължителна.
- **Пренебрегване на баланс на класовете** – При дисбалансирани класове моделите могат да се пристрастят към по-често срещания клас.

## 7. Short Retrieval Quiz
1. Какво представлява margin в SVM?
2. Как kNN определя класа на нов пример?
3. Какво е основното предположение на Naive Bayes?
4. Защо е важно да нормализираме данните при kNN?
5. Какъв е ефектът от малка стойност на k в kNN?
6. Кога SVM използва kernel функция?
7. Какво означава "support vectors"?

## 8. Quick Recap
- SVM намира оптимална граница за разделяне на класове чрез максимизиране на margin.
- kNN класифицира по базата на мнозинството от най-близките съседи.
- Naive Bayes използва вероятностен модел с предположение за условна независимост.
- Нормализацията на данните е критична за kNN и SVM.
- Изборът на параметри (k, kernel) влияе силно на представянето на моделите.
- Всеки алгоритъм има силни и слаби страни, подходящи за различни типове задачи.
- Практическото прилагане изисква експериментиране и разбиране на данните.

## 9. Spaced Review Plan

| Време след учене | Промпт за преговор                                      |
|-------------------|---------------------------------------------------------|
| 1 ден             | Обяснете с прости думи как работи SVM и какво е margin. |
| 3 дни             | Опишете разликите между kNN и Naive Bayes.              |
| 1 седмица         | Имплементирайте кратък пример с kNN и анализирайте k.   |
| 1 месец           | Сравнете приложението на SVM, kNN и Naive Bayes в реален проект. |