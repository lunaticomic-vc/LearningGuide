# RNNs_LSTMs_GRUs

## 1. Activate Prior Knowledge

- Какво представляват невронните мрежи и как се използват за обработка на данни в последователности?
- Защо стандартните feedforward невронни мрежи не са ефективни при задачи с времеви зависимости?
- Какви са основните предизвикателства при моделирането на дългосрочни зависимости в последователни данни?

## 2. Overview

Рекурентните невронни мрежи (RNNs) са специален тип невронни мрежи, проектирани да обработват последователни данни, като текст, аудио или времеви серии. Те запазват вътрешно състояние, което им позволява да "помнят" информация от предишни стъпки, което е ключово за разбиране на контекст и зависимост във времето.

Въпреки това, класическите RNN модели страдат от проблема с изчезващия и експлодиращ градиент, което ограничава способността им да учат дългосрочни зависимости. За да се преодолее това, са разработени архитектури като LSTM (Long Short-Term Memory) и GRU (Gated Recurrent Unit), които въвеждат механизми за контрол на потока на информация и по-добро запазване на важни характеристики във времето.

Тези модели са широко използвани в множество приложения като машинен превод, разпознаване на реч, анализ на времеви серии и др. Разбирането на тяхната архитектура и работа е фундаментално за всеки, който иска да разработва ефективни AI системи, работещи с последователни данни.

## 3. Key Concepts

- **Recurrent Neural Network (RNN)** – Невронна мрежа, която обработва входни данни последователно, като използва скрити състояния за запомняне на предишна информация. Може да се сравни с човек, който чете текст и запомня контекста от предходните изречения.

- **Vanishing Gradient Problem** – Проблем при обучението на RNN, при който градиентите стават прекалено малки, за да обновят ефективно тежестите, особено при дълги последователности. Това е като да се опитваш да предадеш съобщение по верига от хора, където то се губи с всяка следваща стъпка.

- **Long Short-Term Memory (LSTM)** – Специална архитектура на RNN, която използва "гейтове" (входен, забравящ и изходен) за контролиране на информацията, която се запазва или забравя. Може да се мисли като памет с филтри, която решава какво да запази и какво да изхвърли.

- **Gated Recurrent Unit (GRU)** – По-опростена версия на LSTM, която комбинира някои гейтове и има по-малко параметри, но също така ефективно управлява дългосрочната памет.

- **Gate (Гейт)** – Механизъм, който контролира потока на информация в RNN архитектурите, подобно на врати, които се отварят или затварят, за да пуснат или блокират данни.

- **Hidden State (Скрито състояние)** – Вътрешното състояние на RNN, което съхранява информация от предишни стъпки и се обновява на всяка итерация.

## 4. Step-by-step Learning Path

1. **Запознаване с класически RNN**
   - Фокус: Разберете основната структура и как се обработват последователности.
   - Задача: Имплементирайте базов RNN за прогнозиране на следващ символ в текст.
   - Въпроси: Какво представлява скритото състояние? Защо RNN може да запомня предишни входове?

2. **Изследване на проблема с изчезващия градиент**
   - Фокус: Анализирайте защо класическите RNN не могат да научат дългосрочни зависимости.
   - Задача: Тествайте RNN върху дълги последователности и наблюдавайте обучението.
   - Въпроси: Как изчезващият градиент влияе на обучението? Какво се случва с информацията в дълги последователности?

3. **Разбиране на LSTM архитектурата**
   - Фокус: Изучете гейтовете и как те контролират паметта.
   - Задача: Имплементирайте LSTM клетка и тествайте на задачата от стъпка 1.
   - Въпроси: Какви са функциите на входния, забравящия и изходния гейт? Как LSTM решава проблема с изчезващия градиент?

4. **Сравнение с GRU**
   - Фокус: Разберете разликите и приликите между LSTM и GRU.
   - Задача: Имплементирайте GRU и сравнете резултатите с LSTM.
   - Въпроси: Как GRU опростява архитектурата? В какви случаи GRU може да е по-подходящ?

5. **Приложение в реални задачи**
   - Фокус: Използвайте LSTM/GRU в задачи като машинен превод или анализ на времеви серии.
   - Задача: Обучете модел за прогнозиране на времеви ред и оценете резултатите.
   - Въпроси: Какви са предимствата на RNN архитектурите в реални приложения? Как да изберем подходящ модел?

## 5. Examples

### Пример 1: Базов RNN за прогнозиране на следващ символ (PyTorch)

```python
import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.rnn(x)
        out = self.fc(out[:, -1, :])
        return out

# Инициализация и примерен вход
model = SimpleRNN(input_size=10, hidden_size=20, output_size=10)
x = torch.randn(5, 15, 10)  # batch=5, seq_len=15, input_size=10
output = model(x)
print(output.shape)  # torch.Size([5, 10])
```

### Пример 2: LSTM за прогнозиране на времеви ред

```python
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out
```

### Пример 3: GRU за класификация на текст

```python
class GRUClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(GRUClassifier, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.gru(x)
        out = self.fc(out[:, -1, :])
        return out
```

## 6. Common Pitfalls

- **Игнориране на проблема с изчезващия градиент** – Опит за обучение на класически RNN върху дълги последователности без LSTM/GRU води до лоши резултати.
- **Неправилна инициализация на скритото състояние** – Трябва да се инициализира правилно, за да се избегнат нестабилности.
- **Прекомерна сложност на модела** – Използване на твърде големи LSTM/GRU мрежи без достатъчно данни води до пренасищане.
- **Неправилно разбиране на гейтовете** – Гейтовете не са просто филтри, а динамични контролери на информацията, които трябва да се тренират внимателно.
- **Липса на нормализация и регуляризация** – Може да доведе до пренасищане и нестабилно обучение.

## 7. Short Retrieval Quiz

1. Какво представлява скритото състояние в RNN?
2. Защо класическите RNN имат проблем с изчезващия градиент?
3. Каква е ролята на забравящия гейт в LSTM?
4. Каква е основната разлика между LSTM и GRU?
5. В какви приложения са особено полезни RNN архитектурите?
6. Как гейтовете помагат за запазване на дългосрочна памет?
7. Какво се случва с градиентите при експлодиращ градиент и как се контролира?

## 8. Quick Recap

- RNN са невронни мрежи, специално проектирани за обработка на последователни данни чрез скрито състояние.
- Класическите RNN страдат от проблема с изчезващия градиент, което ограничава ученето на дългосрочни зависимости.
- LSTM и GRU са архитектури с гейтове, които контролират потока на информация и подобряват запомнянето.
- LSTM използва три гейта (входен, забравящ, изходен), докато GRU комбинира някои от тях за по-опростена структура.
- Тези модели намират приложение в машинен превод, разпознаване на реч, анализ на времеви серии и други.
- Внимателното обучение, инициализация и избор на архитектура са ключови за успеха.
- Изучаването на тези модели е фундаментално за разработка на съвременни AI системи, работещи с последователни данни.

## 9. Spaced Review Plan

| Време след изучаване | Промпт за преглед                                      |
|----------------------|-------------------------------------------------------|
| 1 ден                | Обяснете как работи LSTM и защо е по-добър от RNN.    |
| 3 дни                | Опишете разликите между LSTM и GRU.                   |
| 1 седмица            | Имплементирайте прост GRU модел за текстова задача.   |
| 1 месец              | Прегледайте и сравнете приложенията на RNN, LSTM и GRU в реални проекти. |