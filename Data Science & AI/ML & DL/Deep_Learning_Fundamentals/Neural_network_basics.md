# Neural_network_basics

## 1. Activate Prior Knowledge
- Какво представлява изкуственият интелект и какви са основните му компоненти?
- Какво знаете за начина, по който човешкият мозък обработва информация?
- Можете ли да предположите как компютър може да „учи“ от данни, за да взема решения?

## 2. Overview
Невронните мрежи са основен инструмент в съвременния изкуствен интелект, вдъхновени от структурата и функцията на човешкия мозък. Те представляват компютърни модели, които могат да разпознават модели, да класифицират данни и да правят прогнози, като се обучават върху примери.

В по-широк контекст, невронните мрежи са част от машинното обучение и дълбокото обучение, които са ключови за разработката на интелигентни системи – от разпознаване на образи и глас, до автономни превозни средства и препоръчителни системи.

Разбирането на основите на невронните мрежи е важно, защото те са в основата на много съвременни софтуерни решения и инженерни приложения, които изискват адаптивност и способност за обработка на големи обеми от данни.

## 3. Key Concepts
- **Neuron (Неврон)** – Основната изчислителна единица в мрежата, която приема входни сигнали, прилага тегла и функция за активиране, и издава изход. Може да се сравни с биологичен неврон, който обработва и предава информация.
- **Layer (Слой)** – Група от неврони, които работят паралелно. Има входен слой, един или повече скрити слоеве и изходен слой. Мислете за слоевете като етапи в обработката на информация.
- **Weights (Тегла)** – Параметри, които определят важността на входните сигнали към неврона. Те се настройват по време на обучението, подобно на начина, по който човек се учи чрез опит.
- **Activation Function (Функция за активиране)** – Нелинейна функция, която решава дали и колко силно да се активира невронът. Пример: ReLU, Sigmoid. Тя позволява на мрежата да учи сложни модели.
- **Forward Propagation (Напредно разпространение)** – Процесът на преминаване на входните данни през слоевете, за да се получи изход.
- **Backpropagation (Обратно разпространение)** – Метод за корекция на теглата чрез изчисляване на грешката и нейното разпространение назад в мрежата.
- **Loss Function (Функция на загуба)** – Мярка за това колко далеч е прогнозата на мрежата от реалната стойност. Целта е да се минимизира тази загуба.
- **Epoch (Епоха)** – Един пълен цикъл на обучение, при който всички обучителни данни са преминали през мрежата веднъж.

## 4. Step-by-step Learning Path
1. **Запознаване с архитектурата на невронната мрежа**  
   - Фокус: Разберете ролята на входния, скрития и изходния слой.  
   - Задача: Нарисувайте схема на проста мрежа с 3 входа, 1 скрит слой с 4 неврона и 1 изход.  
   - Въпроси: Каква е функцията на скрития слой? Защо имаме повече от един слой?

2. **Изучаване на функцията за активиране**  
   - Фокус: Разгледайте основните функции – ReLU, Sigmoid, Tanh.  
   - Задача: Напишете кратък код, който визуализира графиките на тези функции.  
   - Въпроси: Каква е разликата между ReLU и Sigmoid? Кога е полезна нелинейността?

3. **Практика с forward propagation**  
   - Фокус: Изчислете ръчно изхода на малка мрежа с дадени тегла и входове.  
   - Задача: Използвайте Python, за да имплементирате forward pass за 1 слой.  
   - Въпроси: Как теглата влияят на изхода? Какво се случва, ако всички тегла са нули?

4. **Запознаване с backpropagation и оптимизация**  
   - Фокус: Разберете как се изчислява грешката и как се коригират теглата.  
   - Задача: Симулирайте една стъпка на обратно разпространение с прост пример.  
   - Въпроси: Защо е важно да минимизираме функцията на загуба? Какво е learning rate?

5. **Обучение на мрежа върху реални данни**  
   - Фокус: Използвайте библиотека като TensorFlow или PyTorch за обучение.  
   - Задача: Обучете проста мрежа за класификация на ръкописни цифри (MNIST).  
   - Въпроси: Как оценявате качеството на модела? Какво е overfitting?

## 5. Examples
### Пример 1: Forward propagation на един неврон
```python
import numpy as np

inputs = np.array([0.5, 0.3, 0.2])
weights = np.array([0.4, 0.7, 0.2])
bias = 0.1

weighted_sum = np.dot(inputs, weights) + bias
output = max(0, weighted_sum)  # ReLU активация

print("Output:", output)
```

### Пример 2: Обучение на малка мрежа с PyTorch
```python
import torch
import torch.nn as nn
import torch.optim as optim

class SimpleNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(3, 4)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(4, 1)
    
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        return x

model = SimpleNN()
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

# Примерни данни
inputs = torch.tensor([[0.5, 0.3, 0.2]])
labels = torch.tensor([[1.0]])

# Обучение
optimizer.zero_grad()
outputs = model(inputs)
loss = criterion(outputs, labels)
loss.backward()
optimizer.step()

print("Loss:", loss.item())
```

### Пример 3: Използване на функция за загуба
Функцията за загуба измерва разликата между предсказани и реални стойности, например средно квадратична грешка (MSE).

## 6. Common Pitfalls
- **Прекалено сложна архитектура за малки данни** – води до overfitting. Решение: използвайте по-малки модели или регуляризация.
- **Неправилен избор на функция за активиране** – например Sigmoid в скрити слоеве може да доведе до изчезващ градиент.
- **Лошо инициализиране на теглата** – може да забави обучението или да го направи нестабилно.
- **Прекалено висок learning rate** – води до нестабилно обучение и прескачане на оптимума.
- **Игнориране на нормализация на входните данни** – може да забави или провали обучението.

## 7. Short Retrieval Quiz
1. Каква е ролята на функцията за активиране в невронната мрежа?  
2. Какво представлява backpropagation?  
3. Защо е важно да минимизираме функцията на загуба?  
4. Какво е overfitting и как може да се избегне?  
5. Какво означава epoch в контекста на обучение на мрежа?  
6. Как теглата влияят на изхода на неврона?  
7. Кои са основните слоеве в една невронна мрежа?

## 8. Quick Recap
- Невронните мрежи са компютърни модели, вдъхновени от биологичния мозък, използвани за обработка на сложни данни.  
- Основните компоненти са неврони, слоеве, тегла и функции за активиране.  
- Forward propagation изчислява изхода, а backpropagation оптимизира теглата чрез минимизиране на функцията на загуба.  
- Изборът на архитектура, функции и параметри влияе силно върху качеството на модела.  
- Практическото обучение изисква работа с реални данни и инструменти като TensorFlow или PyTorch.  
- Често срещани грешки включват overfitting, лош избор на функции и параметри.  
- Разбирането на тези основи е ключово за разработка на ефективни AI системи.

## 9. Spaced Review Plan

| Време след учене | Промпт за преглед                                         |
|------------------|-----------------------------------------------------------|
| 1 ден            | Обяснете ролята на функцията за активиране и backpropagation. |
| 3 дни            | Опишете основните компоненти на една невронна мрежа.       |
| 1 седмица        | Направете кратък код за forward propagation на малка мрежа. |
| 1 месец          | Обсъдете как да избегнете overfitting при обучение на мрежа. |