# DPO_and_preference_optimization

## 1. Activate Prior Knowledge

- Какво разбирате под термина „оптимизация“ в контекста на машинно обучение и изкуствен интелект?
- Как бихте дефинирали „предпочитания“ в системи, които вземат решения или генерират изходи?
- Какви примери на системи, които се адаптират според обратна връзка от потребители, познавате?

## 2. Overview

Direct Preference Optimization (DPO) е метод за обучение на модели, който използва предпочитания, изразени чрез сравнения, за да оптимизира поведението на системата. Вместо да се опитва да моделира абсолютни оценки или награди, DPO се фокусира върху това да научи кои изходи са предпочитани пред други, което е по-естествен и често по-надежден начин за обучение на модели, особено в сложни задачи като генериране на текст или препоръчителни системи.

В по-широк контекст, DPO се вписва в семейството на методите за оптимизация с обратна връзка, които използват човешки или автоматизирани оценки, за да подобрят качеството на изхода. Този подход е особено важен в ситуации, където е трудно да се дефинира точна функция за награда, но е възможно да се събират сравнителни предпочитания.

Предпочитателната оптимизация (preference optimization) е ключова за създаването на AI системи, които са по-съобразени с човешките нужди и етични стандарти, тъй като позволява модели да се обучават директно върху това, което хората намират за полезно или желателно.

## 3. Key Concepts

- **Direct Preference Optimization (DPO)** – метод за обучение, който използва двойки сравнения, за да научи модел да предпочита един изход пред друг, без да изисква експлицитна наградна функция. Може да се мисли като „обучение чрез избор“.
- **Preference Data** – данни, които съдържат сравнителни оценки, например „Изход A е по-добър от Изход B“. Това е основата за DPO и други preference-based методи.
- **Reward Modeling** – процес на създаване на функция, която оценява качеството на изхода. В DPO тази функция се учи индиректно чрез предпочитания, а не директно.
- **Policy Optimization** – оптимизиране на поведението на модела (политиката) спрямо наградна функция или предпочитания. В DPO това се прави директно чрез preference data.
- **Human-in-the-Loop (HITL)** – включване на човешка обратна връзка в процеса на обучение, често чрез събиране на предпочитания, което е критично за DPO.
- **Pairwise Comparison** – метод за събиране на preference data, където два изхода се сравняват и се избира по-добрият, подобно на „битка между двама“ в спорт.

## 4. Step-by-step Learning Path

1. **Запознайте се с основите на reinforcement learning и supervised learning**
   - Фокус: Разберете разликите между двата подхода и как се използват за обучение на модели.
   - Задача: Прочетете кратко въведение в RL и SL, направете диаграма на основните компоненти.
   - Въпроси: Какво е основното различие между RL и SL? Защо RL е подходящ за оптимизация на поведение?

2. **Изучете концепцията за preference data и pairwise comparisons**
   - Фокус: Разберете как се събират и използват данни за предпочитания.
   - Задача: Създайте малък набор от pairwise preference данни за текстови отговори.
   - Въпроси: Какви са предимствата на pairwise comparisons пред абсолютните оценки? Какви проблеми могат да възникнат при събирането на preference data?

3. **Разгледайте алгоритми за Direct Preference Optimization**
   - Фокус: Научете как DPO използва preference data за обучение на модели.
   - Задача: Имплементирайте опростена версия на DPO върху малък модел (например за класификация).
   - Въпроси: Как DPO оптимизира модела директно? Какво е предимството на DPO пред класическото RL?

4. **Практикувайте интеграция на DPO в реални AI системи**
   - Фокус: Внедрете DPO в система за генериране на текст или препоръчителна система.
   - Задача: Съберете preference data от потребители и използвайте DPO за подобряване на модела.
   - Въпроси: Какви предизвикателства срещнахте при събирането на данни? Как оценихте подобрението?

5. **Анализирайте етични и практични аспекти на preference optimization**
   - Фокус: Разгледайте потенциалните пристрастия и ограничения на системите, обучени с preference data.
   - Задача: Напишете кратък анализ на рисковете и възможностите при използване на DPO.
   - Въпроси: Как може да се избегне пристрастие в preference data? Какви са етичните импликации?

## 5. Examples

### Пример 1: Оптимизация на чатбот чрез DPO

```python
# Псевдокод за DPO обучение с preference data
for (output_a, output_b, preferred) in preference_dataset:
    # Изчисляване на вероятност, че preferred е по-добър
    p = model.prob_prefer(output_a, output_b)
    loss = -log(p if preferred == output_a else 1 - p)
    model.optimize(loss)
```

### Пример 2: Система за препоръки с preference optimization

- Потребителите сравняват два продукта и избират предпочитания.
- Моделът се обучава да предсказва по-добрия продукт, подобрявайки препоръките.

### Пример 3: Обучение на езиков модел с човешки предпочитания

- Човек оценява два генерирани от модела отговора.
- Моделът се оптимизира директно да предпочита по-добрия отговор, без да се използва експлицитна наградна функция.

## 6. Common Pitfalls

- **Недостатъчно или пристрастни preference данни** – малък или едностранчив набор от предпочитания води до лошо обобщение и пристрастия.
- **Прекалено усложнени модели без достатъчно данни** – DPO изисква достатъчно preference data, за да бъде ефективен.
- **Игнориране на човешкия фактор** – липсата на качествена човешка обратна връзка намалява качеството на оптимизацията.
- **Смесване на абсолютни оценки с preference data** – това може да доведе до несъответствия в обучението.
- **Пренебрегване на етични аспекти** – оптимизацията по предпочитания може да засили нежелани пристрастия или да игнорира малцинствени мнения.

## 7. Short Retrieval Quiz

1. Какво представлява Direct Preference Optimization?
2. Защо pairwise comparisons са полезни при preference optimization?
3. Каква е ролята на human-in-the-loop в DPO?
4. Как DPO се различава от традиционното reinforcement learning?
5. Кои са основните рискове при използване на preference data?
6. Какво е reward modeling и как се свързва с DPO?
7. Какви са типичните грешки при имплементация на preference optimization?

## 8. Quick Recap

- DPO използва сравнителни предпочитания за директна оптимизация на модели.
- Preference data се събира чрез pairwise comparisons, което е по-естествено и надеждно.
- DPO е част от по-широкия контекст на reinforcement learning и human-in-the-loop системи.
- Качеството и обемът на preference data са критични за успеха на оптимизацията.
- Етичните и практични аспекти са ключови при внедряване на preference-based системи.
- DPO позволява модели да се адаптират по-добре към човешките нужди и оценки.
- Често срещани грешки включват пристрастия в данните и неправилна интеграция на обратна връзка.

## 9. Spaced Review Plan

| Time After Learning | Review Prompt                                  |
|---------------------|------------------------------------------------|
| 1 day               | Обяснете основната идея на DPO с ваши думи.   |
| 3 days              | Дайте пример за събиране на preference data.  |
| 1 week              | Опишете разликите между DPO и традиционното RL.|
| 1 month             | Анализирайте потенциалните рискове при DPO.   |