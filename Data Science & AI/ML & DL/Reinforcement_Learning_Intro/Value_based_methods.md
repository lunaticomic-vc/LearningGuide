# Value_based_methods

## 1. Activate Prior Knowledge
- Какво разбирате под „стойност“ (value) в контекста на изкуствения интелект и вземането на решения?
- Как бихте описали ролята на оценката на действията в системи за обучение с подсилване (Reinforcement Learning)?
- Можете ли да си представите как една програма може да избира оптимално действие, базирано на предишен опит?

## 2. Overview
Value-based methods са основен клас алгоритми в областта на обучението с подсилване (Reinforcement Learning, RL), които се фокусират върху оценката на стойността на действията или състоянията, за да определят най-добрите решения. Тези методи изграждат модели, които оценяват колко „добро“ е едно действие в дадено състояние, без да моделират директно динамиката на средата.

Те са ключови за системи, които трябва да вземат последователни решения в динамични и неопределени среди, като роботи, автономни превозни средства или интелигентни агенти в игри. Value-based подходите позволяват на агента да научи оптимална политика чрез максимизиране на очакваната кумулативна награда.

Тези методи се вписват в по-широката рамка на RL, където целта е да се намери политика, която максимизира дългосрочната стойност. За разлика от policy-based методите, value-based подходите се фокусират върху оценката на функции за стойност, които след това се използват за извличане на оптимални действия.

## 3. Key Concepts
- **Value Function (Функция на стойността)** – Оценява колко добра е дадена позиция (състояние) или действие в контекста на бъдещи награди. Може да се разглежда като „прогноза“ за бъдещата полза.
- **Q-Function (Q-функция)** – Специален вид value function, която оценява стойността на конкретно действие в конкретно състояние. Мислете за нея като за карта, която казва „колко е полезно да направя това действие тук“.
- **Bellman Equation (Белманово уравнение)** – Основно уравнение, което дефинира връзката между стойностите на състоянията и действията, позволявайки итеративно подобряване на оценките.
- **Policy (Политика)** – Правило или функция, която избира действие на базата на текущото състояние. В value-based методите, политиката се извлича от стойностните функции.
- **Exploration vs Exploitation (Изследване срещу използване)** – Баланс между опитването на нови действия за събиране на информация и използването на вече научените най-добри действия.
- **Temporal Difference Learning (Обучение с временна разлика)** – Метод за обновяване на стойностните функции, използвайки разликата между последователни оценки, което ускорява ученето.

## 4. Step-by-step Learning Path
1. **Разберете основите на Reinforcement Learning и ролята на стойностните функции**  
   - Задача: Прочетете и обяснете на колега какво представлява стойностната функция и как тя се използва за вземане на решения.  
   - Въпроси: Какво измерва стойностната функция? Защо е важна в RL?

2. **Изучете Bellman уравнението и неговото значение**  
   - Задача: Запишете Bellman уравнението за стойностна функция и обяснете всяка част.  
   - Въпроси: Как Bellman уравнението помага за итеративното подобряване на стойностите?

3. **Практикувайте с Q-Learning алгоритъма**  
   - Задача: Имплементирайте прост Q-Learning агент за среда с дискретни състояния и действия (например Gridworld).  
   - Въпроси: Как се обновява Q-стойността? Как агентът избира действия?

4. **Изследвайте баланса между exploration и exploitation**  
   - Задача: Добавете ε-greedy стратегия към вашия Q-Learning агент и наблюдавайте ефекта върху ученето.  
   - Въпроси: Какво е ε-greedy? Как влияе на резултатите?

5. **Разгледайте разширения и оптимизации на value-based методите**  
   - Задача: Проучете Deep Q-Networks (DQN) и опишете как невронните мрежи се използват за апроксимация на Q-функцията.  
   - Въпроси: Какво предимство дава използването на дълбоки мрежи? Как се справят с големи пространства от състояния?

## 5. Examples
### Пример 1: Q-Learning за Gridworld
```python
import numpy as np

# Дискретни състояния и действия
states = range(16)
actions = ['up', 'down', 'left', 'right']
Q = np.zeros((16, 4))  # Q-таблица

def choose_action(state, epsilon=0.1):
    if np.random.rand() < epsilon:
        return np.random.choice(len(actions))
    else:
        return np.argmax(Q[state])

def update_Q(state, action, reward, next_state, alpha=0.1, gamma=0.9):
    best_next = np.max(Q[next_state])
    Q[state, action] += alpha * (reward + gamma * best_next - Q[state, action])
```

### Пример 2: Bellman Equation (формула)
\[
Q(s,a) = \mathbb{E}\left[ r + \gamma \max_{a'} Q(s', a') \mid s, a \right]
\]

Тук \(r\) е наградата, \(\gamma\) – дисконт факторът, \(s'\) – следващото състояние.

### Пример 3: ε-greedy стратегия
```python
def epsilon_greedy_policy(Q, state, epsilon=0.1):
    if np.random.rand() < epsilon:
        return np.random.choice(len(Q[state]))
    else:
        return np.argmax(Q[state])
```

## 6. Common Pitfalls
- **Прекалено ранно експлоатиране (exploitation)** – Ако агентът не изследва достатъчно, може да се заклещи в подоптимални решения. Използвайте стратегии като ε-greedy, за да избегнете това.
- **Неправилна настройка на параметрите (learning rate, discount factor)** – Твърде голям или малък learning rate може да забави или дестабилизира обучението.
- **Игнориране на корелацията между последователни опити** – В класически Q-Learning това може да доведе до нестабилност; използването на replay buffer (в DQN) помага.
- **Недостатъчна апроксимация на стойностната функция при големи пространства** – Табличните методи не са приложими при сложни задачи, затова е необходимо използване на функции за апроксимация като невронни мрежи.
- **Пренебрегване на конвергенцията** – Не всички value-based методи гарантират конвергенция при всякакви условия; важно е да се спазват предпоставките и да се следи обучението.

## 7. Short Retrieval Quiz
1. Какво представлява Q-функцията и каква е нейната роля?
2. Какво описва Bellman уравнението?
3. Какво е ε-greedy стратегия и защо е важна?
4. Как се обновява стойността на Q в Q-Learning?
5. Защо value-based методите са подходящи за задачи с последователни решения?
6. Какво е основното предизвикателство при използването на таблични Q-методи в големи пространства от състояния?
7. Какво означава балансът между exploration и exploitation?

## 8. Quick Recap
- Value-based methods оценяват стойността на действия или състояния, за да намерят оптимална политика.
- Q-функцията е ключов инструмент, който свързва състояния и действия с очакваната награда.
- Bellman уравнението дефинира итеративния процес на обновяване на стойностите.
- Балансът между изследване и използване е критичен за ефективно обучение.
- Q-Learning е класически алгоритъм, който използва тези принципи за обучение без модел.
- При големи и сложни задачи се използват апроксимации като Deep Q-Networks.
- Често срещани грешки включват лоша настройка на параметри и недостатъчно изследване.

## 9. Spaced Review Plan

| Време след учене | Прегледен въпрос/задача                                   |
|------------------|-----------------------------------------------------------|
| 1 ден            | Обяснете с прости думи какво представлява Q-функцията.    |
| 3 дни            | Напишете Bellman уравнението и обяснете ролята му.        |
| 1 седмица        | Имплементирайте ε-greedy стратегия и я тествайте.         |
| 1 месец          | Сравнете value-based методите с policy-based методите.    |