# Hallucination_control_and_confidence_calibration

## 1. Activate Prior Knowledge

- Какво разбирате под термина „халуцинация“ в контекста на изкуствения интелект и защо може да бъде проблем?
- Как бихте дефинирали „калибриране на увереността“ при модели за машинно обучение?
- Как мислите, че контролът на халюцинациите и калибрирането на увереността влияят върху надеждността на AI системите?

## 2. Overview

Халуцинациите в AI системите са грешни или измислени отговори, които моделите генерират, въпреки че изглеждат убедителни. Те представляват сериозен проблем, особено в приложения, където точността и надеждността са критични, като медицинска диагностика, юридически съвети или автоматизирани системи за вземане на решения.

Контролът на халюцинациите се стреми да намали или елиминира тези грешки чрез различни техники – от архитектурни подобрения до филтриране и постобработка на изхода. В същото време, калибрирането на увереността е процес, при който системата коригира своите прогнози за вероятността, че даден отговор е верен. Това позволява на потребителите и други системи да разчитат по-добре на изхода и да вземат информирани решения.

Тези два аспекта са тясно свързани и са част от по-широката тема за надеждност и доверие в AI. Те се интегрират в pipeline-ите на съвременните AI системи, подпомагайки по-безопасното и ефективно използване на автоматизираните решения.

## 3. Key Concepts

- **Hallucination** – Неправилна, измислена или несъответстваща на реалността информация, генерирана от AI модел. Може да се сравни с „фалшив спомен“ в човешката памет.
- **Confidence Calibration** – Процес на настройване на вероятностните оценки на модела така, че те да отговарят на реалната точност. Аналогично на това, когато човек оценява колко е сигурен в даден отговор.
- **Overconfidence** – Когато моделът дава висока увереност на грешен отговор, което води до подвеждащи резултати.
- **Underconfidence** – Когато моделът е прекалено предпазлив и дава ниска увереност дори на правилни отговори.
- **Post-hoc Calibration** – Техники, прилагани след обучението на модела, за да се коригират увереностите, например Platt Scaling или Isotonic Regression.
- **Uncertainty Estimation** – Оценка на степента на несигурност в предсказанията, която подпомага калибрирането и контрол на халюцинациите.
- **Human-in-the-loop** – Включване на човешки експерт за проверка и корекция на изхода на AI, особено при съмнителни или нискоуверени отговори.

## 4. Step-by-step Learning Path

1. **Разберете как се появяват халюцинациите в AI модели**
   - Фокус: Проучете причините за халюцинации в големи езикови модели (LLMs).
   - Задача: Прочетете статия за халюцинации и идентифицирайте основните фактори.
   - Въпроси: Какво предизвиква халюцинации? Защо те са по-чести при определени задачи?

2. **Изучете основите на калибрирането на увереността**
   - Фокус: Научете как се измерва и оценява калибрирането (например чрез reliability diagrams).
   - Задача: Използвайте библиотека като scikit-learn, за да визуализирате калибрирането на прост класификатор.
   - Въпроси: Какво показва reliability diagram? Какво означава добре калибриран модел?

3. **Практикувайте техники за контрол на халюцинациите**
   - Фокус: Запознайте се с методи като филтриране на изхода, използване на външни бази данни и human-in-the-loop.
   - Задача: Имплементирайте прост филтър, който маркира отговори с ниска увереност.
   - Въпроси: Как филтрирането помага за намаляване на халюцинациите? Какво е ролята на човека в процеса?

4. **Прилагайте post-hoc calibration техники**
   - Фокус: Научете как да използвате Platt Scaling или Isotonic Regression.
   - Задача: Калибрирайте вероятностите на модел и сравнете резултатите преди и след.
   - Въпроси: Какви са предимствата на post-hoc calibration? Кога е подходящо да се използва?

5. **Интегрирайте контрол и калибриране в реална AI система**
   - Фокус: Създайте pipeline, който комбинира предсказания, оценка на увереност и филтриране.
   - Задача: Разработете прототип, който предупреждава потребителя при ниска увереност.
   - Въпроси: Как интеграцията подобрява надеждността? Какви са предизвикателствата при внедряване?

## 5. Examples

### Пример 1: Филтриране на нискоуверени отговори

```python
def filter_low_confidence(predictions, confidences, threshold=0.7):
    filtered = []
    for pred, conf in zip(predictions, confidences):
        if conf >= threshold:
            filtered.append(pred)
        else:
            filtered.append("Uncertain - requires review")
    return filtered

preds = ["Paris", "London", "Mars"]
confs = [0.95, 0.60, 0.80]
print(filter_low_confidence(preds, confs))
# Изход: ['Paris', 'Uncertain - requires review', 'Mars']
```

### Пример 2: Калибриране с Platt Scaling (scikit-learn)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

X, y = make_classification(n_samples=1000, n_features=20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf = LogisticRegression()
clf.fit(X_train, y_train)

calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid')
calibrated_clf.fit(X_train, y_train)

print("Before calibration:", clf.predict_proba(X_test)[:5])
print("After calibration:", calibrated_clf.predict_proba(X_test)[:5])
```

### Пример 3: Human-in-the-loop за проверка на отговори

- AI генерира отговори с оценка на увереност.
- Отговори с увереност под 0.75 се изпращат за човешка проверка.
- Човек маркира отговорите като верни или грешни, което помага за допълнително обучение.

## 6. Common Pitfalls

- **Игнориране на калибрирането** – Много разработчици приемат вероятностите на модела за абсолютна истина, което води до грешни решения.
- **Прекалено висока увереност при грешки** – Моделите често са overconfident, което затруднява откриването на халюцинации.
- **Недостатъчно внимание към контекста** – Халуцинациите могат да се появят по-често при специфични типове входни данни или задачи.
- **Прекалено агресивно филтриране** – Може да се загуби полезна информация, ако се блокират твърде много отговори.
- **Липса на човешка проверка при критични приложения** – Автоматизацията без човешки контрол може да доведе до сериозни грешки.

## 7. Short Retrieval Quiz

1. Какво представлява халюцинацията в AI модел?
2. Защо е важно калибрирането на увереността?
3. Какво означава overconfidence и как влияе на системата?
4. Какви техники могат да се използват за post-hoc calibration?
5. Как human-in-the-loop помага за контрол на халюцинациите?
6. Какво показва reliability diagram?
7. Какво е риска при прекалено агресивно филтриране на отговори?

## 8. Quick Recap

- Халуцинациите са грешни, но убедителни отговори, които компрометират надеждността на AI.
- Калибрирането на увереността настройва вероятностите на модела да отразяват реалната точност.
- Overconfidence е честа причина за подвеждащи резултати.
- Техники като Platt Scaling и human-in-the-loop са ефективни за подобряване на надеждността.
- Контролът на халюцинациите и калибрирането са ключови за безопасното използване на AI.
- Внедряването изисква баланс между автоматизация и човешка проверка.
- Визуализации като reliability diagrams помагат за оценка на калибрирането.

## 9. Spaced Review Plan

| Време след учене | Промпт за преглед                                      |
|-------------------|-------------------------------------------------------|
| 1 ден             | Обяснете с прости думи какво е халюцинация и защо е проблем. |
| 3 дни             | Опишете какво представлява калибрирането на увереността и защо е важно. |
| 1 седмица         | Прегледайте основните техники за контрол на халюцинациите и калибриране. |
| 1 месец           | Приложете наученото, като анализирате калибрирането на модел и предложите метод за контрол на халюцинации. |