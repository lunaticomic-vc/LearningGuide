# Context_windows

## 1. Activate Prior Knowledge
- Какво представлява „контекст“ в рамките на изкуствения интелект и обработката на естествен език?
- Защо ограничението на паметта или капацитета за обработка е важно при проектирането на AI модели?
- Как мислите, че системите за машинно обучение използват предишна информация, за да подобрят текущите си предсказания?

## 2. Overview
Контекстните прозорци (context windows) са ключов компонент в архитектурите на съвременните AI системи, особено при модели за обработка на естествен език (NLP). Те определят колко информация от предишния текст или събития моделът може да „види“ и използва при генериране на отговор или предсказание. Това е като да имате ограничен прозорец към миналото, през който системата гледа, за да разбере настоящето.

В по-широката система контекстният прозорец служи за балансиране между капацитета на паметта и ефективността на изчисленията. Ако прозорецът е твърде малък, моделът губи важна информация, което води до по-слаби резултати. Ако е твърде голям, изчислителните ресурси се натоварват и времето за обработка се увеличава. Затова разбирането и оптимизирането на контекстните прозорци е от съществено значение за изграждането на ефективни и надеждни AI системи.

## 3. Key Concepts
- **Context Window** – ограничен обем от последователна информация (например думи или токени), която моделът използва за обработка и вземане на решения. Може да се сравни с краткосрочната памет на човека, която задържа последните няколко елемента от разговор.
- **Token** – основна единица на текст, която моделът обработва (например дума, част от дума или символ). Контекстният прозорец се измерва в брой токени.
- **Sliding Window** – техника, при която прозорецът се „плъзга“ през дълъг текст, за да се обработи последователно цялата информация, без да се загуби контекст.
- **Memory Constraints** – ограниченията в изчислителната памет и време, които влияят на размера на контекстния прозорец.
- **Attention Mechanism** – метод в трансформър моделите, който позволява на модела да „фокусира“ вниманието си върху различни части от контекста, въпреки ограничението на прозореца.

## 4. Step-by-step Learning Path
1. **Разберете какво е токен и как се измерва контекстният прозорец**
   - Задача: Използвайте библиотека като Hugging Face Tokenizers, за да токенизирате няколко изречения и пребройте токените.
   - Въпроси: Какво е токен? Колко токена има в изречението „Аз уча изкуствен интелект“?

2. **Изследвайте как размерът на контекстния прозорец влияе върху производителността на моделите**
   - Задача: Сравнете резултатите на модел с контекстен прозорец 128 токена и 512 токена върху една и съща задача.
   - Въпроси: Как се променя точността? Какво се случва с времето за обработка?

3. **Практикувайте прилагане на sliding window техника**
   - Задача: Напишете скрипт, който разделя дълъг текст на припокриващи се прозорци от 100 токена с припокриване 20 токена.
   - Въпроси: Защо е полезно припокриването? Какво би станало, ако няма припокриване?

4. **Разгледайте ролята на attention механизмите при работа с контекст**
   - Задача: Анализирайте визуализация на attention weights в трансформър модел при различни размери на контекстния прозорец.
   - Въпроси: Как моделът разпределя вниманието си? Как това помага при дълги контексти?

5. **Оптимизирайте контекстния прозорец за конкретна задача**
   - Задача: Изберете задача (напр. чатбот, резюмиране) и експериментирайте с различни размери на прозореца, за да намерите баланс между качество и ефективност.
   - Въпроси: Кой размер е най-подходящ и защо? Какви компромиси правите?

## 5. Examples
### Пример 1: Токенизиране и контекстен прозорец
```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
text = "Аз уча изкуствен интелект."
tokens = tokenizer.tokenize(text)
print(tokens)
print(f"Брой токени: {len(tokens)}")
```

### Пример 2: Sliding window за дълъг текст
```python
def sliding_window(tokens, window_size=100, overlap=20):
    step = window_size - overlap
    windows = []
    for i in range(0, len(tokens), step):
        window = tokens[i:i+window_size]
        if len(window) < window_size:
            break
        windows.append(window)
    return windows

tokens = list(range(250))  # примерни токени
windows = sliding_window(tokens)
print(f"Брой прозорци: {len(windows)}")
```

### Пример 3: Влияние на размера на контекстния прозорец върху производителността
- При по-малък прозорец моделът може да пропусне важна информация от предишен текст.
- При по-голям прозорец се увеличава времето за изчисление и използването на памет.

## 6. Common Pitfalls
- **Игнориране на ограничението на контекстния прозорец** – опит за подаване на твърде дълъг текст без разделяне, което води до загуба на информация.
- **Липса на припокриване при sliding window** – води до загуба на контекст между прозорците.
- **Прекалено голям прозорец без оптимизация** – може да доведе до изчерпване на паметта и забавяне.
- **Подценяване на ролята на attention** – мислене, че само размерът на прозореца е важен, без да се отчита как моделът използва вниманието.
- **Неправилно токенизиране** – различни токенизатори могат да дават различен брой токени за един и същ текст, което влияе на размера на прозореца.

## 7. Short Retrieval Quiz
1. Какво представлява контекстният прозорец в NLP моделите?
2. Защо е важно да има припокриване при sliding window техниката?
3. Какво е токен и как се различава от дума?
4. Как attention механизмът помага при ограничен контекстен прозорец?
5. Какви са рисковете при използване на твърде голям контекстен прозорец?
6. Как контекстният прозорец влияе на производителността и качеството на модела?
7. Как бихте разделили дълъг текст за обработка от модел с ограничен контекст?

## 8. Quick Recap
- Контекстният прозорец определя колко предишна информация моделът може да използва.
- Той се измерва в токени, които са основните единици за обработка.
- Sliding window е техника за обработка на дълги текстове чрез припокриващи се прозорци.
- Размерът на прозореца влияе на точността, времето за обработка и използването на памет.
- Attention механизмът позволява ефективно използване на контекста въпреки ограниченията.
- Оптимизацията на контекстния прозорец е баланс между качество и ресурси.
- Често срещани грешки включват неправилно разделяне на текста и игнориране на паметните ограничения.

## 9. Spaced Review Plan

| Време след изучаване | Промпт за преглед                                      |
|----------------------|-------------------------------------------------------|
| 1 ден                | Обяснете какво е контекстен прозорец и защо е важен.  |
| 3 дни                | Опишете sliding window техниката и нейните предимства.|
| 1 седмица            | Дайте пример как attention механизмът работи с контекст.|
| 1 месец              | Как бихте оптимизирали контекстния прозорец за конкретна задача? |