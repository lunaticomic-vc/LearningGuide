# Decision_trees_and_random_forests

## 1. Activate Prior Knowledge

- Какво представлява процесът на вземане на решения в компютърна система? Можете ли да си представите как една програма би могла да „решава“ задачи, базирайки се на данни?
- Какво знаете за алгоритмите за класификация и регресия? Как биха могли да се използват за прогнозиране или категоризиране на информация?
- Какво представлява ансамбловото обучение и защо може да е по-ефективно от използването на един модел?

## 2. Overview

Дърветата на решения (Decision Trees) са интуитивни и мощни модели за машинно обучение, които имитират човешкия процес на вземане на решения чрез последователни въпроси или условия. Те се използват както за задачи на класификация, така и за регресия, като разделят данните на базата на характеристики, за да достигнат до прогноза или решение.

В по-широката система за изкуствен интелект, дърветата на решения служат като основен строителен блок за разбираеми и интерпретируеми модели. Въпреки това, те могат да бъдат склонни към пренасищане (overfitting), когато са твърде дълбоки или сложни.

Случайните гори (Random Forests) са ансамблов метод, който съчетава множество дървета на решения, обучени върху различни подмножества от данни и характеристики. Този подход намалява риска от пренасищане и повишава общата точност и стабилност на модела, което ги прави изключително популярни в практиката.

## 3. Key Concepts

- **Decision Tree (Дърво на решения)** – Модел, който разделя данните чрез поредица от въпроси или условия, образувайки дървовидна структура, където всеки клон представлява решение, а листата – прогноза.
- **Node (Възел)** – Точка в дървото, където се прави разделяне на данните според определено условие; може да е вътрешен (решаващ) или листен (краен).
- **Overfitting (Пренасищане)** – Ситуация, при която моделът се адаптира твърде много към тренировъчните данни, включително шум и аномалии, и губи способността си да обобщава.
- **Random Forest (Случайна гора)** – Ансамблов метод, който съчетава множество дървета на решения, като всеки модел се обучава върху случайна извадка от данни и характеристики, за да се подобри точността и стабилността.
- **Bootstrap Aggregation (Bagging)** – Техника за създаване на множество тренировъчни подмножества чрез случайно избиране с връщане, използвана за обучение на отделните дървета в случайната гора.
- **Feature Importance (Важност на характеристиките)** – Мярка за това колко дадена характеристика допринася за точността на модела, полезна за разбиране и интерпретиране на резултатите.
- **Entropy / Gini Impurity (Ентропия / Gini несъвършенство)** – Метрики за измерване на чистотата на разделенията в дървото; колкото по-ниска е стойността, толкова по-добро е разделянето.

## 4. Step-by-step Learning Path

1. **Запознаване с основите на дърветата на решения**  
   - Фокус: Разберете структурата на дървото и как се правят разделенията.  
   - Задача: Нарисувайте ръчно дърво на решения за прост проблем (напр. класифициране на плодове по цвят и размер).  
   - Въпроси: Какво представлява възел? Какво е листо в дървото?

2. **Изучаване на критериите за разделяне**  
   - Фокус: Научете за ентропията и Gini несъвършенството.  
   - Задача: Изчислете ентропия и Gini за даден набор от данни и изберете най-доброто разделяне.  
   - Въпроси: Какво означава ниска ентропия? Защо използваме тези метрики?

3. **Практическо изграждане на дърво с Python (scikit-learn)**  
   - Фокус: Имплементирайте дърво на решения върху реален набор от данни (например Iris).  
   - Задача: Обучете модел, визуализирайте дървото и интерпретирайте резултатите.  
   - Въпроси: Какви са предимствата на визуализацията? Какво показват листата?

4. **Въведение в случайните гори**  
   - Фокус: Разберете принципа на bagging и случайния избор на характеристики.  
   - Задача: Обучете случайна гора на същия набор от данни и сравнете резултатите с единичното дърво.  
   - Въпроси: Как случайните гори намаляват пренасищането? Какво е bagging?

5. **Оценка и тълкуване на моделите**  
   - Фокус: Научете как да оценявате точността, важността на характеристиките и да избягвате пренасищане.  
   - Задача: Използвайте cross-validation и feature importance в случайната гора.  
   - Въпроси: Какво показва важността на характеристиките? Как да разпознаете пренасищане?

## 5. Examples

### Пример 1: Дърво на решения за класификация с Python

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

iris = load_iris()
X, y = iris.data, iris.target

clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X, y)

plt.figure(figsize=(12,8))
plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()
```

### Пример 2: Случайна гора за класификация и feature importance

```python
from sklearn.ensemble import RandomForestClassifier
import numpy as np

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

importances = rf.feature_importances_
for name, importance in zip(iris.feature_names, importances):
    print(f"{name}: {importance:.3f}")
```

### Пример 3: Използване на случайна гора за регресия

```python
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

data = fetch_california_housing()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0)

regressor = RandomForestRegressor(n_estimators=100, random_state=0)
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)

print(f"MSE: {mean_squared_error(y_test, y_pred):.3f}")
```

## 6. Common Pitfalls

- **Пренасищане с твърде дълбоки дървета** – Дълбоките дървета могат да научат шума в данните, което води до лоша генерализация. Използвайте параметри като `max_depth` и `min_samples_leaf`.
- **Игнориране на баланса на класовете** – При неравномерни класове, дърветата могат да са пристрастни към по-често срещания клас. Използвайте техники за балансиране или параметри като `class_weight`.
- **Прекалено голям брой дървета в случайната гора** – Въпреки че повече дървета обикновено подобряват точността, прекалено много увеличава времето за обучение и предсказване без значителна полза.
- **Неправилна интерпретация на важността на характеристиките** – Важността показва относителен принос, но не гарантира причинно-следствена връзка.
- **Недостатъчно крос-валидация** – Без подходяща оценка моделът може да изглежда по-добър, отколкото е в реалността.

## 7. Short Retrieval Quiz

1. Какво представлява възел в дървото на решения?
2. Каква е основната разлика между дърво на решения и случайна гора?
3. Какво е пренасищане и как може да се избегне при дървета на решения?
4. Каква роля играе bagging в случайните гори?
5. Коя метрика измерва чистотата на разделенията в дървото?
6. Как може да се интерпретира важността на характеристиките?
7. Какво означава „max_depth“ в контекста на дърветата на решения?

## 8. Quick Recap

- Дърветата на решения моделират процеса на вземане на решения чрез последователни разделения на данните.
- Основните компоненти са възлите, клоновете и листата, които формират дървовидна структура.
- Пренасищането е често срещан проблем, който се контролира чрез параметри като дълбочина и минимален брой проби.
- Случайните гори комбинират множество дървета, обучени върху различни подмножества, за да подобрят точността и стабилността.
- Bagging и случайният избор на характеристики са ключови техники в случайните гори.
- Важността на характеристиките помага за разбиране на влиянието на отделните променливи.
- Правилната оценка и интерпретация на моделите са критични за успешното им приложение.

## 9. Spaced Review Plan

| Време след изучаване | Прегледен въпрос                                      |
|----------------------|------------------------------------------------------|
| 1 ден                | Какво представлява дърво на решения и как се изгражда? |
| 3 дни                | Как случайните гори подобряват стабилността на модела? |
| 1 седмица            | Какво е пренасищане и как да го избегнем при дървета?  |
| 1 месец              | Обяснете ролята на важността на характеристиките в случайните гори. |