# Embedding_models_and_text_chunking

## 1. Activate Prior Knowledge
- Какво представлява векторното представяне на текст и защо е полезно в системите за изкуствен интелект?
- Как бихте разделили дълъг текст на по-малки части, за да улесните обработката му от компютър?
- Какви предизвикателства могат да възникнат при работа с големи текстови корпуси в NLP задачи?

## 2. Overview
Embedding моделите са техники за преобразуване на текстови данни в числови вектори, които запазват семантичната информация на думите или изреченията. Те са основен компонент в съвременните AI системи, тъй като позволяват на компютрите да „разбират“ и обработват естествен език по ефективен начин.

Текстовото chunking (разделяне на текст на части) е процесът на разбиване на дълги текстове на по-малки, управляеми сегменти. Това е особено важно при работа с embedding модели, тъй като те имат ограничения по отношение на дължината на входа и за да се запази контекстът, текстът трябва да бъде разделен интелигентно.

Комбинацията от embedding модели и текстово chunking е ключова за изграждането на системи за търсене, класификация, резюмиране и други NLP приложения. Тя позволява по-добра обработка на големи обеми текст, като същевременно запазва смисъла и контекста.

## 3. Key Concepts
- **Embedding (Вграждане)** – числов вектор, който представя дума, изречение или документ, като улавя семантични и синтактични характеристики. Можете да го си представите като координати на дума в многомерно пространство.
- **Text Chunking (Разделяне на текст)** – процес на разбиване на текст на по-малки части (chunk-ове), които са по-лесни за обработка. Аналогия: нарязване на голям хляб на филийки, за да го ядете по-лесно.
- **Context Window (Контекстно прозорче)** – максималната дължина на входа, която embedding моделът може да обработи наведнъж.
- **Semantic Similarity (Семантична близост)** – мярка за това колко близки са по смисъл два embedding вектора.
- **Tokenization (Токенизация)** – разделяне на текст на по-малки единици (думи, поддуми, символи), които embedding моделът използва като вход.

## 4. Step-by-step Learning Path
1. **Разберете основите на embedding моделите**
   - Фокус: как се създават и какво представят embedding векторите.
   - Задача: използвайте библиотека като `gensim` или `transformers`, за да създадете embedding за няколко думи.
   - Въпроси: Какво означава, че две думи са „близки“ в embedding пространството? Как embedding моделите улавят семантична информация?

2. **Изучете принципите на текстовото chunking**
   - Фокус: защо и как се разделя текстът на части.
   - Задача: напишете скрипт, който разбива дълъг текст на chunk-ове с фиксиран брой токени.
   - Въпроси: Какви са рисковете при прекалено малки или прекалено големи chunk-ове? Как chunk-овете влияят на контекста?

3. **Комбинирайте embedding и chunking в практическа задача**
   - Фокус: обработка на дълъг текст с embedding модел, използвайки chunking.
   - Задача: създайте pipeline, който разделя текст, генерира embedding за всеки chunk и изчислява семантична близост между chunk-ове.
   - Въпроси: Как да агрегираш embedding-ите от различни chunk-ове? Как chunking влияе на качеството на резултатите?

4. **Оптимизирайте и валидирайте модела**
   - Фокус: подобряване на точността и ефективността.
   - Задача: експериментирайте с различни размери на chunk-ове и embedding модели, оценете резултатите.
   - Въпроси: Как да изберете оптимален размер на chunk? Как да измерите качеството на embedding?

## 5. Examples
### Пример 1: Създаване на embedding за дума с `transformers` (Python)
```python
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

text = "Изкуственият интелект е бъдещето."
encoded_input = tokenizer(text, return_tensors='pt')
with torch.no_grad():
    model_output = model(**encoded_input)

embedding = model_output.last_hidden_state.mean(dim=1).squeeze()
print(embedding)
```

### Пример 2: Разделяне на текст на chunk-ове с фиксиран брой токени
```python
def chunk_text(text, tokenizer, max_tokens=50):
    tokens = tokenizer.tokenize(text)
    chunks = []
    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i:i+max_tokens]
        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)
        chunks.append(chunk_text)
    return chunks

text = "Това е примерен дълъг текст, който ще бъде разделен на по-малки части."
chunks = chunk_text(text, tokenizer)
print(chunks)
```

### Пример 3: Изчисляване на семантична близост между два текста
```python
from sklearn.metrics.pairwise import cosine_similarity

embedding1 = embedding1.unsqueeze(0).numpy()
embedding2 = embedding2.unsqueeze(0).numpy()

similarity = cosine_similarity(embedding1, embedding2)[0][0]
print(f"Семантична близост: {similarity:.4f}")
```

## 6. Common Pitfalls
- **Игнориране на контекста при chunking** – разделянето на текст без внимание към смисловите граници може да доведе до загуба на важна информация.
- **Прекалено големи chunk-ове** – може да надхвърлят лимитите на embedding модела и да доведат до грешки или загуба на ефективност.
- **Прекалено малки chunk-ове** – губи се контекст и семантична цялост, което намалява качеството на embedding.
- **Неправилна агрегация на embedding-ите** – просто усредняване без вземане предвид на важността на chunk-овете може да доведе до неточни представяния.
- **Неоптимална токенизация** – използването на неподходящ tokenizer може да наруши качеството на embedding.

## 7. Short Retrieval Quiz
1. Какво представлява embedding векторът?
2. Защо е важно да се разделя текстът на chunk-ове при използване на embedding модели?
3. Какво е контекстно прозорче (context window)?
4. Какви са рисковете при прекалено големи или прекалено малки chunk-ове?
5. Как може да се измери семантичната близост между два embedding вектора?
6. Какво е токенизация и каква е ролята ѝ?
7. Какво може да се случи, ако не се вземе предвид контекстът при chunking?

## 8. Quick Recap
- Embedding моделите превръщат текст в числови вектори, запазващи смисъла.
- Текстовото chunking разделя дълги текстове на управляеми части, улеснявайки обработката.
- Контекстът и размерът на chunk-овете са критични за качеството на embedding.
- Токенизацията е първата стъпка в подготовката на текст за embedding.
- Семантичната близост се измерва чрез метрики като косинусова близост.
- Комбинирането на embedding и chunking е основа за много NLP приложения.
- Избягвайте прекалено големи или малки chunk-ове и небрежна агрегация на embedding-ите.

## 9. Spaced Review Plan

| Време след учене | Промпт за преговор                                   |
|------------------|-----------------------------------------------------|
| 1 ден            | Обяснете с прости думи какво е embedding и защо chunking е важен. |
| 3 дни            | Опишете процеса на разделяне на текст и как това влияе на embedding качеството. |
| 1 седмица        | Демонстрирайте с пример как да създадете embedding за текст, използвайки chunking. |
| 1 месец          | Анализирайте потенциални грешки при chunking и как да ги избегнете в реални проекти. |