# Evaluation_metrics

## 1. Activate Prior Knowledge
- Какво е основната цел на оценката на производителността в AI системи и софтуерното инженерство?
- Кои метрики сте срещали при оценка на модели за класификация или регресия?
- Как бихте преценили дали един модел е „добър“ или „лош“ в контекста на конкретна задача?

## 2. Overview
Оценъчните метрики са количествени показатели, които измерват колко добре един модел или система изпълнява своята задача. Те са критични за сравняване на различни модели, оптимизиране на параметри и вземане на решения за внедряване. В контекста на AI, метриките служат като обективна основа за оценка на точността, надеждността и ефективността на алгоритмите.

В по-широката система на разработка, оценъчните метрики са част от процеса на валидация и тестване, който гарантира, че моделът не само работи добре върху тренировъчните данни, но и се обобщава успешно върху нови, невиждани примери. Без подходящи метрики, трудно можем да преценим дали подобренията в модела са реални или просто случайни.

Изборът на метрика зависи от конкретната задача – например, при класификация може да използваме точност, прецизност или F1-скор, докато при регресия – средна квадратична грешка. Разбирането на тези метрики и техните ограничения е ключово за правилна интерпретация на резултатите и избягване на подвеждащи изводи.

## 3. Key Concepts
- **Accuracy (Точност)** – Процентът правилно класифицирани примери спрямо общия брой. Аналогия: колко от тестовете сте минали успешно.
- **Precision (Прецизност)** – Колко от предсказаните положителни случаи са наистина положителни. Мислете за това като за „колко сте точни, когато казвате ‘да’“.
- **Recall (Отзивчивост)** – Колко от всички реални положителни случаи сте успели да откриете. Аналогично на „колко добре хващате всички важни случаи“.
- **F1-score** – Хармонично средно между прецизност и отзивчивост, балансирано представяне при несбалансирани класове.
- **Mean Squared Error (MSE)** – Средната от квадратичните разлики между предсказаните и реалните стойности при регресия. Представете си го като средната „болка“ от грешките.
- **Confusion Matrix (Матрица на объркване)** – Таблица, която показва броя на правилните и грешни предсказания, разделени по класове. Полезна за визуализиране на грешките.
- **ROC Curve и AUC** – Графика и числова стойност, които измерват способността на класификатора да различава класовете при различни прагове.

## 4. Step-by-step Learning Path
1. **Фокус:** Разбиране на основните метрики за класификация (accuracy, precision, recall, F1).  
   **Задача:** Изчислете тези метрики ръчно за малък набор от данни с известни предсказания и истински етикети.  
   **Въпроси:** Какво показва високата прецизност, но ниския recall? Защо F1-score е полезен при несбалансирани класове?

2. **Фокус:** Изучаване на матрицата на объркване и нейното значение.  
   **Задача:** Постройте confusion matrix за резултатите от класификатор и анализирайте грешките.  
   **Въпроси:** Какви типове грешки има моделът? Как те влияят на избора на метрика?

3. **Фокус:** Метрики за регресия – MSE, MAE, R².  
   **Задача:** Напишете кратък Python скрипт, който изчислява MSE и MAE за дадени предсказания и истински стойности.  
   **Въпроси:** Кога MSE е по-подходяща от MAE? Какво означава R²?

4. **Фокус:** Разбиране на ROC и AUC за бинарна класификация.  
   **Задача:** Използвайте библиотека (например scikit-learn) за генериране на ROC крива и изчисляване на AUC.  
   **Въпроси:** Какво означава AUC близо до 0.5? Как ROC крива помага при избор на праг?

5. **Фокус:** Избор на подходяща метрика според конкретна задача и бизнес цели.  
   **Задача:** Анализирайте казус с несбалансирани класове и изберете най-подходящата метрика.  
   **Въпроси:** Защо точността може да бъде подвеждаща при несбалансирани данни? Какви са алтернативите?

## 5. Examples
### Пример 1: Изчисляване на метрики за класификация
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

y_true = [0, 1, 1, 0, 1, 0, 1]
y_pred = [0, 0, 1, 0, 1, 1, 1]

print("Accuracy:", accuracy_score(y_true, y_pred))
print("Precision:", precision_score(y_true, y_pred))
print("Recall:", recall_score(y_true, y_pred))
print("F1 Score:", f1_score(y_true, y_pred))
```

### Пример 2: Матрица на объркване
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Предсказани')
plt.ylabel('Истински')
plt.show()
```

### Пример 3: Изчисляване на MSE за регресия
```python
from sklearn.metrics import mean_squared_error

y_true = [3.0, -0.5, 2.0, 7.0]
y_pred = [2.5, 0.0, 2.1, 7.8]

mse = mean_squared_error(y_true, y_pred)
print("Mean Squared Error:", mse)
```

## 6. Common Pitfalls
- Използване на точност при силно несбалансирани класове, което може да доведе до подвеждащи високи стойности.
- Игнориране на контекста на задачата при избора на метрика – например, при медицински диагнози е по-важно да се минимизират фалшивите отрицателни.
- Прекомерно фокусиране върху една метрика без да се разглеждат други аспекти на модела.
- Неправилно интерпретиране на ROC крива и AUC, особено при много класови задачи.
- Забравяне да се проверят метриките върху отделен тестов набор, което води до overfitting.

## 7. Short Retrieval Quiz
1. Какво измерва прецизността в класификацията?
2. Кога F1-score е по-подходяща метрика от точността?
3. Какво показва матрицата на объркване?
4. Каква е разликата между MSE и MAE?
5. Какво означава AUC стойност близка до 1?
6. Защо точността може да е подвеждаща при несбалансирани класове?
7. Какво е основното предназначение на оценъчните метрики?

## 8. Quick Recap
- Оценъчните метрики измерват качеството на модели и системи.
- Изборът на метрика зависи от задачата и контекста.
- Точност, прецизност, отзивчивост и F1-score са ключови за класификация.
- MSE, MAE и R² са основни метрики за регресия.
- Матрицата на объркване визуализира грешките на модела.
- ROC крива и AUC помагат при оценка на бинарни класификатори.
- Избягвайте подвеждащи интерпретации и винаги проверявайте метриките върху отделен тестов набор.

## 9. Spaced Review Plan

| Време след учене | Промпт за преглед                                      |
|------------------|-------------------------------------------------------|
| 1 ден            | Обяснете с прости думи какво е F1-score и кога се използва. |
| 3 дни            | Изчислете ръчно прецизност и отзивчивост за малък пример.  |
| 1 седмица        | Опишете разликите между MSE и MAE и кога да използвате всяка. |
| 1 месец          | Дайте пример за избор на метрика при несбалансирани класове и обяснете защо. |