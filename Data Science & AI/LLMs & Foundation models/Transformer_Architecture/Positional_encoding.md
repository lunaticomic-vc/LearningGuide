# Positional_encoding

## 1. Activate Prior Knowledge
- Какво представлява последователната информация в контекста на обработката на естествен език или времеви серии?
- Защо стандартните невронни мрежи трудно улавят реда на елементите в последователност?
- Как бихте представили позицията на дума в изречение, ако използвате само векторни представяния?

## 2. Overview
Positional encoding е техника, използвана в трансформър архитектурите, за да се въведе информация за позицията на елементите в последователност. Тъй като трансформърите не обработват данните по ред, както рекурентните невронни мрежи, те се нуждаят от допълнителен механизъм, който да им каже къде се намира всеки елемент в реда.

Тази информация е критична, защото значението на думите или символите често зависи от тяхната позиция. Без позиционна информация, моделът би трябвало да третира входните елементи като множество без подредба, което би довело до загуба на контекст и смисъл.

Positional encoding се добавя към входните вектори преди да влязат в трансформър слоя, като по този начин позволява на модела да използва както съдържанието, така и позицията на всеки елемент при обработката.

## 3. Key Concepts
- **Positional Encoding** – Вектор, който кодира позицията на елемент в последователност, добавен към входния ембединг, за да се запази информация за реда.
- **Sinusoidal Encoding** – Често използван метод за позиционно кодиране, който използва синус и косинус функции с различни честоти, за да създаде уникални позиционни вектори.
- **Learnable Positional Embeddings** – Алтернативен подход, при който позиционните вектори се тренират заедно с модела, вместо да се изчисляват по формула.
- **Self-Attention** – Механизъм в трансформърите, който позволява на всеки елемент да се свързва с всеки друг, като позиционното кодиране помага да се разбере редът на тези връзки.
- **Sequence Order** – Подредбата на елементите в последователност, която е ключова за разбиране на контекст и смисъл.

## 4. Step-by-step Learning Path
1. **Разберете защо е нужна позиционна информация в трансформърите**  
   - Фокус: Прочетете за архитектурата на трансформър и ролята на self-attention.  
   - Задача: Направете кратко резюме защо трансформърите не могат да използват реда без позиционна информация.  
   - Въпроси: Какво е self-attention? Защо редът на думите е важен?

2. **Изучете синусоидалното позиционно кодиране**  
   - Фокус: Разберете формулите за изчисляване на позиционните вектори.  
   - Задача: Напишете функция, която генерира позиционни вектори за дадена дължина и размер на ембединг.  
   - Въпроси: Как се използват синус и косинус във формулата? Защо се използват различни честоти?

3. **Сравнете синусоидалните и learnable позиционни ембединг**  
   - Фокус: Разберете предимствата и недостатъците на двата подхода.  
   - Задача: Имплементирайте learnable positional embeddings в прост трансформър модел.  
   - Въпроси: Кога learnable embeddings могат да са по-добри? Какво е предимството на синусоидалното кодиране?

4. **Интегрирайте позиционното кодиране в трансформър модел**  
   - Фокус: Добавете позиционното кодиране към входните ембединг в реален код.  
   - Задача: Модифицирайте съществуващ трансформър код, за да включва позиционни вектори.  
   - Въпроси: Как позиционното кодиране влияе на резултатите? Как се комбинира с входните ембединг?

5. **Анализирайте ефекта от позиционното кодиране**  
   - Фокус: Тествайте модела с и без позиционна информация.  
   - Задача: Проведете експеримент и сравнете точността на модела.  
   - Въпроси: Как се променя поведението на модела? Защо позиционното кодиране подобрява резултатите?

## 5. Examples

### Синусоидално позиционно кодиране на Python
```python
import numpy as np

def get_positional_encoding(seq_len, d_model):
    positional_encoding = np.zeros((seq_len, d_model))
    for pos in range(seq_len):
        for i in range(0, d_model, 2):
            positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))
            if i + 1 < d_model:
                positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i)/d_model)))
    return positional_encoding

# Пример за 10 позиции и размер на ембединг 16
pos_encoding = get_positional_encoding(10, 16)
print(pos_encoding.shape)  # (10, 16)
```

### Добавяне на позиционно кодиране към входни ембединг в PyTorch
```python
import torch
import torch.nn as nn

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return x

# Използване:
# embedding = nn.Embedding(vocab_size, d_model)
# pos_encoder = PositionalEncoding(d_model)
# x = embedding(input_ids)
# x = pos_encoder(x)
```

## 6. Common Pitfalls
- **Пренебрегване на позиционното кодиране** – без него трансформърът губи информация за реда, което води до лоши резултати.
- **Грешно изчисляване на позиционните вектори** – неправилно използване на формулите за синус и косинус може да доведе до неразпознаваеми позиции.
- **Добавяне на позиционното кодиране след self-attention слоя** – позиционното кодиране трябва да се добави преди трансформър слоя, за да бъде ефективно.
- **Използване на еднакви позиционни вектори за различни дължини на последователности** – позиционното кодиране трябва да покрива максималната дължина, с която работи моделът.
- **Забравяне да се използва `.register_buffer` при PyTorch имплементация** – това гарантира, че позиционните вектори не се тренират, но се пренасят на GPU.

## 7. Short Retrieval Quiz
1. Защо трансформърите се нуждаят от позиционно кодиране?  
2. Какви функции се използват в синусоидалното позиционно кодиране?  
3. Каква е разликата между learnable и синусоидално позиционно кодиране?  
4. Къде в трансформър архитектурата се добавя позиционното кодиране?  
5. Как позиционното кодиране влияе на self-attention механизма?  
6. Какво може да се случи, ако позиционното кодиране не покрива максималната дължина на входа?  
7. Как се гарантира, че позиционните вектори не се променят по време на обучение в PyTorch?

## 8. Quick Recap
- Трансформърите не обработват последователностите по ред, затова се нуждаят от позиционна информация.  
- Positional encoding въвежда информация за позицията на всеки елемент чрез вектори, добавяни към входните ембединг.  
- Синусоидалното позиционно кодиране използва синус и косинус функции с различни честоти за уникални позиционни вектори.  
- Алтернативно, learnable positional embeddings се тренират заедно с модела.  
- Позиционното кодиране подобрява способността на модела да улавя контекст и ред в последователностите.  
- Правилната имплементация и позициониране на позиционното кодиране е ключово за успешна работа на трансформърите.  
- Експерименталното сравнение с и без позиционно кодиране показва значително подобрение в резултатите.

## 9. Spaced Review Plan

| Време след изучаване | Промпт за преговор                                         |
|----------------------|------------------------------------------------------------|
| 1 ден                | Обяснете защо трансформърите се нуждаят от позиционно кодиране. |
| 3 дни                | Опишете как се изчислява синусоидалното позиционно кодиране.    |
| 1 седмица            | Сравнете learnable и синусоидално позиционно кодиране.          |
| 1 месец              | Как позиционното кодиране влияе на self-attention механизма?     |