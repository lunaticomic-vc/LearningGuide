# Loss_functions_and_optimizers

## 1. Activate Prior Knowledge

- Какво представлява целта на машинното обучение и как измерваме успеха на един модел?
- Какво е ролята на грешката (error) в процеса на обучение на невронна мрежа?
- Как бихте обяснили защо е важно да имаме метод за актуализиране на параметрите на модела?

## 2. Overview

В сърцето на обучението на изкуствени невронни мрежи и други модели стои концепцията за **функция на загуба (loss function)** и **оптимизатор (optimizer)**. Функцията на загуба измерва колко добре или зле моделът предсказва спрямо истинските данни, като количествено изразява грешката. Тя е основният сигнал, който казва на модела как да се подобри.

Оптимизаторът е алгоритъмът, който използва тази грешка, за да актуализира параметрите на модела (например теглата в невронна мрежа). Той търси минимизиране на функцията на загуба чрез итеративни стъпки, като по този начин подобрява представянето на модела.

Тези две компоненти са фундаментални за обучението на модели в машинното обучение и дълбокото учене. Без добре дефинирана функция на загуба и ефективен оптимизатор, моделът няма да се научи да прави точни прогнози, което прави разбирането им ключово за всеки професионалист в AI.

## 3. Key Concepts

- **Loss Function (Функция на загуба)** – Математическа функция, която измерва разликата между предсказанията на модела и истинските стойности. Аналогия: като "термометър" за грешката на модела.
- **Optimizer (Оптимизатор)** – Алгоритъм, който коригира параметрите на модела, за да минимизира функцията на загуба. Може да се сравни с навигатор, който помага на модела да намери най-краткия път към по-добри резултати.
- **Gradient Descent (Градиентен спуск)** – Основен метод за оптимизация, който използва производните на функцията на загуба, за да направи малки стъпки към минималната стойност.
- **Learning Rate (Скорост на учене)** – Параметър, който определя размера на стъпката при всяка итерация на оптимизатора. Ако е твърде голям, може да пропуснем минимума; ако е твърде малък – обучението ще е бавно.
- **Overfitting (Претоварване)** – Ситуация, в която моделът се научава твърде добре на тренировъчните данни, но не се обобщава добре върху нови данни. Функцията на загуба и оптимизаторът трябва да се избират внимателно, за да се избегне това.
- **Batch Size (Размер на партида)** – Брой примери, използвани за изчисляване на градиента в една итерация. По-малки партиди водят до по-шумни, но по-чести актуализации.

## 4. Step-by-step Learning Path

1. **Разбиране на функцията на загуба**
   - Фокус: Изучете различни видове функции на загуба (MSE, Cross-Entropy).
   - Задача: Имплементирайте MSE (Mean Squared Error) функция на загуба за регресионен проблем.
   - Въпроси: Какво измерва MSE? Защо е важно да минимизираме функцията на загуба?

2. **Запознаване с градиентния спуск**
   - Фокус: Разберете как градиентният спуск използва производната на функцията на загуба.
   - Задача: Напишете ръчно една итерация на градиентен спуск за прост модел с един параметър.
   - Въпроси: Какво означава градиентът в контекста на оптимизацията? Как влияе learning rate?

3. **Изследване на различни оптимизатори**
   - Фокус: Научете разликите между SGD, Adam, RMSprop.
   - Задача: Сравнете представянето на SGD и Adam върху малък набор от данни.
   - Въпроси: Кога е подходящо да използваме Adam вместо SGD? Какво е предимството на адаптивните оптимизатори?

4. **Практика с tuning на learning rate и batch size**
   - Фокус: Разберете влиянието на тези хиперпараметри върху обучението.
   - Задача: Проведете експерименти с различни learning rates и batch sizes и анализирайте резултатите.
   - Въпроси: Как се отразява голям learning rate на конвергенцията? Защо batch size влияе на стабилността на обучението?

5. **Интегриране на функция на загуба и оптимизатор в реален проект**
   - Фокус: Свържете теорията с практиката в рамките на ML pipeline.
   - Задача: Използвайте TensorFlow или PyTorch, за да обучите модел с избрана функция на загуба и оптимизатор.
   - Въпроси: Как се дефинира функция на загуба в избраната библиотека? Как се задава оптимизатор?

## 5. Examples

### Пример 1: MSE за регресия (Python)

```python
import numpy as np

def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

y_true = np.array([3.0, -0.5, 2.0, 7.0])
y_pred = np.array([2.5, 0.0, 2.0, 8.0])

loss = mse_loss(y_true, y_pred)
print(f"MSE Loss: {loss}")
```

### Пример 2: Cross-Entropy за класификация (PyTorch)

```python
import torch
import torch.nn as nn

loss_fn = nn.CrossEntropyLoss()
outputs = torch.tensor([[2.0, 1.0, 0.1]])
labels = torch.tensor([0])

loss = loss_fn(outputs, labels)
print(f"Cross-Entropy Loss: {loss.item()}")
```

### Пример 3: Използване на Adam оптимизатор в TensorFlow

```python
import tensorflow as tf

model = tf.keras.Sequential([tf.keras.layers.Dense(1)])
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
loss_fn = tf.keras.losses.MeanSquaredError()

x = tf.constant([[1.0], [2.0], [3.0]])
y = tf.constant([[2.0], [4.0], [6.0]])

with tf.GradientTape() as tape:
    predictions = model(x)
    loss = loss_fn(y, predictions)

gradients = tape.gradient(loss, model.trainable_variables)
optimizer.apply_gradients(zip(gradients, model.trainable_variables))

print(f"Loss after one optimization step: {loss.numpy()}")
```

## 6. Common Pitfalls

- **Избор на неподходяща функция на загуба:** Например, използване на MSE за класификационен проблем води до лоши резултати. Винаги съобразявайте типа задача.
- **Твърде висок learning rate:** Може да предизвика нестабилно обучение или прескачане на минимума.
- **Игнориране на нормализацията на данните:** Това може да забави или блокира конвергенцията.
- **Прекалено малък batch size:** Води до много шумни градиенти и нестабилно обучение.
- **Липса на monitoring на функцията на загуба:** Без наблюдение не можете да разберете дали моделът се учи правилно.
- **Пренебрегване на overfitting:** Ниска загуба на тренировъчните данни не гарантира добри резултати на нови данни.

## 7. Short Retrieval Quiz

1. Каква е ролята на функцията на загуба в обучението на модел?
2. Какво представлява градиентният спуск?
3. Защо learning rate е важен параметър?
4. Кога е подходящо да използваме Cross-Entropy loss?
5. Какво прави оптимизаторът Adam различен от стандартния SGD?
6. Как batch size влияе на процеса на обучение?
7. Какво е overfitting и как може да се избегне?

## 8. Quick Recap

- Функцията на загуба измерва колко добре моделът предсказва.
- Оптимизаторът актуализира параметрите, за да минимизира загубата.
- Градиентният спуск е основен метод за оптимизация.
- Learning rate контролира размера на стъпките при обучение.
- Изборът на функция на загуба трябва да съответства на задачата.
- Batch size влияе на стабилността и скоростта на обучение.
- Внимателното наблюдение и настройка предотвратяват overfitting и нестабилност.

## 9. Spaced Review Plan

| Време след учене | Преговорна задача                                    |
|------------------|-----------------------------------------------------|
| 1 ден            | Обяснете с прости думи ролята на функцията на загуба и оптимизатора. |
| 3 дни            | Имплементирайте и сравнете MSE и Cross-Entropy loss върху малък набор от данни. |
| 1 седмица        | Проведете експеримент с различни оптимизатори и learning rates. Анализирайте резултатите. |
| 1 месец          | Напишете кратко резюме с примери за това как функция на загуба и оптимизатор влияят на обучението на модел в реален проект. |