# Data_pipelines_and_retraining

## 1. Activate Prior Knowledge
- Какво представлява един типичен процес на обработка на данни в софтуерна система или AI модел?
- Защо е важно да поддържаме моделите актуални с нови данни?
- Какви са възможните последици от използването на остарели данни или модели в продукционна среда?

## 2. Overview
Data pipelines (данни тръбопроводи) са системи, които автоматизират събирането, трансформацията и преноса на данни от източници към крайни потребители или модели. Те осигуряват надежден и последователен поток на данни, който е критичен за изграждането и поддържането на AI системи.

Retraining (претрениране) е процесът на обновяване на машинен модел с нови данни, за да се адаптира към промени в средата или данните. Това е ключова практика за поддържане на точността и релевантността на модела във времето.

В по-широк контекст, data pipelines и retraining са част от жизнения цикъл на AI системите, гарантирайки, че те остават ефективни и надеждни при реални условия. Без тях, моделите бързо остаряват и могат да доведат до грешни решения.

## 3. Key Concepts
- **Data Pipeline** – Автоматизиран процес за събиране, обработка и доставка на данни. Може да се сравни с фабрика, където суровите материали (данни) се преработват и подготвят за използване.
- **ETL (Extract, Transform, Load)** – Основен модел в data pipelines, включва извличане на данни, тяхната трансформация и зареждане в целева система.
- **Model Retraining** – Процес на обновяване на машинен модел с нови данни, за да се подобри или запази неговата производителност.
- **Data Drift** – Промяна в разпределението на входните данни с времето, която може да намали точността на модела.
- **Concept Drift** – Промяна в зависимостите между входните данни и целевата променлива, което изисква адаптация на модела.
- **Automation** – Използване на скриптове и инструменти за минимизиране на човешката намеса в data pipelines и retraining.
- **Monitoring** – Наблюдение на производителността на модела и качеството на данните, за да се открият проблеми навреме.

## 4. Step-by-step Learning Path
1. **Запознаване с основите на data pipelines**
   - Фокус: Разберете какво представлява data pipeline и ролята му.
   - Задача: Създайте прост pipeline, който извлича CSV файл, трансформира данните (напр. филтрира редове) и ги записва в нов файл.
   - Въпроси: Какво е ETL? Защо е важно автоматизирането на този процес?

2. **Изучаване на концепцията за retraining**
   - Фокус: Разберете кога и защо е необходимо да се претренира модел.
   - Задача: Използвайте малък ML модел (напр. регресия) и го претренирайте с добавени нови данни.
   - Въпроси: Какво е data drift? Какво е concept drift?

3. **Автоматизация на pipeline и retraining**
   - Фокус: Научете как да автоматизирате pipeline и retraining с помощта на скриптове или инструменти (например Apache Airflow).
   - Задача: Настройте автоматичен pipeline, който всеки ден зарежда нови данни и стартира retraining.
   - Въпроси: Какви са предимствата на автоматизацията? Какво може да се обърка при автоматично retraining?

4. **Мониторинг и поддръжка**
   - Фокус: Разберете как да следите качеството на данните и производителността на модела.
   - Задача: Имплементирайте мониторинг, който алармира при спад в точността на модела.
   - Въпроси: Какво е важно да се следи в pipeline? Как да реагираме на аларми?

## 5. Examples

### Пример 1: Прост ETL pipeline с Python
```python
import pandas as pd

# Extract
data = pd.read_csv('raw_data.csv')

# Transform
filtered_data = data[data['value'] > 10]

# Load
filtered_data.to_csv('processed_data.csv', index=False)
```

### Пример 2: Претренировка на модел с нови данни (scikit-learn)
```python
from sklearn.linear_model import LinearRegression
import pandas as pd

# Зареждане на първоначални данни
data_old = pd.read_csv('data_old.csv')
X_old = data_old[['feature1', 'feature2']]
y_old = data_old['target']

# Обучение на модел
model = LinearRegression()
model.fit(X_old, y_old)

# Зареждане на нови данни
data_new = pd.read_csv('data_new.csv')
X_new = data_new[['feature1', 'feature2']]
y_new = data_new['target']

# Претренировка с новите данни
X_combined = pd.concat([X_old, X_new])
y_combined = pd.concat([y_old, y_new])
model.fit(X_combined, y_combined)
```

### Пример 3: Автоматизация с Apache Airflow (псевдокод)
```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def etl_task():
    # Код за ETL

def retrain_task():
    # Код за retraining

dag = DAG('data_pipeline_retraining', start_date=datetime(2024, 1, 1), schedule_interval='@daily')

t1 = PythonOperator(task_id='etl', python_callable=etl_task, dag=dag)
t2 = PythonOperator(task_id='retrain', python_callable=retrain_task, dag=dag)

t1 >> t2
```

## 6. Common Pitfalls
- **Игнориране на data drift и concept drift** – Моделът остарява и губи точност, ако не се следи и не се претренира.
- **Ръчно изпълнение на pipeline** – Води до грешки и забавя процеса; автоматизацията е ключова.
- **Недостатъчен мониторинг** – Без мониторинг не се откриват навреме проблеми с данните или модела.
- **Претренировка с нискокачествени данни** – Въвежда шум и влошава модела.
- **Прекалено често retraining** – Може да доведе до overfitting или ненужно натоварване на ресурсите.
- **Липса на версиониране на данни и модели** – Трудно е да се проследи историята и да се възстанови предишно състояние.

## 7. Short Retrieval Quiz
1. Какво представлява data pipeline?
2. Каква е ролята на ETL в един pipeline?
3. Какво е model retraining и защо е важно?
4. Какво означава data drift?
5. Какви са рисковете при ръчно изпълнение на pipeline?
6. Как може да се автоматизира retraining процесът?
7. Защо е необходим мониторинг на модела?

## 8. Quick Recap
- Data pipelines осигуряват автоматизиран поток от данни за AI системи.
- ETL е ключов процес в pipeline, включващ извличане, трансформация и зареждане на данни.
- Retraining обновява моделите с нови данни, за да поддържа тяхната точност.
- Data drift и concept drift са основни причини за необходимостта от retraining.
- Автоматизацията и мониторингът са критични за надеждността и ефективността на pipeline и retraining.
- Лошото качество на данните и липсата на контрол могат да доведат до сериозни проблеми.
- Практическото прилагане включва изграждане на ETL, обучение и претренировка, автоматизация и наблюдение.

## 9. Spaced Review Plan

| Време след учене | Промпт за преглед                                  |
|------------------|---------------------------------------------------|
| 1 ден            | Обяснете какво е data pipeline и защо е важен.   |
| 3 дни            | Кога и защо се налага retraining на модел?       |
| 1 седмица        | Опишете основните стъпки в автоматизиран pipeline.|
| 1 месец          | Идентифицирайте и обяснете common pitfalls при data pipelines и retraining. |