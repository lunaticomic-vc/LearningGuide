# Feature_selection_and_dimensionality_reduction

## 1. Activate Prior Knowledge
- Какво е значението на характеристиките (features) в контекста на машинното обучение и изкуствения интелект?
- Защо може да е проблематично да използваме всички налични характеристики в даден набор от данни?
- Какви са възможните последици от висока размерност на данните върху ефективността и точността на моделите?

## 2. Overview
Feature selection и dimensionality reduction са две ключови техники в предварителната обработка на данни, които целят да подобрят ефективността и качеството на машинните модели чрез намаляване на броя на характеристиките. Те помагат да се елиминират излишните, шумни или корелирани характеристики, които могат да влошат представянето на модела или да увеличат изчислителните разходи.

Feature selection се фокусира върху избора на подмножество от оригиналните характеристики, които са най-важни за задачата, без да променя самите характеристики. Dimensionality reduction, от друга страна, трансформира данните в по-нискоизмерно пространство, като запазва максимално възможната информация, често чрез линейни или нелинейни трансформации.

Тези техники са критични в контекста на големи данни и сложни AI системи, където излишната информация може да доведе до overfitting, по-бавна обработка и трудности при интерпретация. Те са основна част от pipeline-а за машинно обучение и софтуерно инженерство, особено при работа с високоизмерни данни като текст, изображения и биоинформатика.

## 3. Key Concepts
- **Feature Selection** – Процесът на избор на най-важните характеристики от оригиналния набор, подобряващ точността и намаляващ изчислителната сложност. Може да се сравни с избиране на най-важните инструменти от куфар с инструменти.
- **Dimensionality Reduction** – Трансформация на данните в по-нискоизмерно пространство, запазвайки максимално информацията. Представете си компресиране на снимка без загуба на видими детайли.
- **Overfitting** – Ситуация, при която моделът се адаптира твърде много към тренировъчните данни, включително шум, и губи способността си да обобщава.
- **Curse of Dimensionality** – Проблем, при който увеличаването на броя на характеристиките води до експоненциално нарастване на обема на пространството, правейки данните по-разредени и модели по-трудни за обучение.
- **Filter Methods** – Техники за feature selection, които оценяват характеристиките независимо от модела, например чрез корелация или статистически тестове.
- **Wrapper Methods** – Използват модел за оценка на подмножества от характеристики, като търсят оптимален набор чрез итеративно обучение.
- **Embedded Methods** – Вградени в алгоритъма за обучение, като Lasso регресия, която автоматично извършва селекция на характеристики.
- **Principal Component Analysis (PCA)** – Популярна техника за dimensionality reduction, която намира линейни комбинации на характеристиките, максимизиращи вариацията.
- **t-SNE и UMAP** – Нелинейни техники за визуализация и намаляване на размерността, подходящи за сложни структури в данните.

## 4. Step-by-step Learning Path
1. **Разбиране на проблема с висока размерност**
   - Фокус: Изучете как размерността влияе на моделирането и защо е проблем.
   - Задача: Анализирайте набор от данни с много характеристики и измерете времето за обучение на прост модел.
   - Въпроси: Какво е curse of dimensionality? Как размерността влияе на overfitting?

2. **Изучаване на методи за feature selection**
   - Фокус: Научете разликите между filter, wrapper и embedded методи.
   - Задача: Използвайте filter метод (напр. корелация) за избор на характеристики в Python.
   - Въпроси: Кога е подходящ filter метод? Как wrapper методите използват модела?

3. **Практика с dimensionality reduction**
   - Фокус: Разберете принципите на PCA и приложението му.
   - Задача: Прилагайте PCA върху реален набор от данни и визуализирайте резултатите.
   - Въпроси: Какво представляват главните компоненти? Как избираме броя на компонентите?

4. **Изучаване на нелинейни техники**
   - Фокус: Запознайте се с t-SNE и UMAP за визуализация и намаляване на размерността.
   - Задача: Визуализирайте сложен набор от данни с t-SNE.
   - Въпроси: Как t-SNE различава структури в данните? Кога е по-подходящ от PCA?

5. **Интегриране в ML pipeline**
   - Фокус: Включете feature selection и dimensionality reduction в реален ML проект.
   - Задача: Създайте pipeline с feature selection и PCA, обучете модел и сравнете резултатите.
   - Въпроси: Как намаляването на размерността влияе на точността? Как да избегнем загуба на важна информация?

## 5. Examples

### Пример 1: Filter метод с корелация в Python
```python
import pandas as pd
from sklearn.datasets import load_boston

data = load_boston()
df = pd.DataFrame(data.data, columns=data.feature_names)
correlations = df.corrwith(pd.Series(data.target))
selected_features = correlations[correlations.abs() > 0.5].index.tolist()
print("Избрани характеристики:", selected_features)
```

### Пример 2: PCA за намаляване на размерността
```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

X = df[selected_features]
X_scaled = StandardScaler().fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
print("Обяснена вариация от първите 2 компоненти:", sum(pca.explained_variance_ratio_))
```

### Пример 3: t-SNE визуализация
```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

plt.scatter(X_tsne[:,0], X_tsne[:,1], c=data.target, cmap='viridis')
plt.colorbar()
plt.title("t-SNE визуализация")
plt.show()
```

## 6. Common Pitfalls
- Избор на твърде малък брой характеристики, което води до загуба на важна информация.
- Използване на dimensionality reduction без стандартизация на данните.
- Прекалено доверие на filter методи без проверка на взаимодействия между характеристики.
- Пренебрегване на влиянието на feature selection върху интерпретируемостта на модела.
- Използване на t-SNE за големи набори от данни без подходящи параметри, което води до забавяне и нестабилни резултати.

## 7. Short Retrieval Quiz
1. Каква е разликата между feature selection и dimensionality reduction?
2. Какво представлява curse of dimensionality?
3. Кои са трите основни типа методи за feature selection?
4. Какво прави PCA с данните?
5. Защо е важно да стандартизираме данните преди PCA?
6. Кога е подходящо да използваме t-SNE?
7. Как dimensionality reduction може да помогне за предотвратяване на overfitting?

## 8. Quick Recap
- Feature selection избира важни характеристики, докато dimensionality reduction трансформира даните.
- Високата размерност може да влоши представянето на моделите и да увеличи изчислителните разходи.
- Filter, wrapper и embedded са основните подходи за feature selection.
- PCA е най-популярната линейна техника за намаляване на размерността.
- Нелинейни методи като t-SNE са полезни за визуализация и сложни структури.
- Стандартизация на данните е задължителна преди прилагането на много техники.
- Интегрирането на тези техники в ML pipeline подобрява ефективността и точността.

## 9. Spaced Review Plan

| Време след учене | Прегледен въпрос/задача                                    |
|------------------|------------------------------------------------------------|
| 1 ден            | Обяснете разликата между feature selection и dimensionality reduction. |
| 3 дни            | Изберете подходящ метод за feature selection за даден набор от данни. |
| 1 седмица        | Прилагайте PCA върху нов набор от данни и интерпретирайте резултатите. |
| 1 месец          | Създайте ML pipeline с feature selection и dimensionality reduction и оценете ефекта върху модела. |