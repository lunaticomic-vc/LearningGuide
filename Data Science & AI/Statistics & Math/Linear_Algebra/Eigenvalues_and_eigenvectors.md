# Eigenvalues_and_eigenvectors

## 1. Activate Prior Knowledge
- Какво представлява линейната трансформация и как тя променя вектори в пространството?
- Как бихте описали значението на собствените стойности и собствени вектори в контекста на обработка на данни или машинно обучение?
- Можете ли да предвидите как собствените стойности и вектори могат да помогнат при намаляване на размерността на данни или при стабилността на системи?

## 2. Overview
Собствените стойности (eigenvalues) и собствени вектори (eigenvectors) са фундаментални концепции в линейната алгебра, които описват как дадена матрица трансформира вектори в пространството. Те ни позволяват да разберем основните направления, по които трансформацията действа просто — чрез скалиране, без промяна на посоката.

В контекста на изкуствения интелект и софтуерното инженерство, тези понятия са ключови за анализ на системи, оптимизация, компютърна графика и машинно обучение. Например, при Principal Component Analysis (PCA) собствените вектори определят основните компоненти, а собствените стойности показват тяхната важност.

Разбирането на собствените стойности и вектори помага при стабилността на динамични системи, решаване на диференциални уравнения и при моделиране на сложни процеси. Те са инструмент за разбиране на структурата на данните и поведението на алгоритми.

## 3. Key Concepts
- **Eigenvalue (Собствена стойност)** – скалар, който показва колко се разтяга или свива вектор при линейна трансформация. Мислете за него като за коефициент на усилване по определена посока.
- **Eigenvector (Собствен вектор)** – вектор, който не променя посоката си при прилагане на матрицата, а само се умножава по собствената стойност. Представете си го като оста на въртене или симетрия.
- **Characteristic Polynomial (Характеристичен полином)** – полином, чийто корени са собствените стойности на матрицата. Той е инструмент за намиране на тези стойности.
- **Diagonalization (Диагонализация)** – процесът на преобразуване на матрица в диагонална форма чрез собствените ѝ вектори, което опростява изчисленията.
- **Spectral Theorem (Спектрален теорем)** – твърди, че симетричните матрици могат да бъдат диагонализирани с ортогонална матрица, което е важно за стабилност и числени методи.
- **Multiplicities (Мултиплитети)** – броят пъти, в които дадена собствена стойност се повтаря (алгебрична) и размерът на пространството на съответните собствени вектори (геометрична).

## 4. Step-by-step Learning Path
1. **Разберете дефиницията на собствена стойност и собствен вектор**  
   - Фокус: Какво означава уравнението \(A\mathbf{v} = \lambda \mathbf{v}\).  
   - Задача: Вземете малка 2x2 матрица и намерете собствените ѝ стойности ръчно.  
   - Въпроси: Какво означава, ако \(\lambda = 0\)? Какво се случва с \(\mathbf{v}\)?

2. **Научете как да намирате характеристичния полином**  
   - Фокус: Изчисляване на \(\det(A - \lambda I) = 0\).  
   - Задача: Изчислете характеристичния полином на дадена 3x3 матрица.  
   - Въпроси: Защо детерминантата трябва да е нула?

3. **Практикувайте намиране на собствени вектори**  
   - Фокус: Решаване на системи \( (A - \lambda I)\mathbf{v} = 0 \).  
   - Задача: Намерете собствени вектори за дадени собствените стойности.  
   - Въпроси: Какво означава, ако системата има безкрайно много решения?

4. **Изучете диагонализация и приложението ѝ**  
   - Фокус: Преобразуване на матрица в диагонална форма.  
   - Задача: Диагонализирайте 2x2 матрица с реални собствени стойности.  
   - Въпроси: Защо диагонализацията улеснява изчисленията?

5. **Разгледайте приложения в машинно обучение и системи**  
   - Фокус: PCA, стабилност на системи, динамични модели.  
   - Задача: Имплементирайте PCA на малък набор от данни с помощта на numpy.  
   - Въпроси: Как собствените стойности влияят на избора на главни компоненти?

## 5. Examples

### Пример 1: Намиране на собствените стойности и вектори на матрица
Нека  
\[
A = \begin{bmatrix} 4 & 1 \\ 2 & 3 \end{bmatrix}
\]

Характеристичен полином:  
\[
\det(A - \lambda I) = (4-\lambda)(3-\lambda) - 2 = \lambda^2 - 7\lambda + 10 = 0
\]

Корени: \(\lambda_1 = 5, \lambda_2 = 2\)

За \(\lambda_1 = 5\), решаваме \((A - 5I)\mathbf{v} = 0\):  
\[
\begin{bmatrix} -1 & 1 \\ 2 & -2 \end{bmatrix} \mathbf{v} = 0 \Rightarrow \mathbf{v} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
\]

### Пример 2: PCA с Python
```python
import numpy as np

# Данни: 3 точки в 2D
X = np.array([[2.5, 2.4],
              [0.5, 0.7],
              [2.2, 2.9]])

# Центриране
X_centered = X - np.mean(X, axis=0)

# Ковариационна матрица
cov_matrix = np.cov(X_centered, rowvar=False)

# Собствени стойности и вектори
eigvals, eigvecs = np.linalg.eig(cov_matrix)

print("Eigenvalues:", eigvals)
print("Eigenvectors:\n", eigvecs)
```

### Пример 3: Диагонализация
Матрица  
\[
B = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}
\]

Собствени стойности: 3 и 2  
Собствени вектори: \(\mathbf{v}_1 = [1,0]^T\), \(\mathbf{v}_2 = [1,-1]^T\)  
Диагонализация:  
\[
P = [\mathbf{v}_1 \quad \mathbf{v}_2], \quad D = \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}
\]

## 6. Common Pitfalls
- **Смесване на собствена стойност със собствен вектор** – собствената стойност е скалар, собственият вектор е посока.  
- **Игнориране на мултиплитети** – някои собствени стойности могат да са повтарящи се, което влияе на броя на независимите собствени вектори.  
- **Опит за диагонализация на недиагонализируеми матрици** – не всички матрици могат да бъдат диагонализирани; трябва да се проверят условията.  
- **Неправилно центриране на данни при PCA** – липсата на центриране води до грешни собствени стойности и вектори.  
- **Пренебрегване на числени грешки** – при изчисления с плаваща запетая собствените стойности могат да имат малки отклонения.

## 7. Short Retrieval Quiz
1. Какво е уравнението, което дефинира собствените стойности и вектори?  
2. Как се намира характеристичният полином на матрица?  
3. Какво означава, ако собствената стойност е нула?  
4. Защо диагонализацията е полезна?  
5. Какво представлява спектралният теорем?  
6. Как собствените стойности и вектори се използват в PCA?  
7. Кога една матрица не може да бъде диагонализирана?

## 8. Quick Recap
- Собствените стойности и вектори описват скалиране и посока при линейна трансформация.  
- Характеристичният полином дава собствените стойности като корени.  
- Собствените вектори се намират чрез решаване на хомогенни системи.  
- Диагонализацията опростява изчисления и анализ на матрици.  
- В машинното обучение собствените стойности и вектори са основа за техники като PCA.  
- Спектралният теорем гарантира диагонализация на симетрични матрици с ортогонални собствени вектори.  
- Важно е да се избягват често срещани грешки като неправилно центриране и пренебрегване на мултиплитети.

## 9. Spaced Review Plan

| Време след учене | Промпт за преглед                                  |
|------------------|---------------------------------------------------|
| 1 ден            | Опишете уравнението за собствена стойност и вектор. |
| 3 дни            | Намерете характеристичния полином на 2x2 матрица.  |
| 1 седмица        | Обяснете как се използват собствените стойности в PCA. |
| 1 месец          | Диагонализирайте матрица и интерпретирайте резултатите. |