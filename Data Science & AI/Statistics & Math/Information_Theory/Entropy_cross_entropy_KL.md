# Entropy_cross_entropy_KL

## 1. Activate Prior Knowledge
- Какво знаете за вероятностните разпределения и тяхната роля в моделирането на несигурност?
- Как бихте описали "разлика" или "разстояние" между две вероятностни разпределения?
- Защо в машинното обучение е важно да измерваме колко добре един модел предсказва истинските данни?

## 2. Overview
Ентропията, крос-ентропията и KL дивергенцията са фундаментални понятия в теорията на информацията и статистиката, които намират широко приложение в изкуствения интелект и софтуерното инженерство. Те измерват количеството несигурност, разликата между разпределения и ефективността на модели за прогнозиране.

Ентропията описва средната несигурност или "изненада" в случайна променлива, докато крос-ентропията измерва колко добре едно вероятностно разпределение (например модел) описва друго (истинските данни). KL дивергенцията (Kullback-Leibler divergence) е мярка за разликата между две разпределения и често се използва за оптимизиране на модели чрез минимизиране на тази разлика.

В контекста на AI системи, тези понятия са ключови за обучение на модели, особено при задачи като класификация, където целта е да се минимизира разликата между предсказаното и истинското разпределение на класовете. Разбирането им позволява по-добро проектиране и диагностика на модели.

## 3. Key Concepts
- **Entropy (Ентропия)** – Мярка за средната несигурност или изненада в случайна променлива. Може да се мисли като "колко информация" съдържа съобщение. Например, при монета с равни вероятности ентропията е максимална.
- **Cross-Entropy (Крос-ентропия)** – Мярка за това колко добре едно разпределение (модел) описва друго (истинско разпределение). Представете си я като "средната изненада", когато използвате модел, който не е напълно точен.
- **KL Divergence (KL дивергенция)** – Мярка за разликата между две вероятностни разпределения. Тя е асиметрична и може да се разглежда като "разход" на информация при използване на едно разпределение вместо друго.
- **Probability Distribution (Вероятностно разпределение)** – Функция, която описва вероятностите за всички възможни изходи на случайна променлива.
- **Logarithm in Information Theory (Логаритъм в теорията на информацията)** – Използва се за измерване на информацията в битове или натове, като логаритъмът на вероятността определя "теглото" на събитие.

## 4. Step-by-step Learning Path
1. **Разберете основите на вероятностите и логаритмите**
   - Фокус: Прегледайте вероятностни разпределения и логаритми.
   - Задача: Изчислете ентропията на монета с вероятности 0.5 и 0.9 за глава.
   - Въпроси: Какво е ентропия? Как логаритъмът влияе на изчисленията?

2. **Изучете формулата и интерпретацията на ентропията**
   - Фокус: Формула за ентропия и нейното значение.
   - Задача: Изчислете ентропията на дискретно разпределение с три събития.
   - Въпроси: Защо ентропията е максимална при равномерно разпределение?

3. **Разберете крос-ентропията и връзката ѝ с ентропията**
   - Фокус: Формула и практическа употреба на крос-ентропия.
   - Задача: Изчислете крос-ентропия между две разпределения.
   - Въпроси: Как крос-ентропията измерва качеството на модел?

4. **Изследвайте KL дивергенцията и нейното значение**
   - Фокус: Формула, асиметрия и приложение на KL дивергенция.
   - Задача: Изчислете KL дивергенция между две прости разпределения.
   - Въпроси: Защо KL дивергенцията не е симетрична?

5. **Приложете знанията в машинно обучение**
   - Фокус: Използване на крос-ентропия и KL дивергенция като функции за загуба.
   - Задача: Имплементирайте крос-ентропия като функция за загуба в Python.
   - Въпроси: Как минимизирането на крос-ентропия подобрява модела?

## 5. Examples

**Пример 1: Ентропия на монета**
```python
import math

p = 0.5
entropy = -p * math.log2(p) - (1 - p) * math.log2(1 - p)
print(f"Entropy of fair coin: {entropy} bits")
```

**Пример 2: Крос-ентропия между истинско и предсказано разпределение**
```python
import numpy as np

true_dist = np.array([0.7, 0.3])
pred_dist = np.array([0.6, 0.4])

cross_entropy = -np.sum(true_dist * np.log(pred_dist))
print(f"Cross-Entropy: {cross_entropy}")
```

**Пример 3: KL дивергенция между две разпределения**
```python
kl_div = np.sum(true_dist * np.log(true_dist / pred_dist))
print(f"KL Divergence: {kl_div}")
```

## 6. Common Pitfalls
- **Смесване на крос-ентропия и KL дивергенция** – Крос-ентропия включва ентропията на истинското разпределение, докато KL дивергенцията е разликата между крос-ентропия и ентропия.
- **Използване на логаритъм с база различна от 2 без корекция** – В теорията на информацията логаритмите обикновено са с база 2 (битове), но в някои библиотеки се използва естествен логаритъм (натове).
- **Неправилно нормализиране на вероятностите** – Входните вектори трябва да са валидни вероятностни разпределения (сумата да е 1).
- **Игнориране на числени стабилности** – Логаритъм на нула е неопределен, затова често се добавя малка стойност (epsilon) при изчисления.

## 7. Short Retrieval Quiz
1. Какво измерва ентропията?
2. Каква е разликата между крос-ентропия и KL дивергенция?
3. Защо KL дивергенцията е асиметрична?
4. Какво означава максимална ентропия при разпределение?
5. Как крос-ентропията се използва в машинното обучение?
6. Какво трябва да имат предвид при изчисляване на логаритми в тези формули?
7. Какво представлява вероятностно разпределение?

## 8. Quick Recap
- Ентропията измерва средната несигурност в случайна променлива.
- Крос-ентропията измерва колко добре едно разпределение описва друго.
- KL дивергенцията е мярка за разликата между две вероятностни разпределения.
- В машинното обучение крос-ентропията и KL дивергенцията са често използвани функции за загуба.
- Логаритмите и правилното нормализиране на вероятностите са критични за коректни изчисления.
- Числената стабилност трябва да се осигури при изчисления с логаритми.
- Разбирането на тези концепции подобрява проектирането и анализа на AI модели.

## 9. Spaced Review Plan

| Време след учене | Промпт за преглед                                  |
|-------------------|---------------------------------------------------|
| 1 ден             | Обяснете с прости думи какво е ентропия.          |
| 3 дни             | Изчислете крос-ентропия между две примерни разпределения. |
| 1 седмица         | Опишете разликата между крос-ентропия и KL дивергенция. |
| 1 месец           | Дайте пример за използване на KL дивергенция в машинно обучение. |