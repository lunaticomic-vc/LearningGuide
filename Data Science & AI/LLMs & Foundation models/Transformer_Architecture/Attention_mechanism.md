# Attention_mechanism

## 1. Activate Prior Knowledge
- Какво знаете за начина, по който човешкото внимание филтрира и приоритизира информация в сложни ситуации?
- Как мислите, че софтуерните системи могат да използват подобен механизъм, за да обработват големи обеми данни по-ефективно?
- В контекста на изкуствения интелект, какво би било предимството на система, която „обръща внимание“ на определени части от входните данни?

## 2. Overview
Механизмът на внимание (Attention mechanism) е ключова концепция в съвременните модели за машинно обучение, особено в областта на обработката на естествен език и компютърното зрение. Той позволява на модела да се фокусира върху най-важните части от входните данни, подобрявайки качеството на изхода и ефективността на обучението.

В по-широк контекст, attention се интегрира в архитектури като трансформърите, които са основата на много съвременни AI системи, включително GPT и BERT. Този механизъм замества класическите рекурентни и конволюционни мрежи, като позволява паралелна обработка и по-добро улавяне на дългосрочни зависимости.

Значението на attention идва от способността му да моделира контекста динамично, което е критично при задачи като превод, резюмиране, разпознаване на обекти и други. Това прави системите по-гъвкави и адаптивни към сложни и разнообразни входни данни.

## 3. Key Concepts
- **Attention Score** – Мярка за важността на даден елемент от входа спрямо друг. Може да се мисли като „тегло“, което казва колко силно да се вземе предвид дадена част.
- **Query, Key, Value (QKV)** – Три основни компонента в attention: Query е „запитване“, Key са „етикети“ на входа, а Value са „стойностите“, които се комбинират според съвпадението между Query и Key.
- **Softmax Function** – Функция, която превръща attention scores в вероятности, така че сумата им да е 1, осигурявайки нормализирано внимание.
- **Self-Attention** – Вид attention, при който входът се сравнява със себе си, за да се разберат вътрешните зависимости в данните.
- **Multi-Head Attention** – Техника, която използва няколко паралелни attention механизма, за да се уловят различни аспекти на информацията едновременно.
- **Context Vector** – Резултатът от attention, който представлява обобщена информация, фокусирана върху релевантните части на входа.

## 4. Step-by-step Learning Path
1. **Разберете основите на attention**  
   - Фокус: Какво представлява attention и защо е нужен.  
   - Задача: Прочетете и визуализирайте прост пример с attention score между няколко думи.  
   - Въпроси: Какво е ролята на softmax във вниманието? Защо не използваме просто максимума?

2. **Изучете QKV модела**  
   - Фокус: Как се формират Query, Key и Value в моделите.  
   - Задача: Имплементирайте малък скрипт, който изчислява attention score от зададени Q, K, V матрици.  
   - Въпроси: Какво се случва, ако Q и K са много различни? Как влияе това на attention score?

3. **Практикувайте self-attention**  
   - Фокус: Разберете как входът се сравнява със себе си.  
   - Задача: Напишете код, който изчислява self-attention за малък текстов пример.  
   - Въпроси: Как self-attention помага при моделиране на дългосрочни зависимости?

4. **Изследвайте multi-head attention**  
   - Фокус: Защо и как се използват няколко attention глави.  
   - Задача: Анализирайте примерен код на multi-head attention и обяснете ролята на всяка глава.  
   - Въпроси: Как multi-head attention подобрява представянето на модела?

5. **Интегрирайте attention в трансформър архитектура**  
   - Фокус: Къде точно се използва attention в трансформърите и как влияе на цялостната работа.  
   - Задача: Разгледайте част от код на трансформър и идентифицирайте attention модулите.  
   - Въпроси: Как attention позволява паралелна обработка на входа?

## 5. Examples
### Пример 1: Изчисляване на attention score
```python
import numpy as np

Q = np.array([1, 0, 1])
K = np.array([[1, 0, 1],
              [0, 1, 0],
              [1, 1, 0]])

scores = np.dot(K, Q)  # Изчисляваме dot product между K и Q
print("Attention scores:", scores)
```

### Пример 2: Self-attention върху изречение
```python
import torch
import torch.nn.functional as F

# Примерни Q, K, V матрици (3 думи, размерност 4)
Q = torch.tensor([[1., 0., 1., 0.],
                  [0., 1., 0., 1.],
                  [1., 1., 0., 0.]])
K = Q.clone()
V = Q.clone()

scores = torch.matmul(Q, K.T) / (Q.shape[1] ** 0.5)
weights = F.softmax(scores, dim=1)
output = torch.matmul(weights, V)
print("Self-attention output:\n", output)
```

### Пример 3: Multi-head attention (псевдокод)
```python
def multi_head_attention(Q, K, V, heads=4):
    # Разделяме Q, K, V на 'heads' части
    Q_split = split(Q, heads)
    K_split = split(K, heads)
    V_split = split(V, heads)
    
    outputs = []
    for q, k, v in zip(Q_split, K_split, V_split):
        scores = softmax(dot(q, k.T))
        outputs.append(dot(scores, v))
    
    return concat(outputs)
```

## 6. Common Pitfalls
- **Игнориране на нормализацията (softmax)** – без нея attention scores не са интерпретируеми като вероятности и могат да доведат до нестабилно обучение.
- **Липса на мащабиране на dot product** – при големи размерности dot product стойностите могат да станат прекалено големи, което затруднява softmax функцията.
- **Неправилно разделяне на multi-head attention** – неправилното разпределение на размерностите може да доведе до загуба на информация.
- **Пренебрегване на padding маскиране** – при работа с последователности с различна дължина, липсата на маскиране може да обърка модела.
- **Опит за използване на attention без разбиране на контекста** – attention не е универсално решение и трябва да се прилага съобразно задачата.

## 7. Short Retrieval Quiz
1. Каква е ролята на softmax във вниманието?  
2. Какво представляват Query, Key и Value?  
3. Защо се използва self-attention?  
4. Какво е multi-head attention и какво предимство дава?  
5. Какво може да се случи, ако не се мащабира dot product преди softmax?  
6. Защо attention е важен в трансформър архитектурата?  
7. Какво представлява context vector?

## 8. Quick Recap
- Attention позволява на моделите да се фокусират върху релевантна информация в данните.  
- Основните компоненти са Query, Key и Value, които взаимодействат чрез dot product и softmax.  
- Self-attention сравнява входа със себе си, улавяйки вътрешни зависимости.  
- Multi-head attention използва паралелни глави, за да обхване различни аспекти на информацията.  
- В трансформърите attention замества рекурентните мрежи, позволявайки паралелна обработка и по-добро моделиране на дългосрочни зависимости.  
- Правилната нормализация и мащабиране са критични за стабилността и ефективността на attention.  
- Маскирането е необходимо при работа с последователности с различна дължина.

## 9. Spaced Review Plan

| Време след учене | Промпт за преглед                                      |
|------------------|--------------------------------------------------------|
| 1 ден            | Обяснете с прости думи какво е attention mechanism.    |
| 3 дни            | Опишете ролята на Query, Key и Value в attention.      |
| 1 седмица        | Какво е self-attention и как се използва в трансформър?|
| 1 месец          | Дайте пример за multi-head attention и обяснете ползите.|

