# Pretraining_objectives

## 1. Activate Prior Knowledge
- Какво представлява целта (objective) в контекста на машинното обучение и по-специално при обучението на големи езикови модели?
- Защо е важно да имаме ясна и добре дефинирана цел при предобучението на AI системи?
- Как мислите, че различните предобучителни задачи влияят върху качеството и приложимостта на модела в реални ситуации?

## 2. Overview
Предобучителните цели (pretraining objectives) са основата, върху която се изграждат големите езикови модели и други AI системи. Те дефинират задачите, които моделът трябва да изпълни по време на фазата на предобучение, преди да бъде фино настроен за конкретни приложения. Тези задачи са проектирани така, че да помогнат на модела да разбере структурата на езика, контекста и дълбоките зависимости в данните.

В по-широк контекст, предобучителните цели са част от процеса на самообучение, който позволява на модела да извлече максимална информация от големи обеми неетикетирани данни. Това е ключово, защото етикетираните данни са скъпи и ограничени, докато неетикетираните са изобилни. Добре дефинираната цел води до по-универсални и адаптивни модели, които могат да се използват в множество задачи.

Освен това, изборът на предобучителна цел влияе върху скоростта на обучение, стабилността на модела и неговата способност да генерализира. Затова разбирането на различните типове предобучителни задачи и тяхното приложение е критично за всеки професионалист, който иска да създава или използва съвременни AI системи.

## 3. Key Concepts
- **Masked Language Modeling (MLM)** – Задача, при която някои думи в изречението се заместват с маска, а моделът трябва да предскаже оригиналните думи. Аналогично на пъзел, където липсващите парчета трябва да се възстановят.
- **Next Sentence Prediction (NSP)** – Цел, при която моделът трябва да определи дали едно изречение следва логично друго. Може да се сравни с игра на свързване на изречения в смислов ред.
- **Causal Language Modeling (CLM)** – Моделът предсказва следващата дума, използвайки само предишния контекст, подобно на писане на текст дума по дума.
- **Contrastive Learning** – Метод, при който моделът се учи да различава сходни и несходни примери, подобно на разпознаване на двойки снимки, които са свързани или не.
- **Self-supervised Learning** – Обучение, при което моделът използва самите данни за генериране на етикети, без нужда от външна анотация.

## 4. Step-by-step Learning Path
1. **Запознайте се с основните предобучителни задачи (MLM, NSP, CLM)**
   - Задача: Прочетете оригиналните статии за BERT и GPT, за да разберете техните предобучителни цели.
   - Въпроси: Каква е разликата между MLM и CLM? Защо BERT използва NSP?
2. **Имплементирайте прост MLM на малък корпус текст**
   - Задача: Напишете скрипт, който маскира случайни думи и тренира прост трансформър модел да ги предсказва.
   - Въпроси: Как маскирането влияе на обучението? Какви са предизвикателствата при избора на думи за маскиране?
3. **Експериментирайте с различни предобучителни задачи върху един и същ модел**
   - Задача: Сравнете резултатите от MLM и CLM върху една и съща архитектура.
   - Въпроси: Коя задача води до по-добра генерализация? Каква е ролята на контекста в двете задачи?
4. **Анализирайте влиянието на предобучителните цели върху downstream задачи**
   - Задача: Тествайте модели с различни предобучителни цели върху задачи като класификация или въпроси-отговори.
   - Въпроси: Как предобучителната цел влияе на представянето? Кои задачи са по-чувствителни към избора на цел?

## 5. Examples
- **Masked Language Modeling (BERT)**
```python
from transformers import BertTokenizer, BertForMaskedLM
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')

text = "The quick brown [MASK] jumps over the lazy dog."
inputs = tokenizer(text, return_tensors='pt')
labels = inputs.input_ids.clone()
outputs = model(**inputs, labels=labels)
loss = outputs.loss
print(f"Loss: {loss.item()}")
```

- **Causal Language Modeling (GPT)**
```python
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

text = "The future of AI is"
inputs = tokenizer(text, return_tensors='pt')
outputs = model(**inputs, labels=inputs.input_ids)
loss = outputs.loss
print(f"Loss: {loss.item()}")
```

- **Next Sentence Prediction (NSP)**
```python
# NSP е част от BERT обучението, където се подават двойки изречения и моделът предсказва дали второто следва първото.
```

## 6. Common Pitfalls
- **Прекомерно маскиране** – Маскиране на твърде много думи води до загуба на контекст и затруднява обучението.
- **Неправилен избор на предобучителна цел за задачата** – Например, използване на CLM за задачи, които изискват разбиране на двупосочен контекст.
- **Игнориране на баланса между задачи** – При мултизадачно предобучение, неправилното претегляне на задачите може да доведе до доминиране на една цел.
- **Недостатъчно разнообразие в данните** – Моделът може да се научи да решава задачата, но да не се генерализира добре.
- **Пренебрегване на ефекта от предобучителната цел върху downstream задачи** – Не всички цели са еднакво полезни за всички приложения.

## 7. Short Retrieval Quiz
1. Каква е основната идея зад Masked Language Modeling?
2. Защо Next Sentence Prediction е важна за някои модели?
3. Как се различава Causal Language Modeling от MLM?
4. Какво представлява self-supervised learning?
5. Кои са рисковете при прекомерно маскиране на думи?
6. Как предобучителната цел влияе на представянето на модела в downstream задачи?
7. Какво е contrastive learning и как се използва в предобучението?

## 8. Quick Recap
- Предобучителните цели дефинират задачите, които моделът решава по време на предобучение.
- Masked Language Modeling и Causal Language Modeling са две основни техники с различни приложения.
- Next Sentence Prediction помага на модела да разбира връзките между изречения.
- Изборът на предобучителна цел влияе върху качеството и адаптивността на модела.
- Практическото имплементиране на тези цели изисква баланс и разбиране на контекста.
- Често срещани грешки включват прекомерно маскиране и неподходящ избор на задача.
- Разбирането на предобучителните цели е ключово за успешното прилагане на AI модели.

## 9. Spaced Review Plan

| Време след изучаване | Промпти за преговор                                |
|----------------------|---------------------------------------------------|
| 1 ден                | Какво е Masked Language Modeling?                  |
| 3 дни                | Обяснете разликата между MLM и CLM.                |
| 1 седмица            | Как предобучителната цел влияе на downstream задачи? |
| 1 месец              | Избройте типични грешки при имплементация на предобучителни цели и как да ги избегнем. |