# Vector_databases

## 1. Activate Prior Knowledge
- Какво представляват базите данни и какви типове съществуват (релационни, NoSQL и т.н.)?
- Как се съхраняват и търсят данни в традиционните бази данни спрямо нуждите на AI системи?
- Какво е векторно представяне на данни и защо е важно за съвременните модели за машинно обучение и обработка на естествен език?

## 2. Overview
Vector databases са специализирани бази данни, оптимизирани за съхранение, индексиране и търсене на векторни представяния на данни. В контекста на AI, много данни – като текст, изображения и аудио – се преобразуват в многомерни вектори, които улавят семантична или структурна информация. Традиционните бази данни не са ефективни при обработката на такива данни, поради което се налагат векторни бази данни.

Тези бази данни позволяват бързо и мащабируемо търсене на най-близки съседи (nearest neighbors search) в големи множества от вектори, което е ключово за приложения като препоръчителни системи, търсене по съдържание, класификация и други AI задачи. Те се интегрират в по-широки AI системи като хранилища за ембедингите, които модели като BERT, CLIP или други трансформъри генерират.

Векторните бази данни са от съществено значение за съвременните приложения, защото позволяват ефективна работа с неструктурирани данни и подобряват качеството на търсене и препоръки чрез семантично разбиране, а не само чрез ключови думи или индекси.

## 3. Key Concepts
- **Vector Embedding** – числова репрезентация на обекти (текст, изображения, аудио) в многомерно пространство, където близостта между векторите отразява семантична или структурна близост.
- **Nearest Neighbor Search (NNS)** – процесът на намиране на вектори, които са най-близки до даден вектор по някаква метрика (например Евклидово разстояние или косинусна близост).
- **Approximate Nearest Neighbor (ANN)** – техники за бързо намиране на близки съседи, които не гарантират абсолютно най-близкия резултат, но са много по-бързи и мащабируеми.
- **Indexing Structures** – специални структури като HNSW (Hierarchical Navigable Small World), IVF (Inverted File), PQ (Product Quantization), които ускоряват търсенето в големи множества от вектори.
- **Dimensionality Reduction** – методи като PCA или t-SNE, които намаляват размерността на векторите, за да подобрят скоростта и ефективността на търсенето.
- **Semantic Search** – търсене, което използва смисловата близост между вектори, а не точни съвпадения на ключови думи.

## 4. Step-by-step Learning Path
1. **Основи на векторните представяния**
   - Фокус: Разберете как текст или изображения се преобразуват във вектори.
   - Задача: Използвайте библиотека като `sentence-transformers` за генериране на текстови ембединг.
   - Въпроси: Какво представлява embedding? Защо е полезен в AI?

2. **Запознаване с nearest neighbor search**
   - Фокус: Научете как се намират най-близките вектори.
   - Задача: Имплементирайте прост nearest neighbor search с Евклидово разстояние.
   - Въпроси: Какво е разстояние между вектори? Как се измерва близост?

3. **Изучаване на индексни структури за ANN**
   - Фокус: Разберете HNSW и IVF.
   - Задача: Инсталирайте и използвайте FAISS за индексиране и търсене.
   - Въпроси: Какво представлява approximate nearest neighbor? Защо е по-подходящ за големи данни?

4. **Интеграция на векторна база данни в AI система**
   - Фокус: Свържете embedding генератор с векторна база данни.
   - Задача: Създайте малка система за търсене по семантичен текст с помощта на Pinecone или Weaviate.
   - Въпроси: Как векторната база данни подобрява AI приложението?

5. **Оптимизация и мащабиране**
   - Фокус: Научете техники за компресия и мащабиране.
   - Задача: Използвайте PQ или други техники за компресия в FAISS.
   - Въпроси: Как компресирането влияе на точността и скоростта?

## 5. Examples

### Пример 1: Генериране на текстови ембединг с `sentence-transformers` (Python)
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
sentences = ["Здравей, свят!", "Как си?"]
embeddings = model.encode(sentences)
print(embeddings)
```

### Пример 2: Търсене на най-близък вектор с FAISS
```python
import faiss
import numpy as np

d = 128  # размерност на векторите
nb = 1000  # брой вектори в базата
np.random.seed(1234)
data = np.random.random((nb, d)).astype('float32')

index = faiss.IndexFlatL2(d)  # Евклидово разстояние
index.add(data)

query = np.random.random((1, d)).astype('float32')
k = 5  # брой най-близки съседи
distances, indices = index.search(query, k)
print(indices)
```

### Пример 3: Използване на Pinecone за векторна база данни (псевдо код)
```python
import pinecone

pinecone.init(api_key="YOUR_API_KEY", environment="us-west1-gcp")
index = pinecone.Index("example-index")

# Добавяне на вектори
index.upsert([
    ("id1", [0.1, 0.2, 0.3]),
    ("id2", [0.4, 0.5, 0.6])
])

# Търсене
query_vector = [0.1, 0.2, 0.25]
results = index.query(query_vector, top_k=3)
print(results)
```

## 6. Common Pitfalls
- **Игнориране на избор на метрика за разстояние** – различните задачи изискват различни метрики (косинусна близост, Евклидово разстояние). Неподходящ избор води до лоши резултати.
- **Опит за точно търсене в големи бази** – при големи обеми данни трябва да се използват ANN методи, иначе търсенето е бавно.
- **Липса на нормализация на векторите** – при косинусна близост е важно векторите да са нормализирани.
- **Прекалено висока размерност без компресия** – води до бавна работа и голямо използване на памет.
- **Недостатъчно разбиране на embedding модела** – качеството на embedding пряко влияе на резултатите от търсенето.

## 7. Short Retrieval Quiz
1. Какво представлява векторното представяне на данни?
2. Защо традиционните бази данни не са подходящи за търсене по вектори?
3. Какво е approximate nearest neighbor search?
4. Коя структура за индексиране се използва често във FAISS?
5. Каква е ролята на нормализацията при косинусна близост?
6. Какви типове данни могат да се съхраняват във векторна база данни?
7. Как векторните бази данни подобряват AI приложенията?

## 8. Quick Recap
- Vector databases съхраняват и търсят векторни представяния на данни.
- Те са ключови за AI системи, които работят с неструктурирана информация.
- Основният проблем е ефективното намиране на най-близки съседи в многомерни пространства.
- Approximate nearest neighbor алгоритмите балансират между точност и скорост.
- Индексните структури като HNSW и IVF ускоряват търсенето.
- Качеството на embedding и изборът на метрика са критични за успеха.
- Векторните бази данни се интегрират в реални AI приложения като препоръчителни системи и семантично търсене.

## 9. Spaced Review Plan

| Време след учене | Промпт за преговор                                      |
|------------------|--------------------------------------------------------|
| 1 ден            | Обяснете какво е векторна база данни и защо е нужна.   |
| 3 дни            | Избройте и опишете основните индексни структури.       |
| 1 седмица        | Демонстрирайте с пример как се извършва nearest neighbor search. |
| 1 месец          | Обсъдете типични грешки при работа с векторни бази данни и как да ги избегнете. |