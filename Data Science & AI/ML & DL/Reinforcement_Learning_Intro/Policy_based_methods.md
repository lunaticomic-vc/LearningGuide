# Policy_based_methods

## 1. Activate Prior Knowledge
- Какво разбирате под „политика“ в контекста на изкуствения интелект и машинното обучение?
- Как бихте описали разликата между директно оптимизиране на действия и използването на стойностни функции?
- В кои ситуации бихте предпочели да използвате методи, които директно моделират политика, вместо да учите стойностни функции?

## 2. Overview
Policy-based methods са клас алгоритми в областта на подсилващото обучение (Reinforcement Learning), които директно оптимизират политика — функция, която определя какъв избор на действие да се направи в дадено състояние. Вместо да учат стойностна функция (която оценява колко добри са състоянията или действията), тези методи се фокусират върху намирането на оптималната стратегия за действие.

Тези методи са особено полезни в среди с големи или непрекъснати пространства от действия, където стойностните функции са трудни за моделиране или оптимизиране. Те също така позволяват по-гладко и стабилно обучение, тъй като директно оптимизират параметрите на политиката, често представена чрез невронни мрежи.

В по-широк контекст на AI системи, policy-based methods са ключови за задачи, където се изисква адаптивно поведение и вземане на решения в реално време, като роботика, автономни превозни средства и игри.

## 3. Key Concepts
- **Policy (Политика)** – функция, която за всяко състояние определя вероятностно разпределение върху възможните действия. Може да бъде детерминистична или стохастична. Аналогия: като план или стратегия за игра, която казва как да се действа във всяка ситуация.
- **Policy Gradient (Градиент на политиката)** – метод за оптимизиране на параметрите на политика чрез изчисляване на градиенти на очакваната възнаграждение спрямо тези параметри. Мислете за това като за насочване на „стрелката“ към по-добри действия.
- **Actor-Critic (Актьор-Критик)** – архитектура, която комбинира policy-based метод (актьор), който избира действия, и value-based метод (критик), който оценява качеството на действията, за по-ефективно обучение.
- **Exploration vs Exploitation (Изследване срещу експлоатация)** – баланс между опитване на нови действия и използване на вече научени стратегии. В policy-based методите това често се постига чрез стохастични политики.
- **REINFORCE Algorithm** – базов policy gradient алгоритъм, който използва Monte Carlo оценки за градиентите на политиката.

## 4. Step-by-step Learning Path
1. **Разберете основите на политика и стойност**  
   - Фокус: Разлика между policy-based и value-based методи.  
   - Задача: Напишете кратък текст, който описва кога бихте използвали политика вместо стойностна функция.  
   - Въпроси: Какво е политика? Защо понякога е по-добре да се оптимизира директно?

2. **Изучете градиентите на политиката**  
   - Фокус: Формула и интуиция зад policy gradient.  
   - Задача: Приложете прост REINFORCE алгоритъм върху среда с дискретни действия (например OpenAI Gym CartPole).  
   - Въпроси: Как се изчислява градиентът? Какво означава стохастична политика?

3. **Разгледайте Actor-Critic методите**  
   - Фокус: Как актьорът и критикът работят заедно.  
   - Задача: Имплементирайте базов Actor-Critic модел и сравнете резултатите с REINFORCE.  
   - Въпроси: Каква е ролята на критика? Какво подобрява Actor-Critic спрямо чистия policy gradient?

4. **Практикувайте с непрекъснати пространства от действия**  
   - Фокус: Прилагане на policy-based методи в непрекъснати действия (например с Gaussian политики).  
   - Задача: Обучете агент за управление на непрекъснат контрол (например Pendulum-v0).  
   - Въпроси: Как се моделира политика за непрекъснати действия? Какви предизвикателства възникват?

5. **Оптимизация и стабилност**  
   - Фокус: Техники за стабилизиране на обучението (например Trust Region Policy Optimization, Proximal Policy Optimization).  
   - Задача: Изследвайте и сравнете PPO с базов policy gradient на избрана среда.  
   - Въпроси: Защо е важно да ограничаваме големите промени в политиката? Как PPO постига това?

## 5. Examples

### Пример 1: REINFORCE за CartPole (Python + PyTorch)
```python
import gym
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

env = gym.make('CartPole-v1')

class PolicyNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(4, 128),
            nn.ReLU(),
            nn.Linear(128, 2),
            nn.Softmax(dim=-1)
        )
    def forward(self, x):
        return self.fc(x)

policy = PolicyNetwork()
optimizer = optim.Adam(policy.parameters(), lr=1e-2)

def select_action(state):
    state = torch.from_numpy(state).float()
    probs = policy(state)
    m = Categorical(probs)
    action = m.sample()
    return action.item(), m.log_prob(action)

for episode in range(1000):
    state = env.reset()
    rewards = []
    log_probs = []
    done = False
    while not done:
        action, log_prob = select_action(state)
        next_state, reward, done, _ = env.step(action)
        rewards.append(reward)
        log_probs.append(log_prob)
        state = next_state
    # Compute returns and update policy
    returns = []
    R = 0
    for r in reversed(rewards):
        R = r + 0.99 * R
        returns.insert(0, R)
    returns = torch.tensor(returns)
    returns = (returns - returns.mean()) / (returns.std() + 1e-5)
    loss = 0
    for log_prob, R in zip(log_probs, returns):
        loss -= log_prob * R
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### Пример 2: Actor-Critic концепция
- Актьорът предлага действие, критикът оценява колко добро е действието.
- Това позволява по-бързо и стабилно обучение, тъй като критикът предоставя по-точна обратна връзка.

### Пример 3: Непрекъснати действия с Gaussian политика
- Политиката изкарва средна стойност и стандартно отклонение, от които се взема проба за действие.
- Това позволява плавно управление в задачи като роботика.

## 6. Common Pitfalls
- **Игнориране на варианс в оценките** – REINFORCE има висока вариансност, което може да забави обучението. Използвайте техники като baseline или Actor-Critic, за да намалите варианса.
- **Лош баланс между изследване и експлоатация** – стохастичните политики трябва да поддържат достатъчно изследване, за да не се заклещят в локални оптимуми.
- **Неправилна нормализация на наградите** – липсата на нормализация може да доведе до нестабилно обучение.
- **Прекалено големи стъпки при обновяване на политиката** – могат да доведат до срив на обучението; използвайте техники като PPO.
- **Неправилно моделиране на политика за непрекъснати действия** – неадекватни параметри на разпределението могат да ограничат представянето.

## 7. Short Retrieval Quiz
1. Какво представлява политика в контекста на подсилващото обучение?
2. Каква е основната разлика между policy-based и value-based методи?
3. Какво е целта на policy gradient алгоритмите?
4. Какво прави критикът в Actor-Critic архитектурата?
5. Защо е важно да се контролира големината на обновленията в политиката?
6. Как стохастичните политики подпомагат изследването?
7. Каква е ролята на нормализацията на наградите в REINFORCE?

## 8. Quick Recap
- Policy-based methods оптимизират директно стратегията за избор на действия.
- Те са особено полезни при големи или непрекъснати пространства от действия.
- Policy gradient алгоритмите изчисляват градиенти на очакваното възнаграждение спрямо параметрите на политиката.
- Actor-Critic методите комбинират предимствата на policy- и value-based подходите.
- Стохастичните политики подпомагат баланса между изследване и експлоатация.
- Стабилността на обучението се подобрява чрез техники като PPO.
- Внимателното управление на варианса и нормализацията е ключово за ефективно обучение.

## 9. Spaced Review Plan

| Време след учене | Прегледна задача                                      |
|------------------|------------------------------------------------------|
| 1 ден            | Обяснете с прости думи какво е политика и policy gradient. |
| 3 дни            | Имплементирайте прост REINFORCE алгоритъм и отговорете на въпроси за Actor-Critic. |
| 1 седмица        | Сравнете policy-based и value-based методи с примери. |
| 1 месец          | Прегледайте техники за стабилизиране на обучението и приложете PPO на избрана среда. |