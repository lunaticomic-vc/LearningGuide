# Gradient_descent_and_variants

## 1. Activate Prior Knowledge
- Какво представлява оптимизацията в контекста на машинното обучение и защо е важна?
- Как бихте обяснили ролята на градиентите при обучението на невронни мрежи?
- Какви предизвикателства могат да възникнат при намирането на минимални стойности на сложни функции?

## 2. Overview
Градиентният спуск е основен алгоритъм за оптимизация, широко използван в машинното обучение и изкуствения интелект за минимизиране на функция на загуба. Той позволява на моделите да се адаптират към данните чрез итеративно коригиране на параметрите си в посока на най-бързото намаляване на грешката.

В по-широк контекст, градиентният спуск е сърцето на обучението на невронни мрежи и други модели, където целта е да се намерят параметри, които минимизират разликата между предсказанията и реалните стойности. Без ефективен метод за оптимизация, обучението би било бавно, нестабилно или невъзможно.

Различните варианти на градиентния спуск (като стохастичен, мини-батч и адаптивни методи) са разработени, за да се справят с предизвикателства като големи обеми данни, шум в градиентите и различни пейзажи на функцията на загуба. Разбирането на тези варианти е ключово за изграждане на стабилни и ефективни AI системи.

## 3. Key Concepts
- **Gradient (Градиент)** – Вектор, който показва посоката на най-бързо увеличение на функцията. Мислете за него като за стрелка, която сочи накъде да се движим, за да увеличим стойността.
- **Learning Rate (Скорост на учене)** – Стойност, която определя колко голяма стъпка правим в посока на градиента. Аналогично на педала на газта в кола – ако е твърде силен, може да загубим контрол.
- **Loss Function (Функция на загуба)** – Мярка за това колко далеч са предсказанията на модела от реалните данни. Целта е да я минимизираме.
- **Batch Gradient Descent (Пълен градиентен спуск)** – Изчислява градиента върху целия набор от данни, което е точно, но бавно при големи обеми.
- **Stochastic Gradient Descent (Стохастичен градиентен спуск)** – Изчислява градиента върху един пример, което ускорява обучението, но въвежда шум.
- **Mini-batch Gradient Descent (Мини-батч градиентен спуск)** – Компромис между двата подхода, използва малки подмножества от данни за изчисляване на градиента.
- **Momentum (Импулс)** – Техника, която добавя инерция към стъпките, за да избегне колебания и да ускори конвергенцията.
- **Adaptive Methods (Адаптивни методи)** – Алгоритми като AdaGrad, RMSProp и Adam, които автоматично настройват скоростта на учене за всеки параметър.

## 4. Step-by-step Learning Path
1. **Разберете основите на градиентния спуск**
   - Фокус: Как се изчислява градиентът и как се използва за оптимизация.
   - Задача: Изчислете градиента на проста квадратична функция и направете няколко стъпки на ръка.
   - Въпроси: Какво показва градиентът? Защо стъпките са в обратна посока?

2. **Изучете различните варианти на градиентния спуск**
   - Фокус: Разлики между пълен, стохастичен и мини-батч градиентен спуск.
   - Задача: Имплементирайте стохастичен и мини-батч градиентен спуск за линейна регресия.
   - Въпроси: Как стохастичният градиентен спуск влияе на шума в обучението? Кога е полезен мини-батч подходът?

3. **Разберете ролята на скоростта на учене и импулса**
   - Фокус: Влияние на параметрите learning rate и momentum върху конвергенцията.
   - Задача: Експериментирайте с различни стойности на learning rate и добавете momentum в имплементацията.
   - Въпроси: Какви са рисковете при твърде голяма или твърде малка скорост на учене? Как импулсът помага при неравности в функцията на загуба?

4. **Изучете адаптивните методи**
   - Фокус: Принципи и предимства на алгоритми като Adam и RMSProp.
   - Задача: Сравнете Adam с класическия градиентен спуск при обучение на малка невронна мрежа.
   - Въпроси: Как Adam адаптира скоростта на учене? В какви ситуации адаптивните методи са особено полезни?

5. **Прилагане в реални AI проекти**
   - Фокус: Избор на подходящ метод и параметри според задачата и данните.
   - Задача: Изберете и настройте градиентен спуск за проект по класификация с голям набор от данни.
   - Въпроси: Какви критерии използвате за избор на метод? Как следите и подобрявате конвергенцията?

## 5. Examples
### Пример 1: Пълен градиентен спуск за линейна регресия (Python)
```python
import numpy as np

# Данни
X = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 6, 8])

# Параметри
w = 0.0
b = 0.0
learning_rate = 0.01
epochs = 1000

for _ in range(epochs):
    y_pred = w * X + b
    dw = (1/len(X)) * np.sum((y_pred - y) * X)
    db = (1/len(X)) * np.sum(y_pred - y)
    w -= learning_rate * dw
    b -= learning_rate * db

print(f"w: {w}, b: {b}")
```

### Пример 2: Стохастичен градиентен спуск (SGD)
```python
import numpy as np

X = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 6, 8])

w, b = 0.0, 0.0
learning_rate = 0.01
epochs = 10

for _ in range(epochs):
    for i in range(len(X)):
        y_pred = w * X[i] + b
        dw = (y_pred - y[i]) * X[i]
        db = y_pred - y[i]
        w -= learning_rate * dw
        b -= learning_rate * db

print(f"w: {w}, b: {b}")
```

### Пример 3: Използване на Adam с TensorFlow
```python
import tensorflow as tf

model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
model.compile(optimizer='adam', loss='mse')

X = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 6, 8])

model.fit(X, y, epochs=100)
```

## 6. Common Pitfalls
- **Твърде голяма скорост на учене** – Може да доведе до прескачане на минимуми и нестабилно обучение. Решение: започнете с малка стойност и я настройвайте.
- **Игнориране на нормализацията на данните** – Без нормализация градиентите могат да бъдат много големи или малки, забавяйки обучението.
- **Прекомерно доверие на стохастичния градиентен спуск** – Високият шум може да забави конвергенцията или да доведе до нестабилност.
- **Неправилна инициализация на параметрите** – Може да попречи на ефективното обучение.
- **Пренебрегване на адаптивните методи при сложни модели** – Те често ускоряват и стабилизират обучението.

## 7. Short Retrieval Quiz
1. Какво представлява градиентът и каква е неговата роля в градиентния спуск?
2. Каква е разликата между стохастичен и пълен градиентен спуск?
3. Защо скоростта на учене е критичен параметър?
4. Какво представлява momentum и как помага при оптимизацията?
5. Кога е подходящо да използваме мини-батч градиентен спуск?
6. Как адаптивните методи като Adam подобряват класическия градиентен спуск?
7. Какви са рисковете при твърде голяма скорост на учене?

## 8. Quick Recap
- Градиентният спуск е итеративен метод за минимизиране на функция на загуба чрез движение в посока на отрицателния градиент.
- Основните варианти са пълен, стохастичен и мини-батч градиентен спуск, всеки с предимства и недостатъци.
- Скоростта на учене и импулсът са ключови параметри, които влияят на скоростта и стабилността на обучението.
- Адаптивните методи автоматично настройват скоростта на учене и често водят до по-бърза и стабилна конвергенция.
- Практическото приложение изисква експериментиране и настройка според конкретната задача и данни.

## 9. Spaced Review Plan

| Време       | Промпт за преглед                                      |
|-------------|-------------------------------------------------------|
| 1 ден       | Обяснете как работи градиентният спуск и защо е важен.|
| 3 дни       | Сравнете стохастичен и мини-батч градиентен спуск.   |
| 1 седмица   | Опишете ролята на learning rate и momentum.           |
| 1 месец     | Прегледайте адаптивните методи и кога да ги използвате.|