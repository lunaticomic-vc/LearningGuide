# Adversarial_inputs

## 1. Activate Prior Knowledge

- Какво представлява входът в контекста на изкуствения интелект и софтуерното инженерство?
- Защо би било възможно да се създадат входни данни, които умишлено подвеждат AI системи?
- Какви последици може да има успешна атака с adversarial inputs върху реални приложения на машинно обучение?

## 2. Overview

Adversarial inputs са специално конструирани входни данни, които целят да подведат модели на изкуствен интелект, като ги накарат да направят грешна прогноза или класификация. Тези входове често съдържат малки, на пръв поглед незабележими промени, които обаче водят до значителни грешки в изхода на системата.

В по-широк контекст, adversarial inputs са част от областта на сигурността на машинното обучение и надеждността на AI системите. Те демонстрират уязвимости, които могат да бъдат експлоатирани както в научни изследвания, така и в реални приложения, като автономни превозни средства, системи за разпознаване на лица и медицинска диагностика.

Разбирането и справянето с adversarial inputs е критично за разработването на устойчиви и надеждни AI системи, които могат да функционират безопасно в динамична и потенциално враждебна среда.

## 3. Key Concepts

- **Adversarial Input** – Входни данни, модифицирани по такъв начин, че да заблудят машинен модел, без да се променя съществено човешкото възприятие за тях. Може да се сравни с фалшива банкнота, която изглежда истинска, но подвежда системата.
  
- **Perturbation** – Малка промяна, добавена към оригиналния вход, която е достатъчна да промени изхода на модела. Аналогично на малка пукнатина в стъкло, която отслабва цялата структура.

- **White-box Attack** – Атака, при която атакуващият има пълен достъп до модела, включително архитектура и параметри. Това е като да имаш чертежите на сейф, който искаш да отвориш.

- **Black-box Attack** – Атака без вътрешна информация за модела, базирана само на наблюдение на входове и изходи. Подобно на опит да се разгадае парола чрез проби и грешки.

- **Robustness** – Способността на модел да устои на adversarial inputs и да запази коректното си поведение. Може да се мисли като броня, която предпазва системата от атаки.

## 4. Step-by-step Learning Path

1. **Запознаване с основите на машинното обучение и невронните мрежи**  
   *Задача:* Прегледайте основна архитектура на невронна мрежа и как тя приема входни данни.  
   *Въпроси:* Какво представлява входният вектор? Как се формира изходът?

2. **Изучаване на примери за adversarial inputs**  
   *Задача:* Разгледайте класически примери като FGSM (Fast Gradient Sign Method) атака.  
   *Въпроси:* Как се изчислява perturbation? Защо малката промяна е ефективна?

3. **Практическа реализация на FGSM атака върху прост модел**  
   *Задача:* Имплементирайте FGSM върху предварително обучен модел за класификация на изображения.  
   *Въпроси:* Каква е разликата между оригиналното и adversarial изображение? Как се променя изходът?

4. **Изследване на техники за защита и устойчивост**  
   *Задача:* Проучете adversarial training и други методи за повишаване на robustnes.  
   *Въпроси:* Как adversarial training подобрява устойчивостта? Какви са ограниченията?

5. **Анализ на реални приложения и потенциални рискове**  
   *Задача:* Изследвайте случаи на реални атаки и тяхното въздействие.  
   *Въпроси:* Кои системи са най-уязвими? Какви мерки се прилагат?

## 5. Examples

### Пример 1: FGSM атака върху MNIST

```python
import torch
import torchvision
import torchvision.transforms as transforms
from torch.autograd import Variable

# Зареждане на модел и данни
model = torchvision.models.resnet18(pretrained=True)
model.eval()

# Примерно изображение и цел
image = torch.randn((1, 3, 224, 224), requires_grad=True)
target = torch.tensor([3])  # произволен клас

# Изчисляване на загуба и градиент
output = model(image)
loss = torch.nn.CrossEntropyLoss()(output, target)
loss.backward()

# FGSM perturbation
epsilon = 0.01
perturbation = epsilon * image.grad.data.sign()
adversarial_image = image + perturbation
```

### Пример 2: Black-box атака чрез query-based метод

- Използване на множество заявки към модела с малки промени в входа, за да се открие посоката на perturbation.

### Пример 3: Adversarial training

- Обучение на модел с добавени adversarial inputs, за да се повиши устойчивостта му.

## 6. Common Pitfalls

- **Подценяване на малките промени:** Много разработчици смятат, че малките промени в данните не влияят на модела, което е грешно при adversarial inputs.  
- **Прекалена увереност в модела:** Моделите често дават висока увереност в грешни прогнози при adversarial inputs.  
- **Игнориране на защитните техники:** Липсата на adversarial training или други методи за robustnes води до уязвимости.  
- **Неправилна оценка на атаките:** Използване само на white-box или black-box атаки без комбиниран подход може да доведе до непълна картина.

## 7. Short Retrieval Quiz

1. Какво представлява adversarial input?  
2. Какво е perturbation в контекста на adversarial атаки?  
3. Каква е разликата между white-box и black-box атаки?  
4. Как adversarial training помага за устойчивост на модела?  
5. Защо малките промени в данните могат да доведат до големи грешки?  
6. Кои са основните рискове от adversarial inputs в реални приложения?  
7. Какво означава robustnes на AI модел?

## 8. Quick Recap

- Adversarial inputs са умишлено модифицирани данни, които подвеждат AI модели.  
- Малките perturbations могат да доведат до значителни грешки.  
- White-box и black-box атаки се различават по нивото на достъп до модела.  
- Защитните техники като adversarial training повишават устойчивостта.  
- Разбирането на adversarial inputs е ключово за сигурността на AI системите.  
- Игнорирането на тези уязвимости може да доведе до сериозни последствия.  
- Практическото изучаване включва както теоретични, така и експериментални подходи.

## 9. Spaced Review Plan

| Време след изучаване | Промпт за преглед                                      |
|----------------------|--------------------------------------------------------|
| 1 ден                | Обяснете какво е adversarial input и perturbation.    |
| 3 дни                | Опишете разликите между white-box и black-box атаки.  |
| 1 седмица            | Прегледайте основните методи за защита от атаки.      |
| 1 месец              | Дискутирайте реални приложения и рискове от adversarial inputs. |