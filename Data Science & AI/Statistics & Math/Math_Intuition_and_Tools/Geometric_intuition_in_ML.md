# Geometric_intuition_in_ML

## 1. Activate Prior Knowledge
- Какво разбирате под "пространство на признаците" в контекста на машинното обучение?
- Как бихте визуализирали разделянето на два класа данни в двумерно пространство?
- Как геометричните концепции могат да помогнат при оптимизацията на модели или при избора на функции?

## 2. Overview
Геометричната интуиция в машинното обучение е начин да разберем и визуализираме как алгоритмите обработват данните, използвайки пространствени и векторни представяния. Тя ни позволява да мислим за данните като точки в многомерно пространство, където различните операции – като класификация, регресия или клъстеризация – могат да се интерпретират чрез геометрични трансформации и разстояния.

Тази интуиция е ключова за разбиране на алгоритми като линейна регресия, SVM, невронни мрежи и PCA, тъй като те използват концепции като хиперплоскости, ъгли и проекции. В по-широк контекст, геометричните подходи помагат за оптимизиране на модели, подобряване на обяснимостта и диагностициране на проблеми с данните.

Осъзнаването на геометричните свойства на данните и моделите е особено важно в софтуерното инженерство на AI системи, където ефективното представяне и трансформация на данни влияе директно върху производителността и точността.

## 3. Key Concepts
- **Feature Space (Пространство на признаците)** – Множество от всички възможни стойности на признаците, където всяка точка представлява един обект. Представете си го като координатна система, в която всяка ос е различен признак.
- **Vector Representation (Векторно представяне)** – Данните се представят като вектори, което позволява използването на линейна алгебра за анализ и трансформация.
- **Distance Metrics (Метрични функции за разстояние)** – Начини за измерване на разстояния между точки, например Евклидово разстояние, което е като „правата линия“ между две точки.
- **Hyperplane (Хиперплоскост)** – Общация на линейна граница, която разделя пространството на две части, например при класификация.
- **Projection (Проекция)** – Начин за „преместване“ на точка върху подпространство, подобно на сянка, хвърлена от обект върху стена.
- **Dimensionality Reduction (Намаляване на размерността)** – Процесът на опростяване на данните чрез премахване на излишни признаци, запазвайки най-важната информация, като PCA.
- **Manifold (Многообразие)** – Нелинейна повърхност, върху която лежат данните, дори ако са в по-високоизмерно пространство.

## 4. Step-by-step Learning Path
1. **Фокус:** Вектори и операции с тях  
   **Задача:** Изчислете скаларно произведение и дължина на вектори в Python.  
   **Въпроси:** Какво означава скаларното произведение геометрично? Как се изчислява ъгъл между два вектора?

2. **Фокус:** Разстояния и метрики  
   **Задача:** Имплементирайте Евклидово и Манхатънско разстояние между две точки.  
   **Въпроси:** Кога бихте избрали Манхатънско пред Евклидово разстояние?

3. **Фокус:** Хиперплоскости и линейна класификация  
   **Задача:** Визуализирайте разделяне на два класа с линейна граница в 2D.  
   **Въпроси:** Какво представлява хиперплоскост? Как тя разделя пространството?

4. **Фокус:** Проекции и PCA  
   **Задача:** Използвайте sklearn за PCA върху реален набор от данни и визуализирайте резултатите.  
   **Въпроси:** Какво е целта на проекцията? Как PCA помага при визуализация?

5. **Фокус:** Нелинейни многообразия и kernel методи  
   **Задача:** Изследвайте kernel trick чрез SVM с RBF ядро.  
   **Въпроси:** Как kernel функцията трансформира данните? Защо е полезна?

## 5. Examples
### Пример 1: Векторни операции в Python
```python
import numpy as np

v1 = np.array([1, 2])
v2 = np.array([3, 4])

dot_product = np.dot(v1, v2)
length_v1 = np.linalg.norm(v1)
angle = np.arccos(dot_product / (length_v1 * np.linalg.norm(v2)))

print(f"Dot product: {dot_product}")
print(f"Angle (radians): {angle}")
```

### Пример 2: Линейна граница с SVM
```python
from sklearn import datasets
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np

X, y = datasets.make_blobs(n_samples=50, centers=2, random_state=6)
clf = SVC(kernel='linear')
clf.fit(X, y)

plt.scatter(X[:, 0], X[:, 1], c=y)
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = clf.decision_function(xy).reshape(XX.shape)

ax.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.5)
plt.show()
```

### Пример 3: PCA за намаляване на размерността
```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

data = load_iris()
X = data.data
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data.target)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA на Iris dataset')
plt.show()
```

## 6. Common Pitfalls
- **Игнориране на размерността:** Високата размерност може да доведе до „проклятието на размерността“, където разстоянията губят смисъл. Използвайте техники за намаляване на размерността.
- **Грешно интерпретиране на разстояния:** Не всички метрики са подходящи за всички типове данни. Например, Евклидовото разстояние не е добро за категориални признаци.
- **Прекалено опростяване на модели:** Линейните модели не могат да разделят нелинейно разпределени данни без трансформации.
- **Неправилна визуализация:** Визуализациите в 2D или 3D могат да бъдат подвеждащи, ако данните са с много висока размерност.
- **Забравяне на нормализация:** Различните мащаби на признаците могат да изкривят геометричните отношения.

## 7. Short Retrieval Quiz
1. Какво представлява хиперплоскост в контекста на ML?
2. Как се изчислява Евклидовото разстояние между две точки?
3. Защо проекциите са полезни при намаляване на размерността?
4. Какво е kernel trick и как помага при нелинейна класификация?
5. Какво означава „проклятието на размерността“?
6. Каква е ролята на векторното представяне на данните?
7. Кога бихте използвали Манхатънско разстояние вместо Евклидово?

## 8. Quick Recap
- Данните в машинното обучение могат да се разглеждат като точки във векторно пространство.
- Геометричните концепции като разстояния, ъгли и хиперплоскости са основа за много ML алгоритми.
- Проекции и намаляване на размерността помагат за опростяване и визуализация на данните.
- Kernel методите позволяват нелинейно разделяне чрез трансформация на пространството.
- Високата размерност изисква специално внимание, за да се избегнат проблеми.
- Визуализацията е мощен инструмент, но трябва да се използва внимателно.
- Правилният избор на метрики и трансформации е ключов за успешен ML модел.

## 9. Spaced Review Plan

| Време след учене | Прегледна задача                                  | Цел на прегледа                           |
|-------------------|-------------------------------------------------|------------------------------------------|
| 1 ден             | Прегледайте ключовите концепции и направете кратък тест | Укрепване на основните понятия           |
| 3 дни             | Имплементирайте проста линейна класификация и визуализация | Практическо прилагане на геометричната интуиция |
| 1 седмица         | Разгледайте kernel методи и PCA с нов набор от данни | Разширяване на знанията и свързване на идеи |
| 1 месец           | Обобщете и обяснете на колега как геометрията влияе на ML | Дългосрочно задържане и умение за комуникация |