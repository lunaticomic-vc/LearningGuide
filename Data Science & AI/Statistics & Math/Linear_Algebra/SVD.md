# SVD

## 1. Activate Prior Knowledge
- Какво знаете за матричните разложения и тяхната роля в анализа на данни?
- Как бихте използвали матрични техники за намаляване на размерността в AI системи?
- Можете ли да предвидите как една матрица може да бъде представена чрез по-прости компоненти и защо това би било полезно?

## 2. Overview
Сингулярното разлагане (Singular Value Decomposition, SVD) е фундаментален метод в линейната алгебра, който разлага произволна матрица на три по-прости матрици. Този подход позволява да се разбере структурата на данните, да се извлекат важни характеристики и да се намали размерността на големи масиви от информация.

В контекста на изкуствения интелект и софтуерното инженерство, SVD е ключов инструмент за препоръчителни системи, обработка на естествен език, компресия на изображения и други области, където е необходима ефективна обработка на големи матрици. Той помага да се открият латентни фактори, които описват скрити зависимости между данните.

Разбирането на SVD е важно, защото предоставя математическа основа за множество алгоритми и оптимизации, които подобряват производителността и точността на AI модели. Освен това, SVD е стабилен и универсален метод, който може да се прилага към всякакви матрици, включително и към такива, които не са квадратни или са с нисък ранг.

## 3. Key Concepts
- **Singular Value Decomposition (SVD)** – разлагане на матрица \( A \) на три матрици \( U \), \( \Sigma \) и \( V^T \), където \( U \) и \( V \) са ортогонални, а \( \Sigma \) е диагонална с неотрицателни стойности. Мислете за това като за разглобяване на сложен механизъм на по-прости части.
- **Singular Values** – диагоналните елементи на \( \Sigma \), които показват "силата" или важността на съответните компоненти. Те са като "енергията" на всяка основна посока.
- **Orthogonal Matrices** – матрици, чиито колони са взаимно перпендикулярни и с дължина 1, подобно на координатни оси, които са перфектно независими.
- **Rank of a Matrix** – максималният брой линейно независими редове или колони, който показва "сложността" на матрицата.
- **Low-rank Approximation** – приближение на матрица чрез по-нисък ранг, което запазва най-важната информация и премахва шума, подобно на компресия на изображение.

## 4. Step-by-step Learning Path
1. **Разберете основите на матричната алгебра**  
   *Фокус:* Ортогонални матрици, собствените стойности и вектори.  
   *Задача:* Пресметнете собствените вектори на малка симетрична матрица.  
   *Въпроси:* Какво означава ортогоналност? Защо собствените стойности са важни?

2. **Изучете дефиницията и формулата на SVD**  
   *Фокус:* Формулата \( A = U \Sigma V^T \) и ролята на всяка матрица.  
   *Задача:* Използвайте библиотека (например NumPy) за SVD на малка матрица.  
   *Въпроси:* Какво представляват матриците \( U \), \( \Sigma \) и \( V \)? Какво е значението на диагоналните стойности?

3. **Практикувайте нискорангови приближения**  
   *Фокус:* Използване на първите k сингулярни стойности за приближение.  
   *Задача:* Приближете матрица с по-малък ранг и сравнете грешката.  
   *Въпроси:* Как се избира k? Как влияе приближението върху точността?

4. **Приложете SVD за намаляване на размерността**  
   *Фокус:* Извличане на латентни фактори и компресия на данни.  
   *Задача:* Използвайте SVD за намаляване на размерността на набор от данни (например текстови вектори).  
   *Въпроси:* Как SVD помага при препоръчителни системи? Какво е латентно пространство?

5. **Интегрирайте SVD в реален AI проект**  
   *Фокус:* Оптимизация и интерпретация на резултатите.  
   *Задача:* Изградете прост препоръчителен алгоритъм с помощта на SVD.  
   *Въпроси:* Какви са ограниченията на SVD? Как да интерпретирате резултатите?

## 5. Examples
### Пример 1: SVD на малка матрица с NumPy
```python
import numpy as np

A = np.array([[3, 1], [1, 3]])
U, S, VT = np.linalg.svd(A)

print("U:\n", U)
print("Singular values:", S)
print("V^T:\n", VT)
```

### Пример 2: Нискорангово приближение
```python
k = 1  # брой сингулярни стойности
S_k = np.diag(S[:k])
U_k = U[:, :k]
VT_k = VT[:k, :]

A_approx = U_k @ S_k @ VT_k
print("Приближена матрица:\n", A_approx)
```

### Пример 3: SVD за препоръчителна система (матрица на потребители и продукти)
- Използвайте SVD, за да откриете латентни фактори, които описват предпочитанията и характеристиките на продуктите.
- Намалете размерността, за да направите препоръки по-бързо и по-точно.

## 6. Common Pitfalls
- **Игнориране на нормализацията на данните** – SVD работи по-добре, когато данните са центрирани или нормализирани.  
- **Избор на твърде голям или твърде малък k** – твърде малък k губи важна информация, твърде голям k не премахва шума.  
- **Забравяне, че SVD е чувствителен към шум** – шумните данни могат да изкривят сингулярните стойности.  
- **Опит за прилагане на SVD върху много големи матрици без оптимизация** – изчисленията могат да бъдат бавни и ресурсоемки.  
- **Неправилно интерпретиране на матриците \( U \) и \( V \)** – те не са просто "фактори", а ортогонални бази, които имат специфично значение.

## 7. Short Retrieval Quiz
1. Какво представлява сингулярното разлагане на матрица?  
2. Каква е ролята на матрицата \( \Sigma \) в SVD?  
3. Какво означава ортогоналност на матрица?  
4. Защо SVD е полезен за намаляване на размерността?  
5. Как се избира броят на сингулярните стойности за приближение?  
6. Какви са основните приложения на SVD в AI?  
7. Какво е нискорангова аппроксимация?

## 8. Quick Recap
- SVD разлага матрица на три компоненти: \( U \), \( \Sigma \), \( V^T \).  
- Сингулярните стойности в \( \Sigma \) показват важността на всяка компонента.  
- Ортогоналните матрици \( U \) и \( V \) са бази с перпендикулярни вектори.  
- Нискоранговите приближения позволяват компресия и намаляване на шума.  
- SVD е широко използван в препоръчителни системи, обработка на изображения и текст.  
- Правилната подготовка и избор на параметри са ключови за успешното приложение.  
- Изчислителната сложност изисква оптимизации при големи данни.

## 9. Spaced Review Plan

| Време след учене | Промпт за преглед                                    |
|------------------|-----------------------------------------------------|
| 1 ден            | Обяснете какво е SVD и какво представляват матриците \( U \), \( \Sigma \), \( V^T \). |
| 3 дни            | Дайте пример за нискорангова аппроксимация и защо е полезна. |
| 1 седмица        | Опишете приложението на SVD в препоръчителни системи. |
| 1 месец          | Прегледайте типичните грешки при използване на SVD и как да ги избегнете. |