# Ensemble_methods

## 1. Activate Prior Knowledge
- Какво представлява концепцията за комбиниране на няколко модела в машинното обучение и защо бихме искали да го правим?
- Какви са предимствата и недостатъците на използването на един силен модел спрямо няколко по-слаби модела?
- Можете ли да предвидите как ансамбловите методи могат да подобрят стабилността и точността на AI системи?

## 2. Overview
Ансамбловите методи са техника в машинното обучение, при която множество модели се комбинират, за да се подобри общата производителност на системата. Вместо да разчитаме на един модел, ансамбълът използва колекция от модели, които гласуват или комбинират своите предсказания, за да се получи по-точен и по-стабилен резултат.

Тези методи намират широко приложение в задачи като класификация, регресия и дори в по-сложни системи за препоръки или обработка на естествен език. Те са особено полезни, когато отделните модели имат различни грешки или се обучават върху различни части от данните.

Ансамбловите техники са важни, защото често позволяват да се преодолеят ограниченията на отделните модели, като намаляват риска от пренасищане (overfitting) и повишават устойчивостта на системата към шум и вариации в данните.

## 3. Key Concepts
- **Ensemble** – група от модели, които работят заедно, за да подобрят предсказанията. Можем да си го представим като комитет, който взема решения заедно, вместо един човек.
- **Bagging (Bootstrap Aggregating)** – техника, при която се обучават множество модели върху различни случайни подмножества от данните, за да се намали вариацията. Аналогично на група експерти, които гледат различни части от проблема.
- **Boosting** – метод, който обучава модели последователно, като всеки следващ се фокусира върху грешките на предишните. Подобно на ученик, който се учи от грешките си, за да стане по-добър.
- **Random Forest** – ансамбъл от дървета за решения, използващ bagging и случайно избиране на характеристики, за да намали корелацията между дърветата.
- **Voting** – прост метод за комбиниране на предсказанията на модели чрез гласуване (например мнозинство).
- **Stacking** – техника, при която предсказанията на няколко модела се използват като вход за нов модел, който прави окончателното предсказание.

## 4. Step-by-step Learning Path
1. **Запознайте се с основите на ансамбловите методи**
   - Фокус: Разберете защо ансамблите работят и какви проблеми решават.
   - Задача: Прочетете и обяснете на колега разликата между bagging и boosting.
   - Въпроси: Как bagging намалява вариацията? Как boosting намалява грешките?

2. **Практическо приложение на Bagging**
   - Фокус: Имплементирайте bagging с дървета за решения върху малък набор от данни.
   - Задача: Използвайте sklearn RandomForestClassifier и анализирайте резултатите спрямо единично дърво.
   - Въпроси: Какво се случва с точността при увеличаване на броя дървета? Защо?

3. **Изучаване на Boosting алгоритми**
   - Фокус: Разберете AdaBoost и Gradient Boosting.
   - Задача: Обучете AdaBoost модел и сравнете с Random Forest по отношение на грешки и време за обучение.
   - Въпроси: Как AdaBoost променя тежестите на примерите? Какво е основното предимство на Gradient Boosting?

4. **Експериментиране със Stacking**
   - Фокус: Разберете как да комбинирате различни модели с помощта на stacking.
   - Задача: Създайте stacking ансамбъл с логистична регресия като мета-модел.
   - Въпроси: Какво е ролята на мета-модела? Какви модели са подходящи за първия слой?

5. **Оптимизация и избор на ансамбъл**
   - Фокус: Научете как да избирате и оптимизирате ансамблови модели за конкретни задачи.
   - Задача: Проведете cross-validation за различни ансамбли и изберете най-добрия.
   - Въпроси: Как да избегнем overfitting при ансамбли? Как да балансираме сложност и производителност?

## 5. Examples
### Пример 1: Bagging с Random Forest
```python
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
```

### Пример 2: Boosting с AdaBoost
```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

ada = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)
ada.fit(X_train, y_train)
y_pred_ada = ada.predict(X_test)

print(f"AdaBoost Accuracy: {accuracy_score(y_test, y_pred_ada):.2f}")
```

### Пример 3: Stacking с два базови модела и логистична регресия
```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.svm import SVC

estimators = [
    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
    ('svm', SVC(probability=True, random_state=42))
]

stack = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression()
)

stack.fit(X_train, y_train)
y_pred_stack = stack.predict(X_test)

print(f"Stacking Accuracy: {accuracy_score(y_test, y_pred_stack):.2f}")
```

## 6. Common Pitfalls
- **Прекомерно доверие на един модел в ансамбъла** – ако моделите са твърде сходни, ансамбълът няма да подобри резултата. Използвайте разнообразни модели или данни.
- **Недостатъчно количество модели** – малък брой модели може да не намали достатъчно грешките.
- **Пренасищане при boosting** – ако boosting се изпълнява твърде дълго, може да се получи overfitting.
- **Игнориране на корелацията между моделите** – ансамбълът работи най-добре, когато грешките на моделите са независими.
- **Лош избор на мета-модел при stacking** – неподходящ мета-модел може да влоши резултатите.

## 7. Short Retrieval Quiz
1. Какво е основната идея зад ансамбловите методи?
2. Как bagging намалява вариацията в модела?
3. Как boosting се различава от bagging?
4. Какво представлява Random Forest?
5. Какво е ролята на мета-модела в stacking?
6. Защо е важно моделите в ансамбъла да са разнообразни?
7. Какво може да се случи, ако boosting се изпълнява твърде дълго?

## 8. Quick Recap
- Ансамбловите методи комбинират множество модели за по-добра точност и стабилност.
- Bagging намалява вариацията чрез обучение на модели върху различни подмножества от данните.
- Boosting последователно коригира грешките на предишните модели.
- Random Forest е пример за bagging с дървета за решения и случайно избиране на характеристики.
- Stacking използва мета-модел, който комбинира предсказанията на базовите модели.
- Разнообразието и независимостта на моделите са ключови за успеха на ансамбъла.
- Внимателното настройване и избягване на overfitting са критични при прилагането на ансамбли.

## 9. Spaced Review Plan

| Време след учене | Промпт за преглед                                  |
|------------------|---------------------------------------------------|
| 1 ден            | Обяснете разликата между bagging и boosting.      |
| 3 дни            | Избройте основните типове ансамблови методи.      |
| 1 седмица        | Опишете как работи stacking и ролята на мета-модела.|
| 1 месец          | Дайте пример за реална задача, където ансамбъл подобрява резултатите.|