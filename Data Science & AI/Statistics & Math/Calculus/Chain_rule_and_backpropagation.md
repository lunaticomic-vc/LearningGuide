# Chain_rule_and_backpropagation

## 1. Activate Prior Knowledge
- Какво представлява производната на функция и как се използва за измерване на промени?
- Как мислите, че машинното обучение използва производни, за да оптимизира модели?
- Как бихте описали връзката между сложни функции и техните по-прости компоненти?

## 2. Overview
В основата на обучението на невронни мрежи стои изчисляването на градиенти – мерки за това как малки промени в параметрите влияят на изхода на модела. Тук ключова роля играе **вериговото правило (chain rule)** от диференциалното смятане, което позволява да се намерят производните на сложни функции чрез разлагане на техните компоненти.

**Backpropagation** (обратното разпространение на грешката) е алгоритъм, който използва вериговото правило, за да изчисли ефективно градиентите на загубната функция спрямо параметрите на мрежата. Това е сърцето на обучението с градиентен спуск и позволява на невронните мрежи да се адаптират и подобряват.

Този процес е фундаментален за съвременните AI системи, защото осигурява механизъм за автоматично настройване на милиони параметри, без да се налага ръчно изчисляване на производни. Разбирането му е ключово не само за теоретичното познание, но и за практическата реализация и оптимизация на модели.

## 3. Key Concepts
- **Chain Rule (Веригово правило)** – метод за изчисляване на производната на сложна функция чрез умножение на производните на нейните вложени функции. Мислете за него като за "верига" от малки стъпки, всяка от които променя крайния резултат.
- **Backpropagation (Обратно разпространение)** – алгоритъм, който прилага вериговото правило, за да изчисли градиентите на загубната функция спрямо всички параметри на невронната мрежа, движещ се от изхода към входа.
- **Gradient (Градиент)** – вектор, който показва посоката и скоростта на най-бързото нарастване на функцията; в контекста на обучение, използваме градиента, за да намалим загубата.
- **Loss Function (Загубна функция)** – мярка за това колко далеч е предсказанието на модела от истинската стойност; целта е да минимизираме тази функция.
- **Parameter Update (Актуализация на параметри)** – стъпката, при която параметрите се променят в посока, обратна на градиента, за да се намали загубата.
- **Computational Graph (Изчислителен граф)** – структура, която представя изчисленията като възли (операции) и ребра (поток на данни), улесняваща прилагането на вериговото правило.

## 4. Step-by-step Learning Path
1. **Разберете основите на производните и вериговото правило**
   - Задача: Изчислете производната на функция от вида \( f(x) = \sin(x^2) \) ръчно, използвайки вериговото правило.
   - Въпроси: Какво представлява производната на сложна функция? Как се прилага вериговото правило?

2. **Запознайте се с изчислителните графи**
   - Задача: Нарисувайте изчислителен граф за функцията \( f(x,y) = (x + y)^2 \).
   - Въпроси: Какви са възлите и ребрата в графа? Как вериговото правило се прилага върху графа?

3. **Разберете алгоритъма backpropagation**
   - Задача: Използвайте изчислителния граф от предишната стъпка, за да изчислите градиентите на \( f \) спрямо \( x \) и \( y \).
   - Въпроси: Как backpropagation използва вериговото правило? Защо се движи от изхода към входа?

4. **Имплементирайте backpropagation за проста невронна мрежа**
   - Задача: Напишете код, който изчислява градиентите на еднослойна невронна мрежа с една скрита единица.
   - Въпроси: Как се актуализират параметрите? Какво е значението на learning rate?

5. **Приложете backpropagation в реален ML фреймуърк**
   - Задача: Използвайте TensorFlow или PyTorch, за да обучите малка мрежа и разгледайте автоматичното изчисление на градиенти.
   - Въпроси: Как фреймуъркът автоматизира backpropagation? Какво е предимството на автоматичното диференциране?

## 5. Examples
### Пример 1: Веригово правило за проста функция
Функция: \( f(x) = \sin(x^2) \)

Производна:
\[
\frac{df}{dx} = \cos(x^2) \cdot 2x
\]

### Пример 2: Backpropagation в еднослойна невронна мрежа (Python)
```python
import numpy as np

# Вход и параметри
x = np.array([1.0, 2.0])
w = np.array([0.5, -0.5])
b = 0.0

# Forward pass
z = np.dot(w, x) + b
y = 1 / (1 + np.exp(-z))  # сигмоидна активация

# Загубна функция: (y - target)^2
target = 1.0
loss = (y - target) ** 2

# Backpropagation
dloss_dy = 2 * (y - target)
dy_dz = y * (1 - y)
dz_dw = x

# Градиенти
dloss_dw = dloss_dy * dy_dz * dz_dw
dloss_db = dloss_dy * dy_dz

print("Gradient w.r.t weights:", dloss_dw)
print("Gradient w.r.t bias:", dloss_db)
```

### Пример 3: Автоматично диференциране с PyTorch
```python
import torch

x = torch.tensor([1.0, 2.0], requires_grad=True)
w = torch.tensor([0.5, -0.5], requires_grad=True)
b = torch.tensor(0.0, requires_grad=True)

z = torch.dot(w, x) + b
y = torch.sigmoid(z)
target = torch.tensor(1.0)

loss = (y - target) ** 2
loss.backward()

print("Gradient w.r.t weights:", w.grad)
print("Gradient w.r.t bias:", b.grad)
```

## 6. Common Pitfalls
- **Игнориране на вериговото правило** – опит за директно диференциране на сложна функция без разбиване на стъпки води до грешки.
- **Неправилно изчисляване на градиенти при backpropagation** – често се бъркат знаците или реда на умножение.
- **Пренебрегване на размерностите** – несъответствия в размерите на матриците и векторите водят до runtime грешки.
- **Забравяне за learning rate** – твърде голям или твърде малък learning rate може да спре обучението или да го направи нестабилно.
- **Неизползване на автоматично диференциране** – ръчното изчисляване на градиенти е податливо на грешки и неефективно при големи мрежи.

## 7. Short Retrieval Quiz
1. Какво е веригово правило и защо е важно за backpropagation?
2. Как backpropagation изчислява градиентите в невронна мрежа?
3. Каква е ролята на загубната функция в обучението на мрежа?
4. Какво означава gradient и как се използва при оптимизация?
5. Какво представлява изчислителният граф?
6. Защо е важно да се движим от изхода към входа при backpropagation?
7. Как автоматичното диференциране улеснява обучението на модели?

## 8. Quick Recap
- Вериговото правило позволява изчисляване на производни на сложни функции чрез умножение на производните на вложените функции.
- Backpropagation използва вериговото правило, за да изчисли градиентите на загубната функция спрямо параметрите на невронната мрежа.
- Градиентите показват посоката за актуализация на параметрите с цел минимизиране на загубата.
- Изчислителният граф визуализира изчисленията и улеснява прилагането на вериговото правило.
- Практическото прилагане на backpropagation е в основата на обучението на съвременни AI модели.
- Автоматичното диференциране в ML фреймуъркове автоматизира и оптимизира процеса.
- Вниманието към размерности, learning rate и правилна имплементация е ключово за успешното обучение.

## 9. Spaced Review Plan

| Време след изучаване | Промпт за преговор                                   |
|----------------------|-----------------------------------------------------|
| 1 ден                | Обяснете с прости думи как работи вериговото правило. |
| 3 дни                | Опишете стъпките на backpropagation и ролята на градиентите. |
| 1 седмица            | Имплементирайте backpropagation за проста функция или мрежа. |
| 1 месец              | Анализирайте как автоматичното диференциране оптимизира обучението. |