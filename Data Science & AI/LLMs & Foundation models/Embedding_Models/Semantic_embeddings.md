# Semantic_embeddings

## 1. Activate Prior Knowledge
- Какво разбирате под понятието „векторно представяне“ на текст или данни в контекста на изкуствения интелект?
- Как бихте използвали числови представяния на думи или изречения, за да подобрите търсенето или препоръчителните системи?
- Какви са предимствата и ограниченията на традиционните методи за обработка на текст спрямо подходите, базирани на семантични embeddings?

## 2. Overview
Semantic embeddings са числови векторни представяния на думи, изречения или дори цели документи, които улавят смисъла и контекста на езиковите единици. Те позволяват на компютърните системи да разберат и сравняват текстове не само по буквално съвпадение, а по семантична близост. Това е ключово за множество приложения в изкуствения интелект, като търсачки, чатботове, препоръчителни системи и машинен превод.

В по-широката архитектура на AI системите, semantic embeddings служат като слой, който трансформира неструктуриран текст в структурирани числови данни, удобни за обработка от алгоритми за машинно обучение. Те са мост между човешкия език и математическите модели, които го анализират.

Значението на semantic embeddings се крие в тяхната способност да улеснят разбирането на езика на по-дълбоко ниво, отколкото простото търсене на ключови думи. Това позволява по-интелигентни и адаптивни системи, които могат да интерпретират нюанси, синоними и контекстуални значения.

## 3. Key Concepts
- **Embedding** – числов вектор, който представя обект (например дума или изречение) в многомерно пространство, където близостта между векторите отразява семантична близост.
- **Semantic similarity** – мярка за това колко близки са по смисъл два текста, изчислена чрез сравняване на техните embeddings.
- **Vector space model** – математически модел, в който текстовете се представят като точки или вектори в пространството, позволяващ операции като намиране на разстояния и ъгли между тях.
- **Contextual embeddings** – embeddings, които взимат предвид контекста на думата в изречението, например чрез модели като BERT, за разлика от статичните embeddings като Word2Vec.
- **Dimensionality reduction** – процес на намаляване на броя на измеренията в embedding пространството, за да се улесни обработката и визуализацията, без да се губи съществена информация.
- **Cosine similarity** – често използвана метрика за измерване на ъгловото разстояние между два вектора, показваща колко сходни са те по посока.

## 4. Step-by-step Learning Path
1. **Запознаване с основите на embeddings**
   - Фокус: Разберете как се представят думи като вектори (Word2Vec, GloVe).
   - Практическа задача: Използвайте библиотека като Gensim, за да заредите предварително обучен Word2Vec модел и намерете най-близките думи до „учене“.
   - Въпроси: Какво означава, че две думи са близки в embedding пространството? Защо това е полезно?

2. **Изследване на контекстуални embeddings**
   - Фокус: Научете как BERT и подобни модели създават embeddings, които зависят от контекста.
   - Практическа задача: Използвайте Hugging Face Transformers, за да извлечете embeddings на изречения и сравнете резултатите при различен контекст.
   - Въпроси: Как контекстът променя embedding на една и съща дума? Как това подобрява разбирането на текста?

3. **Измерване на семантична близост**
   - Фокус: Научете как да използвате cosine similarity и други метрики за сравнение на embeddings.
   - Практическа задача: Напишете функция, която приема две изречения и връща тяхната семантична близост.
   - Въпроси: Защо cosine similarity е предпочитана метрика? Какво означава стойност близка до 1?

4. **Прилагане в реални системи**
   - Фокус: Интегрирайте semantic embeddings в търсачка или препоръчителна система.
   - Практическа задача: Създайте прост търсач, който намира най-подходящите документи по семантичен критерий.
   - Въпроси: Как embeddings подобряват резултатите спрямо традиционното търсене по ключови думи? Какви са ограниченията?

## 5. Examples

**Пример 1: Намиране на близки думи с Word2Vec**

```python
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print(model.most_similar('learning', topn=5))
```

**Пример 2: Извличане на контекстуални embeddings с BERT**

```python
from transformers import BertTokenizer, BertModel
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

sentence = "Machine learning is fascinating."
inputs = tokenizer(sentence, return_tensors='pt')
outputs = model(**inputs)
embedding = outputs.last_hidden_state.mean(dim=1)  # среден вектор за изречението
print(embedding)
```

**Пример 3: Изчисляване на cosine similarity между две изречения**

```python
from sklearn.metrics.pairwise import cosine_similarity

vec1 = embedding1.detach().numpy()
vec2 = embedding2.detach().numpy()
similarity = cosine_similarity(vec1, vec2)
print(f"Cosine similarity: {similarity[0][0]:.4f}")
```

## 6. Common Pitfalls
- **Използване на неподходящи embeddings за задачата** – статичните embeddings не улавят контекста, което може да доведе до грешни заключения.
- **Пренебрегване на нормализацията на векторите** – липсата на нормализация може да изкриви резултатите при сравнения.
- **Използване на твърде висока размерност без нужда** – води до по-бавна обработка и риск от overfitting.
- **Игнориране на семантичните нюанси** – embeddings не винаги улавят ирония, сарказъм или сложни езикови конструкции.
- **Прекалено доверие на similarity метриките** – висока семантична близост не винаги означава релевантност в конкретен контекст.

## 7. Short Retrieval Quiz
1. Какво представлява semantic embedding?
2. Каква е разликата между статични и контекстуални embeddings?
3. Коя метрика често се използва за измерване на семантична близост между вектори?
4. Защо е важно да се използват embeddings в NLP системи?
5. Какво означава, че две думи са „близки“ в embedding пространството?
6. Как контекстът влияе на embedding на дума?
7. Какви са рисковете при използване на embeddings с висока размерност?

## 8. Quick Recap
- Semantic embeddings са числови векторни представяния, които улавят смисъла на текст.
- Те позволяват на AI системите да сравняват и обработват език на по-дълбоко ниво.
- Статичните embeddings са фиксирани за дадена дума, докато контекстуалните се адаптират според изречението.
- Cosine similarity е стандартна метрика за измерване на семантична близост.
- Правилната употреба на embeddings подобрява търсене, класификация и препоръчване.
- Важно е да се избягват често срещани грешки като неподходящ избор на модел и липса на нормализация.
- Практическите задачи и експерименти са ключови за дълбоко разбиране.

## 9. Spaced Review Plan

| Време след изучаване | Промпт за преговор                                      |
|----------------------|--------------------------------------------------------|
| 1 ден                | Обяснете какво е semantic embedding и защо е важен.    |
| 3 дни                | Опишете разликите между статични и контекстуални embeddings. |
| 1 седмица            | Демонстрирайте как да изчислите семантична близост между две изречения. |
| 1 месец              | Приложете semantic embeddings в малък проект за търсене или препоръчване. |