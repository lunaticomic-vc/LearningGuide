# Optimization_in_deep_learning

## 1. Activate Prior Knowledge

- Какво представлява функцията на загуба (loss function) и защо е важна за обучението на невронни мрежи?
- Какви са основните предизвикателства при намирането на оптимални параметри в сложни модели като дълбоки невронни мрежи?
- Как бихте обяснили ролята на градиентния спуск в процеса на обучение на изкуствени невронни мрежи?

## 2. Overview

Оптимизацията в дълбокото обучение е процесът на намиране на най-добрите параметри (тегла и пристрастия) на невронната мрежа, които минимизират функцията на загуба. Този процес е сърцето на обучението и определя колко добре моделът ще се представя на нови, невиждани данни.

В по-широк контекст, оптимизацията е част от цикъла на обучение, който включва събиране на данни, дефиниране на архитектура, избор на функция на загуба и метод за оптимизация. Без ефективна оптимизация, дори най-сложните архитектури няма да могат да научат полезни представяния.

Оптимизационните алгоритми трябва да се справят с високоизмерни, нелинейни и често шумни функции на загуба, което прави задачата предизвикателна. Разбирането на тези алгоритми и техните свойства е ключово за създаване на стабилни и ефективни AI системи.

## 3. Key Concepts

- **Функция на загуба (Loss Function)** – Математическа функция, която измерва колко далеч е предсказанието на модела от истинската стойност. Аналогично на "цена", която плащаме за грешка.
- **Градиентен спуск (Gradient Descent)** – Итеративен метод за минимизиране на функцията на загуба чрез движение в посока на най-стръмното намаление (градиента). Представете си топка, която се търкаля надолу по хълм към най-ниската точка.
- **Скорост на учене (Learning Rate)** – Хиперпараметър, който определя размера на стъпката при всяка итерация на градиентния спуск. Твърде голяма стъпка може да пропусне минимума, твърде малка – да забави обучението.
- **Стохастичен градиентен спуск (SGD)** – Вариант на градиентния спуск, който използва случайни подмножества (батчове) от данните за изчисляване на градиента, ускорявайки обучението и помагайки за избягване на локални минимуми.
- **Моментум (Momentum)** – Техника, която ускорява градиентния спуск, като добавя "инерция" към движението, подобно на това как движещо се тяло продължава да се движи.
- **Адаптивни оптимизатори (Adam, RMSprop и др.)** – Алгоритми, които автоматично коригират скоростта на учене за всеки параметър, подобрявайки стабилността и скоростта на обучението.
- **Локални и глобални минимуми** – Точки, където функцията на загуба е минимална в дадена област (локални), или в целия параметричен простор (глобални). Оптимизаторите се стремят към глобалния минимум, но често попадат в локални.

## 4. Step-by-step Learning Path

1. **Разберете функцията на загуба и нейното значение**
   - Фокус: Изучете различни функции на загуба (MSE, Cross-Entropy).
   - Задача: Имплементирайте MSE функция и изчислете загубата за дадени предсказания.
   - Въпроси: Какво означава минимизиране на загубата? Защо различните задачи изискват различни функции на загуба?

2. **Запознайте се с градиентния спуск**
   - Фокус: Разберете как се изчислява градиент и как той се използва за актуализация на параметрите.
   - Задача: Ръчно изчислете една стъпка на градиентен спуск за проста функция.
   - Въпроси: Какво представлява градиентът? Как скоростта на учене влияе на обучението?

3. **Изучете стохастичния градиентен спуск и батчовете**
   - Фокус: Разберете предимствата на SGD спрямо пълния градиентен спуск.
   - Задача: Имплементирайте SGD с малък батч върху синтетични данни.
   - Въпроси: Какво е батч? Защо използваме батчове?

4. **Запознайте се с усъвършенствани оптимизатори**
   - Фокус: Научете как работят Adam и RMSprop.
   - Задача: Сравнете поведението на SGD и Adam върху една и съща задача.
   - Въпроси: Как Adam адаптира скоростта на учене? Кога е по-добре да използваме Adam?

5. **Практикувайте настройка на хиперпараметри**
   - Фокус: Експериментирайте с различни стойности на скорост на учене и батч размер.
   - Задача: Обучете малка невронна мрежа с различни параметри и анализирайте резултатите.
   - Въпроси: Какви са признаците на прекомерно обучение? Как да изберем оптимална скорост на учене?

## 5. Examples

### Пример 1: Градиентен спуск за проста квадратична функция

```python
def f(x):
    return x**2

def grad_f(x):
    return 2*x

x = 10.0
learning_rate = 0.1

for i in range(10):
    grad = grad_f(x)
    x = x - learning_rate * grad
    print(f"Step {i+1}: x={x}, f(x)={f(x)}")
```

### Пример 2: Използване на Adam оптимизатор в PyTorch

```python
import torch
import torch.nn as nn
import torch.optim as optim

model = nn.Linear(10, 1)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

inputs = torch.randn(5, 10)
targets = torch.randn(5, 1)

optimizer.zero_grad()
outputs = model(inputs)
loss = criterion(outputs, targets)
loss.backward()
optimizer.step()
print(f"Loss: {loss.item()}")
```

### Пример 3: Влияние на скоростта на учене

- Обучете една и съща мрежа с lr=0.1 и lr=0.0001 и наблюдавайте разликите в конвергенцията.

## 6. Common Pitfalls

- **Твърде голяма скорост на учене** – Моделът може да прескача минимума и да не конвергира. Използвайте техники като намаляване на скоростта на учене или адаптивни оптимизатори.
- **Прекалено малка скорост на учене** – Обучението става бавно и може да заседне в локални минимуми.
- **Игнориране на батч размера** – Малки батчове водят до шумни градиенти, големи – до бавна актуализация и повече памет.
- **Недостатъчно внимание към инициализацията на параметрите** – Лошата инициализация може да забави или блокира обучението.
- **Прекомерно доверие в оптимизатора** – Оптимизаторът не може да компенсира лошо проектиран модел или неподходяща функция на загуба.

## 7. Short Retrieval Quiz

1. Какво представлява градиентът и как се използва в оптимизацията?
2. Каква е ролята на скоростта на учене?
3. Какво е разликата между пълен градиентен спуск и стохастичен градиентен спуск?
4. Защо използваме адаптивни оптимизатори като Adam?
5. Какво може да се случи, ако скоростта на учене е твърде голяма?
6. Какво е батч и как влияе на обучението?
7. Какво представлява функцията на загуба?

## 8. Quick Recap

- Оптимизацията е процесът на минимизиране на функцията на загуба чрез актуализация на параметрите.
- Градиентният спуск е основният метод за оптимизация в дълбокото обучение.
- Скоростта на учене е критичен хиперпараметър, който контролира стъпките на актуализация.
- Стохастичният градиентен спуск използва случайни подмножества от данни за по-бързо и по-стабилно обучение.
- Адаптивните оптимизатори като Adam автоматично регулират скоростта на учене за всеки параметър.
- Правилната настройка на хиперпараметрите и инициализацията са ключови за успешна оптимизация.
- Избягвайте често срещани грешки като твърде голяма или твърде малка скорост на учене и неподходящ избор на функция на загуба.

## 9. Spaced Review Plan

| Време след изучаване | Промпти за преговор                                   |
|----------------------|-------------------------------------------------------|
| 1 ден                | Обяснете как работи градиентният спуск и ролята на скоростта на учене. |
| 3 дни                | Сравнете SGD и Adam. Кога бихте избрали единия пред другия?          |
| 1 седмица            | Опишете как батч размерът влияе на процеса на оптимизация.           |
| 1 месец              | Прегледайте основните типове функции на загуба и тяхното приложение. |