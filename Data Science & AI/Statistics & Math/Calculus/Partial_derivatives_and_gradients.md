# Partial_derivatives_and_gradients

## 1. Activate Prior Knowledge

- Какво представлява производната на функция с една променлива и как тя описва промяната на функцията?
- Как бихте обяснили ролята на частичните производни в оптимизацията на многоизмерни функции, например при обучение на невронни мрежи?
- Как смятате, че градиентът помага на софтуерните системи да намират оптимални решения?

## 2. Overview

Частичните производни са разширение на концепцията за производна към функции с множество променливи. Те измерват скоростта на промяна на функцията спрямо една от променливите, докато останалите се държат константни. Това е фундаментално в много области на науката и инженерството, особено в машинното обучение и оптимизацията.

Градиентът е вектор, съставен от всички частични производни на функцията. Той показва посоката на най-бързо нарастване на функцията и се използва широко в алгоритми за оптимизация, като градиентен спуск. В контекста на изкуствения интелект, градиентите са ключови за обучението на модели, тъй като позволяват корекция на параметрите с цел минимизиране на грешката.

Разбирането на частичните производни и градиентите е основа за работа с многоизмерни функции и е незаменимо при разработката на ефективни и точни AI системи, както и при решаването на инженерни задачи, свързани с оптимизация.

## 3. Key Concepts

- **Partial derivative (Частична производна)** – мярка за скоростта на промяна на функция с много променливи спрямо една конкретна променлива, докато другите се държат фиксирани. Може да се представи като „срез“ на функцията по една ос.
- **Gradient (Градиент)** – вектор, състоящ се от всички частични производни на функцията, който сочи посоката на най-бързо увеличение на функцията. Мислете за него като за стрелка, показваща накъде да се движите, за да увеличите стойността най-бързо.
- **Directional derivative (Дирекционна производна)** – производна на функцията в произволна посока, която се изчислява чрез скаларното произведение на градиента с вектор, указващ посоката.
- **Gradient descent (Градиентен спуск)** – итеративен метод за намиране на минимум на функция чрез движение в посоката, обратна на градиента.
- **Jacobian (Якобиан)** – матрица от първите частични производни на векторна функция, важна за трансформации и оптимизация.

## 4. Step-by-step Learning Path

1. **Разберете дефиницията на частична производна**
   - Фокус: Как се дефинира частичната производна и как се изчислява.
   - Задача: Изчислете частичните производни на функцията \( f(x,y) = x^2 y + 3xy^2 \).
   - Въпроси: Какво означава да държим една променлива константна? Защо е важно това?

2. **Научете как се формира градиентът**
   - Фокус: Съставяне на градиентен вектор от частичните производни.
   - Задача: Изчислете градиента на функцията от предната стъпка.
   - Въпроси: Каква е ролята на градиента в намирането на екстремуми?

3. **Приложете градиентния спуск за оптимизация**
   - Фокус: Разбиране на алгоритъма и неговото приложение.
   - Задача: Имплементирайте прост градиентен спуск за функция \( f(x,y) \) с начална точка и стъпка.
   - Въпроси: Защо градиентният спуск използва отрицателния градиент? Как се избира стъпковият размер?

4. **Изследвайте градиенти в контекста на машинното обучение**
   - Фокус: Ролята на градиентите при обучение на модели.
   - Задача: Прегледайте backpropagation алгоритъма и идентифицирайте къде се използват частични производни.
   - Въпроси: Как градиентите помагат за корекция на параметрите в невронна мрежа?

5. **Работа с векторни функции и Якобиан**
   - Фокус: Разширяване към векторни функции и матрични производни.
   - Задача: Изчислете Якобиана на функция \( \mathbf{F}(x,y) = (x^2 y, \sin(xy)) \).
   - Въпроси: Каква е разликата между градиент и Якобиан?

## 5. Examples

**Пример 1: Частични производни**

Функция:  
\[ f(x,y) = x^2 y + 3xy^2 \]

Частични производни:  
\[
\frac{\partial f}{\partial x} = 2xy + 3y^2, \quad \frac{\partial f}{\partial y} = x^2 + 6xy
\]

---

**Пример 2: Градиентен спуск на Python**

```python
def f(x, y):
    return x**2 * y + 3 * x * y**2

def grad_f(x, y):
    df_dx = 2 * x * y + 3 * y**2
    df_dy = x**2 + 6 * x * y
    return df_dx, df_dy

x, y = 1.0, 1.0
learning_rate = 0.01

for _ in range(100):
    dx, dy = grad_f(x, y)
    x -= learning_rate * dx
    y -= learning_rate * dy

print(f"Минимум при x={x:.4f}, y={y:.4f}")
```

---

**Пример 3: Якобиан на векторна функция**

Функция:  
\[
\mathbf{F}(x,y) = \begin{bmatrix} x^2 y \\ \sin(xy) \end{bmatrix}
\]

Якобиан:  
\[
J = \begin{bmatrix}
\frac{\partial}{\partial x}(x^2 y) & \frac{\partial}{\partial y}(x^2 y) \\
\frac{\partial}{\partial x}(\sin(xy)) & \frac{\partial}{\partial y}(\sin(xy))
\end{bmatrix} =
\begin{bmatrix}
2xy & x^2 \\
y \cos(xy) & x \cos(xy)
\end{bmatrix}
\]

## 6. Common Pitfalls

- **Игнориране на фиксирането на останалите променливи при частичните производни** – води до грешни изчисления.
- **Смесване на градиент с частична производна** – градиентът е вектор, а частичната производна е скаларна.
- **Неправилен избор на стъпков размер в градиентния спуск** – твърде голям може да доведе до прескачане на минимума, твърде малък – до бавна конвергенция.
- **Пренебрегване на векторния характер на градиента при многомерни функции** – важно е да се работи с вектори и матрици, а не само с числа.
- **Забравяне, че градиентът сочи към локален максимум, а не минимум** – затова за минимизация се използва отрицателният градиент.

## 7. Short Retrieval Quiz

1. Какво измерва частичната производна на функция с много променливи?
2. Какво представлява градиентът и каква е неговата геометрична интерпретация?
3. Защо градиентният спуск използва отрицателния градиент?
4. Каква е разликата между градиент и Якобиан?
5. Какво означава да „държим променливите константни“ при изчисляване на частична производна?
6. Как градиентът се използва в обучението на невронни мрежи?
7. Какви са рисковете при избор на твърде голям стъпков размер в градиентния спуск?

## 8. Quick Recap

- Частичната производна измерва промяната на функция спрямо една променлива, докато другите са фиксирани.
- Градиентът е вектор от всички частични производни и сочи посоката на най-бързо увеличение.
- Градиентният спуск използва отрицателния градиент, за да намери минимум на функция.
- Якобианът е матрица от частични производни за векторни функции.
- Правилното разбиране на тези концепции е ключово за оптимизация и обучение на AI модели.
- Изчисленията трябва да се извършват внимателно, като се държат останалите променливи константни.
- Изборът на параметри при алгоритмите за оптимизация влияе значително върху резултатите.

## 9. Spaced Review Plan

| Време след учене | Прегледна задача                                  | Цел                                      |
|------------------|-------------------------------------------------|------------------------------------------|
| 1 ден            | Преговор на дефинициите и изчисляване на частични производни | Укрепване на основните понятия            |
| 3 дни            | Имплементиране на градиентен спуск за проста функция | Практическо приложение и разбиране       |
| 1 седмица        | Анализ на градиентите в контекста на машинно обучение | Свързване на теорията с реални AI задачи |
| 1 месец          | Решаване на задачи с Якобиан и многомерни функции | Задълбочаване и интеграция на знанията   |