# Autoencoders_and_VAEs

## 1. Activate Prior Knowledge
- Какво представлява компресията на данни и защо е важна в машинното обучение?
- Как бихте описали ролята на невронните мрежи в извличането на характеристики от сложни данни?
- Какво знаете за вероятностните модели и тяхното приложение в генеративните задачи?

## 2. Overview
Автоенкодерите (Autoencoders) са вид невронни мрежи, които се обучават да възстановяват входните си данни след като ги компресират в по-нискоизмерно представяне, наречено латентно пространство. Те са ключов инструмент за безнадзорно обучение, използван за намаляване на размерността, откриване на аномалии и предварителна обработка на данни.

Вариационните автоенкодери (Variational Autoencoders, VAEs) разширяват тази идея, като въвеждат вероятностен подход към латентното пространство. Вместо да кодират входа в точкова стойност, VAEs моделират разпределение, което позволява генерация на нови данни, близки до обучаващите се примери. Това ги прави мощен инструмент в генеративното моделиране.

Тези модели намират приложение в широк спектър от задачи – от компресиране на изображения и откриване на аномалии до генериране на нови образци в медицината, изкуството и роботиката. Разбирането на автоенкодерите и VAEs е фундаментално за всеки, който иска да работи с модерни AI системи, които изискват ефективно представяне и генерация на данни.

## 3. Key Concepts
- **Autoencoder (Автоенкодер)** – Невронна мрежа, която се обучава да възстановява входа си чрез компресиране в латентно пространство и декомпресиране обратно. Мислете за него като за „копирна машина“, която първо намалява детайлите, а после се опитва да ги възстанови.
- **Encoder (Кодировач)** – Частта от автоенкодера, която трансформира входните данни в компактно латентно представяне.
- **Decoder (Декодировач)** – Частта, която възстановява данните от латентното пространство обратно към оригиналния формат.
- **Latent Space (Латентно пространство)** – Нискиизмерното пространство, в което се представят компресираните данни. Може да се мисли като „езикът“, на който мрежата описва входа.
- **Variational Autoencoder (Вариационен автоенкодер, VAE)** – Автоенкодер, който моделира латентното пространство като вероятностно разпределение, позволявайки генерация на нови данни чрез проби от това разпределение.
- **Reparameterization Trick (Трик за репараметризация)** – Техника, която позволява диференциране през случайни променливи, използвана за обучение на VAEs.
- **KL Divergence (Кулбак-Лайблерова дивергенция)** – Мярка за разликата между две вероятностни разпределения, използвана в загубната функция на VAE за насърчаване на латентното разпределение да бъде близко до предварително зададено (например нормално).

## 4. Step-by-step Learning Path
1. **Разбиране на основите на автоенкодерите**
   - Фокус: Архитектура на автоенкодер – encoder, latent space, decoder.
   - Задача: Имплементирайте прост автоенкодер върху MNIST dataset.
   - Въпроси: Какво представлява латентното пространство? Защо е важно компресирането?

2. **Обучение и оценка на автоенкодер**
   - Фокус: Загубна функция (reconstruction loss) и оптимизация.
   - Задача: Обучете автоенкодера и визуализирайте входа и възстановените изходи.
   - Въпроси: Как се измерва качеството на възстановяване? Какво се случва при прекомерно компресиране?

3. **Въвеждане във вариационните автоенкодери**
   - Фокус: Разликата между детерминистичен и вероятностен подход.
   - Задача: Имплементирайте прост VAE и разгледайте латентното пространство.
   - Въпроси: Какво е ролята на KL дивергенцията? Защо използваме трика за репараметризация?

4. **Генериране на нови данни с VAE**
   - Фокус: Пробиране от латентното пространство и декодиране.
   - Задача: Генерирайте нови изображения от латентни вектори.
   - Въпроси: Как се гарантира, че генерираните данни са реалистични? Какво се случва, ако пробираме далеч от обученото разпределение?

5. **Приложения и разширения**
   - Фокус: Аномалия детекция, semi-supervised learning, conditional VAEs.
   - Задача: Използвайте VAE за откриване на аномалии в набор от данни.
   - Въпроси: Какво прави VAE подходящ за аномалия детекция? Как може да се разшири архитектурата за условно генериране?

## 5. Examples
### Пример 1: Прост автоенкодер на MNIST (PyTorch)
```python
import torch
import torch.nn as nn
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

class Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28*28, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 12),
            nn.ReLU(),
            nn.Linear(12, 3)
        )
        self.decoder = nn.Sequential(
            nn.Linear(3, 12),
            nn.ReLU(),
            nn.Linear(12, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 28*28),
            nn.Sigmoid()
        )
    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# Зареждане на данни и обучение оставяме като упражнение
```

### Пример 2: Основен VAE (архитектура)
```python
class VAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 400)
        self.fc21 = nn.Linear(400, 20)  # mean
        self.fc22 = nn.Linear(400, 20)  # log variance
        self.fc3 = nn.Linear(20, 400)
        self.fc4 = nn.Linear(400, 28*28)

    def encode(self, x):
        h1 = torch.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def decode(self, z):
        h3 = torch.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 28*28))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
```

### Пример 3: Генериране на нови изображения
```python
with torch.no_grad():
    z = torch.randn(64, 20)  # пробираме от нормално разпределение
    sample = model.decode(z).cpu()
    # визуализирайте sample като изображения
```

## 6. Common Pitfalls
- **Прекомерно компресиране на латентното пространство** – води до загуба на важна информация и лошо възстановяване.
- **Игнориране на KL дивергенцията във VAE** – без нея моделът не научава гладко и структуриран латентен код.
- **Неправилно прилагане на трика за репараметризация** – води до невъзможност за обучение чрез backpropagation.
- **Използване на неподходящи размери на латентното пространство** – твърде малко или твърде голямо пространство затруднява обучението и генерацията.
- **Липса на нормализация и подходящи функции за активация** – може да доведе до нестабилно обучение.

## 7. Short Retrieval Quiz
1. Каква е основната цел на автоенкодерите?
2. Какво представлява латентното пространство?
3. Каква е разликата между автоенкодер и вариационен автоенкодер?
4. Защо в VAE използваме KL дивергенция?
5. Какво представлява трикът за репараметризация?
6. Как може да се използва VAE за генериране на нови данни?
7. Коя част от автоенкодера отговаря за възстановяването на входа?

## 8. Quick Recap
- Автоенкодерите компресират и възстановяват данни чрез латентно пространство.
- Вариационните автоенкодери моделират латентното пространство като вероятностно разпределение.
- Трикът за репараметризация позволява обучение на VAEs чрез backpropagation.
- KL дивергенцията насърчава латентното разпределение да бъде близко до предварително зададено.
- Автоенкодерите и VAEs са мощни инструменти за безнадзорно обучение и генерация на данни.
- Правилният избор на размер на латентното пространство и загубна функция е ключов за успеха.
- Приложенията им включват компресия, аномалия детекция и генеративни модели.

## 9. Spaced Review Plan

| Време след първоначално учене | Промпт за преговор                                    |
|-------------------------------|------------------------------------------------------|
| 1 ден                         | Обяснете какво е автоенкодер и как работи.          |
| 3 дни                         | Каква е ролята на KL дивергенцията във VAE?          |
| 1 седмица                    | Опишете трика за репараметризация и защо е нужен.   |
| 1 месец                      | Дайте пример за приложение на VAE в реален проект.   |