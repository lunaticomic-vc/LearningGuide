# Encoder_decoder_vs_decoder_only

## 1. Activate Prior Knowledge
- Какво знаете за архитектурите на невронни мрежи, използвани в обработката на естествен език (NLP)?
- Каква е разликата между модели, които генерират текст последователно, и такива, които обработват вход и изход като отделни части?
- Можете ли да предвидите как различните архитектури влияят върху задачите като машинен превод или автогенериране на текст?

## 2. Overview
В съвременните AI системи за обработка на естествен език (NLP) голямо значение имат архитектурите на трансформърите, които се делят основно на две категории: encoder-decoder и decoder-only. Encoder-decoder моделите имат две отделни части – енкодер, който обработва входната информация, и декодер, който генерира изхода. Тази архитектура е особено подходяща за задачи, където има ясно разделение между вход и изход, като машинен превод или обобщаване на текст.

От друга страна, decoder-only моделите използват само декодерна част, която последователно генерира текст, като се основава на предишния контекст. Тези модели са по-гъвкави за задачи, свързани с автогенериране на текст, като чатботове или творческо писане. Разбирането на разликите между тези архитектури е ключово за избор на подходящ модел за конкретна задача и оптимизиране на производителността и качеството на резултатите.

## 3. Key Concepts
- **Encoder** – част от модела, която приема входни данни и ги преобразува в абстрактно, плътно представяне (embedding). Може да се сравни с преводач, който разбира смисъла на текста.
- **Decoder** – част от модела, която използва представянето от енкодера (или собствената си история) за генериране на изходен текст. Аналогично на писател, който създава текст, базиран на разбраната информация.
- **Encoder-Decoder Architecture** – модел с две отделни части, където енкодерът разбира входа, а декодерът генерира изхода. Подходящ за задачи с ясно разделение между вход и изход.
- **Decoder-Only Architecture** – модел, който използва само декодерна част, генерирайки текст последователно, като се основава на предишния контекст. Подходящ за задачи с автогенериране.
- **Self-Attention** – механизъм, чрез който моделът обръща внимание на различни части от входа или генерирания текст, за да улови контекста.
- **Cross-Attention** – в encoder-decoder моделите, декодерът използва cross-attention, за да се фокусира върху релевантни части от енкодера.

## 4. Step-by-step Learning Path
1. **Фокус:** Разберете основната структура на трансформър архитектурата.  
   **Задача:** Прегледайте диаграма на encoder-decoder трансформър и маркирайте енкодера и декодера.  
   **Въпроси:** Каква е ролята на енкодера? Как декодерът използва информацията от енкодера?

2. **Фокус:** Изучете механизма на self-attention и cross-attention.  
   **Задача:** Напишете кратко обяснение с примери за self-attention и cross-attention.  
   **Въпроси:** Какво различава self-attention от cross-attention? В коя архитектура се използва cross-attention?

3. **Фокус:** Сравнете encoder-decoder и decoder-only модели по отношение на задачи и производителност.  
   **Задача:** Изберете две NLP задачи (например машинен превод и чатбот) и аргументирайте коя архитектура е по-подходяща за всяка.  
   **Въпроси:** Защо encoder-decoder е по-добър за машинен превод? Кога decoder-only моделите са по-ефективни?

4. **Фокус:** Практическа имплементация на decoder-only модел с библиотека като Hugging Face Transformers.  
   **Задача:** Заредете предварително обучен GPT модел и генерирайте текст на базата на даден prompt.  
   **Въпроси:** Как се задава началният контекст? Как се контролира дължината на генерирания текст?

5. **Фокус:** Практическа имплементация на encoder-decoder модел за машинен превод.  
   **Задача:** Използвайте T5 или BART модел за превод от английски на български.  
   **Въпроси:** Как се подава входът към енкодера? Как се извлича изходът от декодера?

## 5. Examples
### Пример 1: Encoder-decoder за машинен превод с Hugging Face (Python)
```python
from transformers import MarianMTModel, MarianTokenizer

model_name = 'Helsinki-NLP/opus-mt-en-bg'
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors="pt")
translated = model.generate(**inputs)
print(tokenizer.decode(translated[0], skip_special_tokens=True))
```

### Пример 2: Decoder-only генерация с GPT-2
```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

prompt = "Once upon a time"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(inputs.input_ids, max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## 6. Common Pitfalls
- **Смесване на роли:** Опит да се използва decoder-only модел за задачи с ясно разделение вход-изход без адаптация, което води до по-ниско качество.
- **Пренебрегване на cross-attention:** При encoder-decoder модели, липсата на правилно използване на cross-attention води до загуба на контекст.
- **Генериране без контрол:** При decoder-only модели, липсата на контрол върху дължината и контекста може да доведе до безсмислен или прекалено дълъг текст.
- **Недооценяване на ресурсите:** Encoder-decoder модели често изискват повече изчислителни ресурси и време за обучение и инференция.

## 7. Short Retrieval Quiz
1. Каква е основната разлика между encoder-decoder и decoder-only архитектури?
2. Какво представлява cross-attention и в коя архитектура се използва?
3. За кои задачи encoder-decoder моделите са по-подходящи?
4. Как decoder-only моделите генерират текст?
5. Какво е self-attention и каква е ролята му?
6. Кой модел е по-подходящ за чатбот – encoder-decoder или decoder-only?
7. Какви са основните предизвикателства при използване на decoder-only модели?

## 8. Quick Recap
- Encoder-decoder моделите имат отделен енкодер и декодер, подходящи за задачи с ясно разделение вход-изход.
- Decoder-only моделите използват само декодер, генерирайки текст последователно на базата на контекст.
- Cross-attention е ключов механизъм в encoder-decoder архитектурата за свързване на входа с изхода.
- Self-attention позволява на модела да разбира контекста в рамките на входа или генерирания текст.
- Изборът на архитектура зависи от конкретната задача и изискванията за производителност.
- Практическата работа с модели като GPT (decoder-only) и MarianMT, T5, BART (encoder-decoder) помага за по-добро разбиране.
- Вниманието към контрол на генерирането и ресурсите е важно за успешна имплементация.

## 9. Spaced Review Plan

| Време      | Промпт за преговор                                  |
|------------|----------------------------------------------------|
| 1 ден      | Каква е разликата между encoder-decoder и decoder-only? |
| 3 дни      | Обяснете ролята на cross-attention в encoder-decoder моделите. |
| 1 седмица | Кога и защо бихте избрали decoder-only модел пред encoder-decoder? |
| 1 месец    | Прегледайте пример с машинен превод и обяснете как работи архитектурата. |