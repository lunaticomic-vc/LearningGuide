# Transfer_learning

## 1. Activate Prior Knowledge
- Какво знаете за обучението на модели в изкуствения интелект? Какво означава „обучение от нулата“?
- Можете ли да си представите ситуации, в които използването на вече обучен модел би било полезно?
- Как мислите, че знанията, придобити в една задача, могат да помогнат при решаването на друга, свързана задача?

## 2. Overview
Transfer learning (прехвърлящо обучение) е техника в машинното обучение, при която знания, придобити при решаването на една задача, се използват за подобряване на представянето при друга, свързана задача. Това позволява значително намаляване на времето и ресурсите, необходими за обучение, особено когато данните за новата задача са ограничени.

В по-широк контекст, transfer learning се използва като част от системи за изкуствен интелект, които трябва да се адаптират бързо към нови условия или задачи, без да започват обучението отначало. Това е особено важно в софтуерната инженерия и разработката на AI продукти, където времето за пускане на пазара и ефективността на модела са критични.

Този подход е ключов за напредъка в области като компютърно зрение, обработка на естествен език и роботика, където модели, обучени върху големи набори от данни, могат да бъдат адаптирани към специфични приложения с минимални усилия.

## 3. Key Concepts
- **Source Task** – Задачата, върху която първоначално е обучен моделът. Аналогия: учител, който вече е преподавал един предмет.
- **Target Task** – Новата задача, за която искаме да използваме знанията от source task. Аналогия: ученик, който прилага наученото в нов контекст.
- **Feature Extraction** – Използване на вече обучен модел за извличане на характеристики от данни, които след това се използват за нова задача.
- **Fine-tuning** – Допълнително обучение на вече обучен модел с нови данни, за да се адаптира към target task.
- **Domain Adaptation** – Процесът на адаптиране на модел към нова, но свързана среда или тип данни.
- **Overfitting** – Прекомерно адаптиране към тренировъчните данни, което намалява способността за обобщение. В transfer learning рискът се намалява, ако се използва правилно.

## 4. Step-by-step Learning Path
1. **Разберете основите на машинното обучение и невронните мрежи.**  
   *Задача:* Прегледайте основни архитектури като CNN и RNN.  
   *Въпроси:* Какво представлява невронна мрежа? Какво е backpropagation?

2. **Изучете концепцията за pre-trained модели и тяхното приложение.**  
   *Задача:* Изтеглете предварително обучен модел (напр. ResNet) и го използвайте за feature extraction върху нов набор от изображения.  
   *Въпроси:* Какво е pre-trained модел? Какво представлява feature extraction?

3. **Практикувайте fine-tuning на модел върху малък набор от данни.**  
   *Задача:* Фино настройте ResNet модел върху нова задача с ограничени данни.  
   *Въпроси:* Какво е fine-tuning? Кога е по-добре да се използва fine-tuning вместо feature extraction?

4. **Изследвайте domain adaptation и техники за справяне с различни домейни.**  
   *Задача:* Опитайте да приложите модел, обучен на един тип данни, върху друг тип (например снимки на дневна светлина и нощни снимки).  
   *Въпроси:* Какво е domain adaptation? Какви са предизвикателствата при различни домейни?

5. **Анализирайте резултатите и оптимизирайте процеса.**  
   *Задача:* Измерете представянето на модела и експериментирайте с различни слоеве за фино настройване.  
   *Въпроси:* Как да разпознаем overfitting? Как да подобрим генерализацията?

## 5. Examples
### Пример 1: Компютърно зрение с ResNet
```python
from torchvision import models, transforms
from PIL import Image
import torch

# Зареждане на предварително обучен модел
model = models.resnet18(pretrained=True)
model.eval()

# Зареждане и подготовка на изображение
img = Image.open("cat.jpg")
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])
input_tensor = preprocess(img)
input_batch = input_tensor.unsqueeze(0)

# Извличане на характеристики
with torch.no_grad():
    features = model(input_batch)
print(features)
```

### Пример 2: Fine-tuning на BERT за класификация на текст
```python
from transformers import BertForSequenceClassification, BertTokenizer
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

text = "Transfer learning is very useful."
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
print(outputs.logits)
```

### Пример 3: Domain adaptation с GAN (концептуално)
- Използване на генеративни състезателни мрежи за адаптиране на стилове между различни домейни (например дневни и нощни изображения).

## 6. Common Pitfalls
- **Прекалено доверие на pre-trained модел без адаптация.** Моделът може да не работи добре, ако domain-ът е много различен.
- **Недостатъчно данни за фино настройване.** Малък набор може да доведе до overfitting.
- **Фиксиране на твърде много слоеве при fine-tuning.** Понякога е по-добре да се обучават само последните слоеве.
- **Игнориране на разликите в данните между source и target задачи.** Това може да доведе до лоша генерализация.
- **Пренебрегване на оценката на модела върху валидиращ набор от данни.** Винаги тествайте адаптиран модел.

## 7. Short Retrieval Quiz
1. Какво представлява transfer learning?
2. Каква е разликата между feature extraction и fine-tuning?
3. Какво е source task и target task?
4. Защо transfer learning е полезен при ограничени данни?
5. Какво е domain adaptation?
6. Как може да се избегне overfitting при transfer learning?
7. Кога е по-добре да се използва fine-tuning вместо само feature extraction?

## 8. Quick Recap
- Transfer learning използва знания от една задача за подобряване на друга.
- Позволява спестяване на време и ресурси при обучение.
- Основни методи са feature extraction и fine-tuning.
- Domain adaptation е важен за справяне с различни типове данни.
- Внимателното управление на данните и слоевете е ключово за успех.
- Прехвърлящото обучение е широко приложимо в компютърно зрение и обработка на естествен език.
- Избягвайте overfitting чрез подходяща оценка и адаптация.

## 9. Spaced Review Plan

| Време след учене | Прегледна задача                                | Цел                                    |
|------------------|------------------------------------------------|---------------------------------------|
| 1 ден            | Отговорете на краткия тест от раздел 7          | Активиране на спомените                |
| 3 дни            | Прегледайте ключовите концепции и примери       | Задълбочаване на разбирането          |
| 1 седмица        | Изпълнете практическа задача с fine-tuning     | Приложение на знанията в практика     |
| 1 месец          | Обобщете и обяснете концепциите на друг човек  | Консолидиране и дългосрочно запаметяване |