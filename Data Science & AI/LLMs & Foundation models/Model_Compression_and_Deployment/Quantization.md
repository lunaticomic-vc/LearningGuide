# Quantization

## 1. Activate Prior Knowledge
- Какво представлява цифровото представяне на данни и как се различава от аналоговото?
- Защо при обработката на големи модели в изкуствения интелект е важно да се оптимизира използването на памет и изчислителни ресурси?
- Как мислите, че намаляването на точността на числата (например от 32-битов към 8-битов формат) може да повлияе на качеството на изхода на AI модел?

## 2. Overview
Квантоването е процес на преобразуване на непрекъснати или високопрецизни стойности в по-ограничен набор от стойности с по-ниска точност. В контекста на изкуствения интелект и машинното обучение, това обикновено означава намаляване на битовата дълбочина на параметрите на моделите (например тегла и активации) от 32-битови плаващи точки към 8-битови цели числа или дори по-малки.

Този процес е критичен за внедряване на модели в реални системи, където ресурсите са ограничени – например мобилни устройства, вградени системи или облачни услуги с високи изисквания за латентност и енергийна ефективност. Квантоването позволява значително намаляване на паметната консумация и ускоряване на изчисленията, като същевременно запазва приемливо ниво на точност.

Въпреки това, квантоването е баланс между компресия и загуба на информация. Разбирането на принципите и техниките за квантоване е ключово за разработчиците и изследователите, които искат да оптимизират модели без да жертват качеството на резултатите.

## 3. Key Concepts
- **Quantization (Квантоване)** – Процес на преобразуване на числови стойности с висока точност в по-нискоточни стойности, обикновено с по-малко битове. Може да се сравни с приближаването на детайлна картина с по-груби пиксели.
- **Bit-Depth (Битова дълбочина)** – Броят на битовете, използвани за представяне на числова стойност. По-малката битова дълбочина означава по-малко възможни стойности и по-малко памет.
- **Uniform Quantization (Равноразпределено квантоване)** – Квантоване, при което интервалите между квантовите нива са равни. Представете си разделяне на числова линия на равни сегменти.
- **Non-uniform Quantization (Неравноразпределено квантоване)** – Квантоване с различни по големина интервали, оптимизирано за специфични разпределения на данните.
- **Quantization Error (Грешка от квантоване)** – Разликата между оригиналната стойност и квантования еквивалент. Тази грешка е неизбежна, но трябва да се минимизира.
- **Post-Training Quantization (Квантоване след обучение)** – Квантоване, прилагано върху вече обучен модел, без допълнително обучение.
- **Quantization-Aware Training (Обучение с осъзнато квантоване)** – Обучение на модел, като се симулира квантоването по време на тренировъчния процес, за да се намали загубата на точност.

## 4. Step-by-step Learning Path
1. **Разбиране на цифровите представяния и битова дълбочина**  
   - Фокус: Научете как числата се представят в компютърната памет (float32, int8 и др.).  
   - Задача: Напишете малка програма, която конвертира float32 число в int8 и обратно, като наблюдавате загубата на информация.  
   - Въпроси: Какво се случва с числото при конверсия? Защо някои стойности се губят?

2. **Изучаване на методите за квантоване**  
   - Фокус: Разгледайте разликите между равномерно и неравноразпределено квантоване.  
   - Задача: Имплементирайте равномерно квантоване върху масив с реални числа.  
   - Въпроси: Как се изчисляват квантовите нива? Как грешката от квантоване се влияе от броя на битовете?

3. **Прилагане на Post-Training Quantization**  
   - Фокус: Научете как да приложите квантоване върху предварително обучен модел.  
   - Задача: Използвайте TensorFlow или PyTorch, за да конвертирате модел от float32 към int8.  
   - Въпроси: Какви са ефектите върху точността? Какво е компромисът между размер и производителност?

4. **Изучаване на Quantization-Aware Training**  
   - Фокус: Разберете как обучението с осъзнато квантоване подобрява резултатите.  
   - Задача: Тренирайте прост невронен модел с QAT и сравнете точността с пост-тренировъчното квантоване.  
   - Въпроси: Защо QAT дава по-добри резултати? Как се симулира квантоването по време на обучение?

5. **Оптимизация и отстраняване на проблеми**  
   - Фокус: Научете техники за минимизиране на грешката и избягване на типични проблеми.  
   - Задача: Анализирайте грешките на квантован модел и експериментирайте с различни настройки.  
   - Въпроси: Какви са най-честите източници на грешки? Как да ги коригираме?

## 5. Examples
### Пример 1: Равноразпределено квантоване на float масив
```python
import numpy as np

def uniform_quantize(x, bits=8):
    qmin = 0
    qmax = 2**bits - 1
    min_val = np.min(x)
    max_val = np.max(x)
    scale = (max_val - min_val) / (qmax - qmin)
    zero_point = qmin - min_val / scale
    q_x = np.round(x / scale + zero_point)
    q_x = np.clip(q_x, qmin, qmax)
    return (q_x - zero_point) * scale

data = np.array([0.1, 0.5, 0.9, 1.5, 2.0])
quantized_data = uniform_quantize(data)
print("Original:", data)
print("Quantized:", quantized_data)
```

### Пример 2: Post-Training Quantization с TensorFlow
```python
import tensorflow as tf

# Зареждаме предварително обучен модел
model = tf.keras.applications.MobileNetV2(weights='imagenet')

# Конвертираме модела към int8
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()

with open('mobilenet_v2_quant.tflite', 'wb') as f:
    f.write(tflite_quant_model)
```

### Пример 3: Квантоване с осъзнато обучение (QAT) в PyTorch
```python
import torch
import torch.quantization

model = torchvision.models.resnet18(pretrained=True)
model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
torch.quantization.prepare_qat(model, inplace=True)

# Тренираме модела с QAT
# train(model, train_loader)

torch.quantization.convert(model.eval(), inplace=True)
```

## 6. Common Pitfalls
- **Прекомерна загуба на точност** – Квантоването с твърде малко битове без адаптация на модела води до значително влошаване на резултатите. Използвайте QAT или внимателно настройвайте параметрите.
- **Игнориране на разпределението на данните** – Равноразпределеното квантоване не винаги е оптимално за всички данни. Анализирайте разпределението и използвайте неравноразпределено квантоване при нужда.
- **Липса на калибрация** – При post-training quantization липсата на правилна калибрация на скалите и zero points може да доведе до грешки.
- **Пренебрегване на хардуерните ограничения** – Не всички хардуерни платформи поддържат всички видове квантоване. Проверявайте съвместимостта.
- **Недостатъчно тестване след квантоване** – Винаги валидирайте модела след квантоване, за да откриете потенциални проблеми.

## 7. Short Retrieval Quiz
1. Какво е основната цел на квантоването в AI модели?
2. Какво представлява равномерното квантоване?
3. Каква е разликата между post-training quantization и quantization-aware training?
4. Какво е грешка от квантоване и защо е важна?
5. Кои са основните предимства на използването на int8 вместо float32?
6. Какво може да се случи, ако не се направи калибрация при post-training quantization?
7. Защо QAT обикновено дава по-добри резултати от пост-тренировъчното квантоване?

## 8. Quick Recap
- Квантоването намалява битовата дълбочина на числовите стойности, за да оптимизира памет и изчисления.
- Основните методи са post-training quantization и quantization-aware training.
- Равноразпределеното квантоване е най-простият метод, но не винаги най-ефективен.
- Грешката от квантоване е неизбежна, но трябва да се минимизира чрез подходящи техники.
- Квантоването е критично за внедряване на AI модели в ограничени хардуерни среди.
- Винаги валидирайте и тествате модела след квантоване.
- Използването на QAT позволява по-добро запазване на точността при по-ниска битова дълбочина.

## 9. Spaced Review Plan

| Време след учене | Промпт за преговор                                      |
|------------------|--------------------------------------------------------|
| 1 ден            | Обяснете какво е квантоване и защо е важно в AI.       |
| 3 дни            | Опишете разликите между post-training quantization и QAT. |
| 1 седмица        | Приложете равномерно квантоване върху примерен масив.  |
| 1 месец          | Анализирайте потенциалните грешки при квантоване и как да ги избегнете. |