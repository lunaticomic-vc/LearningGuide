# Gradient_descent_and_backpropagation

## 1. Activate Prior Knowledge
- Какво представлява оптимизацията в контекста на машинното обучение и защо е важна?
- Как бихте обяснили ролята на грешката (error) в обучението на невронна мрежа?
- Какво знаете за начина, по който компютърните програми могат да „научат“ от данни чрез итеративни процеси?

## 2. Overview
Gradient descent (градиентен спуск) и backpropagation (обратното разпространение на грешката) са фундаментални алгоритми в обучението на невронни мрежи. Те позволяват на модела да се адаптира към данните, като минимизират грешката между предсказанията и реалните стойности. Gradient descent е метод за оптимизация, който използва информация за наклона (градиента) на функцията на загуба, за да намери минималната й стойност.

Backpropagation е алгоритъм, който ефективно изчислява тези градиенти за всички параметри в мрежата, използвайки правилото на веригата от диференциалното смятане. Той позволява на модела да „научи“ кои параметри трябва да се променят и в каква посока, за да подобри представянето си.

Тези методи са сърцето на съвременните AI системи, тъй като позволяват обучение на сложни модели с милиони параметри. Разбирането им е ключово за всеки, който иска да проектира, оптимизира и анализира невронни мрежи и други модели за машинно обучение.

## 3. Key Concepts
- **Gradient (градиент)** – Вектор, който показва посоката и скоростта на най-бързото увеличение на функцията. Мислете за него като за стрела, сочеща накъде да се движите, за да увеличите стойността.
- **Loss function (функция на загуба)** – Мярка за това колко далеч са предсказанията на модела от истинските стойности. Аналогично на „цена“, която плащаме за грешките.
- **Learning rate (скорост на учене)** – Малко число, което определя колко големи стъпки правим в посока на градиента. Ако е прекалено голямо, можем да прескочим минималума; ако е твърде малко – обучението ще е бавно.
- **Backpropagation (обратно разпространение)** – Метод за изчисляване на градиентите чрез преминаване назад през мрежата, използвайки правилото на веригата.
- **Epoch (епоха)** – Един пълен цикъл през целия тренировъчен набор от данни.
- **Overfitting (преобучение)** – Когато моделът се научи твърде добре на тренировъчните данни, но не се представя добре на нови, непознати данни.

## 4. Step-by-step Learning Path
1. **Разберете функцията на загуба и нейното значение**
   - Фокус: Какво измерва и защо минимизирането й е цел.
   - Задача: Имплементирайте проста MSE (mean squared error) функция за малък набор от данни.
   - Въпроси: Какво означава стойност на загубата 0? Защо не искаме загубата да е голяма?

2. **Научете как се изчислява градиентът**
   - Фокус: Диференциране на функцията на загуба спрямо параметрите.
   - Задача: Изчислете ръчно градиент за проста линейна регресия.
   - Въпроси: Какво показва знакът на градиента? Какво означава голям градиент?

3. **Разберете алгоритъма на градиентния спуск**
   - Фокус: Как се използва градиентът за актуализация на параметрите.
   - Задача: Напишете код за един стъпков градиентен спуск.
   - Въпроси: Как скоростта на учене влияе на обучението? Какво се случва при твърде голяма скорост?

4. **Изучете backpropagation в невронни мрежи**
   - Фокус: Как се изчисляват градиентите ефективно за всеки слой.
   - Задача: Следете изчисленията на backpropagation за мрежа с един скрит слой.
   - Въпроси: Защо backpropagation е по-ефективен от директно диференциране? Какво е правилото на веригата?

5. **Практикувайте с реални данни и библиотеки**
   - Фокус: Прилагане на gradient descent и backpropagation с TensorFlow или PyTorch.
   - Задача: Обучете малка невронна мрежа за класификация на MNIST.
   - Въпроси: Какви параметри на оптимизатора можете да променяте? Как да следите загубата по време на обучение?

## 5. Examples

### Пример 1: Градиентен спуск за линейна регресия (Python)
```python
import numpy as np

# Данни
X = np.array([1, 2, 3, 4])
y = np.array([2, 4, 6, 8])

# Параметри
w = 0.0
b = 0.0
learning_rate = 0.01

# Функция на загуба и градиенти
def compute_loss_and_gradients(X, y, w, b):
    N = len(X)
    y_pred = w * X + b
    loss = np.mean((y_pred - y) ** 2)
    dw = (2/N) * np.sum((y_pred - y) * X)
    db = (2/N) * np.sum(y_pred - y)
    return loss, dw, db

# Обучение
for epoch in range(100):
    loss, dw, db = compute_loss_and_gradients(X, y, w, b)
    w -= learning_rate * dw
    b -= learning_rate * db
    if epoch % 10 == 0:
        print(f"Epoch {epoch}: loss={loss:.4f}, w={w:.4f}, b={b:.4f}")
```

### Пример 2: Backpropagation в проста невронна мрежа (псевдокод)
```
Forward pass:
  z1 = w1 * x + b1
  a1 = sigmoid(z1)
  z2 = w2 * a1 + b2
  y_pred = z2

Compute loss:
  loss = (y_pred - y)^2

Backward pass:
  dloss/dy_pred = 2 * (y_pred - y)
  dloss/dw2 = dloss/dy_pred * a1
  dloss/db2 = dloss/dy_pred
  dloss/da1 = dloss/dy_pred * w2
  dloss/dz1 = dloss/da1 * sigmoid'(z1)
  dloss/dw1 = dloss/dz1 * x
  dloss/db1 = dloss/dz1

Update weights:
  w1 -= learning_rate * dloss/dw1
  b1 -= learning_rate * dloss/db1
  w2 -= learning_rate * dloss/dw2
  b2 -= learning_rate * dloss/db2
```

## 6. Common Pitfalls
- **Твърде голяма скорост на учене** – води до нестабилно обучение и прескачане на минимуми.
- **Игнориране на нормализация на данните** – може да забави или блокира обучението.
- **Липса на проверка на градиентите** – грешки в изчисленията на backpropagation могат да останат незабелязани.
- **Прекалено малък брой епохи** – моделът не се обучава достатъчно добре.
- **Прекалено голям брой епохи без регуляризация** – води до преобучение.
- **Неизползване на валидационен набор** – няма как да се следи дали моделът се обобщава добре.

## 7. Short Retrieval Quiz
1. Какво измерва функцията на загуба?
2. Какво представлява градиентът и каква е ролята му в gradient descent?
3. Защо backpropagation използва правилото на веригата?
4. Какво може да се случи, ако скоростта на учене е твърде голяма?
5. Какво е епоха в контекста на обучение на невронна мрежа?
6. Как backpropagation помага за оптимизацията на параметрите?
7. Какво означава преобучение и как може да се избегне?

## 8. Quick Recap
- Gradient descent е метод за минимизиране на функция на загуба чрез итеративно актуализиране на параметрите в посока на отрицателния градиент.
- Backpropagation изчислява градиентите ефективно чрез преминаване назад през слоевете на невронната мрежа.
- Функцията на загуба измерва колко добре моделът предсказва спрямо истинските данни.
- Скоростта на учене контролира размера на стъпките при актуализация на параметрите.
- Правилното разбиране и прилагане на тези алгоритми е ключово за успешното обучение на невронни мрежи.
- Често срещани грешки включват неправилна настройка на скоростта на учене и липса на валидация.
- Практическото упражнение с реални данни и библиотеки затвърждава теоретичните знания.

## 9. Spaced Review Plan

| Време след първоначално учене | Промпт за преглед                                      |
|-------------------------------|-------------------------------------------------------|
| 1 ден                         | Обяснете с прости думи как gradient descent намира минимум. |
| 3 дни                         | Опишете как backpropagation изчислява градиентите.   |
| 1 седмица                     | Напишете кратък код за един стъпков градиентен спуск. |
| 1 месец                       | Обсъдете как скоростта на учене влияе на обучението и как да я настроите. |