# Jacobians_and_Hessians

## 1. Activate Prior Knowledge

- Какво представлява производната на функция с една променлива и как тя описва локалната промяна?
- Как бихте обяснили на колега как се обобщава производната при функции с много променливи?
- Защо в машинното обучение и оптимизацията е важно да знаем как се променя изходът спрямо множество входни параметри?

## 2. Overview

В многоизмерните системи, като тези в изкуствения интелект и софтуерното инженерство, често работим с функции, които приемат вектори като вход и връщат вектори или скалари като изход. За да разберем как малки промени във входните данни влияят на изхода, използваме производни, обобщени чрез матрици, наречени Якобиани и Хесиани.

Якобианът е матрица на първите частни производни на векторна функция спрямо нейните входни променливи. Той описва локалната линейна аппроксимация и е ключов за алгоритми като обратното разпространение в невронните мрежи. Хесианът, от своя страна, е матрица на вторите частни производни на скаларна функция, която ни дава информация за кривината на функцията – важна при оптимизация и анализ на стабилност.

Разбирането и ефективното изчисляване на Якобиани и Хесиани е фундаментално за разработването на надеждни и бързи AI системи, особено при оптимизация на сложни модели и при числени методи за решаване на уравнения.

## 3. Key Concepts

- **Jacobian (Якобиан)** – матрица, съдържаща всички първи частни производни на векторна функция спрямо входните ѝ променливи. Мислете за нея като за обобщена производна, която описва как всеки изход се променя при малка промяна на всеки вход.
- **Hessian (Хесиан)** – квадратна матрица на всички втори частни производни на скаларна функция. Тя описва кривината на функцията и помага да разберем дали дадена точка е минимум, максимум или седловина.
- **Partial derivative (Частна производна)** – производна на функция спрямо една променлива, като другите се държат константни. Аналогично на наблюдение как промяната на един параметър влияе на резултата.
- **Gradient (Градиент)** – вектор от първите частни производни на скаларна функция. Той показва посоката на най-бързо нарастване.
- **Chain rule (Правило на верижната производна)** – метод за диференциране на сложни функции чрез умножение на производните на вложените функции. Ключов при изчисляване на Якобиани в сложни системи.

## 4. Step-by-step Learning Path

1. **Фокус:** Прегледайте частните производни и градиента.
   - **Задача:** Изчислете градиента на функцията \( f(x,y) = x^2 y + 3y \).
   - **Въпроси:** Какво означава частната производна спрямо \(x\)? Как градиентът показва посоката на най-бързо нарастване?

2. **Фокус:** Разберете структурата на Якобиана за векторни функции.
   - **Задача:** Изчислете Якобиана на функцията \( \mathbf{f}(\mathbf{x}) = \begin{bmatrix} x_1^2 + x_2 \\ \sin(x_1) x_2 \end{bmatrix} \).
   - **Въпроси:** Какво представлява всеки елемент в Якобиана? Как Якобианът обобщава градиентите?

3. **Фокус:** Изучете Хесиана и неговото значение за кривината.
   - **Задача:** Изчислете Хесиана на функцията \( f(x,y) = x^3 + y^2 - xy \).
   - **Въпроси:** Какво ни казва Хесианът за вида на критичната точка? Какво означава положително определен Хесиан?

4. **Фокус:** Прилагане на веригното правило за сложни функции.
   - **Задача:** Изчислете Якобиана на \( \mathbf{g}(\mathbf{x}) = \mathbf{f}(\mathbf{h}(\mathbf{x})) \), където \( \mathbf{f} \) и \( \mathbf{h} \) са векторни функции.
   - **Въпроси:** Как се комбинират Якобианите на вложените функции? Как това се използва в обратното разпространение?

5. **Фокус:** Практическо изчисление с помощта на софтуер (например Python и NumPy).
   - **Задача:** Имплементирайте функция за изчисляване на Якобиан и Хесиан на дадена функция с помощта на автоматично диференциране (например с библиотеката autograd или JAX).
   - **Въпроси:** Как автоматичното диференциране улеснява изчисленията? Какви са предимствата пред числените методи?

## 5. Examples

### Пример 1: Якобиан на векторна функция

Функция:  
\[
\mathbf{f}(x,y) = \begin{bmatrix} x^2 y \\ \sin(x) + y \end{bmatrix}
\]

Якобиан:  
\[
J = \begin{bmatrix}
\frac{\partial (x^2 y)}{\partial x} & \frac{\partial (x^2 y)}{\partial y} \\
\frac{\partial (\sin(x) + y)}{\partial x} & \frac{\partial (\sin(x) + y)}{\partial y}
\end{bmatrix} = 
\begin{bmatrix}
2xy & x^2 \\
\cos(x) & 1
\end{bmatrix}
\]

### Пример 2: Хесиан на скаларна функция

Функция:  
\[
f(x,y) = x^3 + y^2 - xy
\]

Хесиан:  
\[
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{bmatrix} = 
\begin{bmatrix}
6x & -1 \\
-1 & 2
\end{bmatrix}
\]

### Пример 3: Python код за изчисляване на Якобиан с autograd

```python
import autograd.numpy as np
from autograd import jacobian

def f(x):
    return np.array([x[0]**2 * x[1], np.sin(x[0]) + x[1]])

J = jacobian(f)
x = np.array([1.0, 2.0])
print(J(x))
```

## 6. Common Pitfalls

- **Смесване на Якобиан и Хесиан** – Якобианът е матрица на първи производни на векторна функция, а Хесианът – матрица на втори производни на скаларна функция. Важно е да не се бъркат.
- **Неправилно изчисляване на частните производни** – особено при сложни функции с много променливи, лесно се пропускат зависимости.
- **Игнориране на размерите на матриците** – Якобианът има размер \(m \times n\) за функция от \(\mathbb{R}^n \to \mathbb{R}^m\), Хесианът е квадратен \(n \times n\).
- **Пренебрегване на симетрията на Хесиана** – за функции с непрекъснати втори производни Хесианът е симетричен, което може да се използва за оптимизация.
- **Изчисляване на производни ръчно при сложни функции** – риск от грешки; предпочитайте автоматично диференциране.

## 7. Short Retrieval Quiz

1. Какво представлява Якобианът и каква е неговата размерност?
2. Каква информация дава Хесианът за функцията?
3. Как се свързват градиентът и Якобианът?
4. Какво е веригното правило и как се прилага при изчисляване на Якобиан?
5. Защо е важно да знаем дали Хесианът е положително определен?
6. Как автоматичното диференциране помага при изчисляване на Якобиани и Хесиани?
7. Каква е разликата между частна и обща производна?

## 8. Quick Recap

- Якобианът обобщава първите частни производни на векторна функция и описва локалната линейна промяна.
- Хесианът е матрица на вторите частни производни на скаларна функция и дава информация за кривината.
- Частните производни измерват чувствителността спрямо отделни входни променливи.
- Веригното правило е ключово за диференциране на сложни вложени функции.
- Автоматичното диференциране значително улеснява изчисленията и намалява грешките.
- Правилното разбиране на размерите и структурата на Якобиани и Хесиани е критично за успешна имплементация.
- Хесианът може да се използва за определяне на типа критична точка (минимум, максимум, седловина).

## 9. Spaced Review Plan

| Време след учене | Промпт за преглед                                      |
|------------------|-------------------------------------------------------|
| 1 ден            | Обяснете с прости думи какво е Якобиан и Хесиан.      |
| 3 дни            | Изчислете Якобиана на проста векторна функция.        |
| 1 седмица        | Опишете ролята на Хесиана в оптимизацията.             |
| 1 месец          | Имплементирайте функция за изчисляване на Якобиан с автоматично диференциране. |