# Hugging_Face

## 1. Activate Prior Knowledge
- Какво знаете за трансформър архитектурите и тяхната роля в съвременните AI модели?
- Какви библиотеки или платформи използвате за работа с естествен език (NLP) и машинно обучение?
- Как бихте интегрирали предварително обучен модел в собствен софтуерен проект?

## 2. Overview
Hugging Face е водеща платформа и общност, която предоставя инструменти, модели и библиотеки за работа с естествен език и други задачи в областта на изкуствения интелект. Основната ѝ цел е да направи сложните модели за машинно обучение лесно достъпни и приложими за широк кръг от разработчици и изследователи.

Платформата се фокусира върху трансформър базирани модели, които са в основата на много съвременни AI системи за разбиране и генериране на текст, аудио и изображения. Hugging Face предлага както предварително обучени модели, така и инфраструктура за обучение, фина настройка и внедряване.

Това я прави ключов компонент в екосистемата на AI, тъй като улеснява бързото прототипиране и внедряване на интелигентни приложения, като същевременно насърчава споделянето на знания и ресурси в научната и инженерната общност.

## 3. Key Concepts
- **Transformers** – Архитектура за дълбоко обучение, която използва механизъм на внимание (attention) за обработка на последователности от данни, като текст. Можете да си я представите като „фокусиращ“ механизъм, който избира важните части от входа за обработка.
- **Pretrained Models** – Модели, които вече са обучени върху големи корпуси от данни и могат да се използват директно или да се донастройват за специфични задачи. Аналогия: като да купите вече сглобен двигател, който само трябва да адаптирате към вашия автомобил.
- **Tokenization** – Процесът на разделяне на текст на по-малки единици (токени), които моделът може да разбира. Това е като да разделите изречение на думи или части от думи, за да го „преведете“ на езика на модела.
- **Fine-tuning** – Допълнително обучение на предварително обучен модел върху специфичен набор от данни, за да се подобри представянето му в конкретна задача.
- **Hugging Face Hub** – Онлайн хранилище за модели, датасети и код, което позволява лесно споделяне и достъп до ресурси.
- **Transformers Library** – Основната библиотека на Hugging Face, която предоставя интерфейси за зареждане, обучение и използване на трансформър модели.

## 4. Step-by-step Learning Path
1. **Запознаване с Transformers библиотеката**
   - Фокус: Основни класове и функции за зареждане на модели.
   - Задача: Инсталирайте библиотеката и заредете предварително обучен модел за класификация на текст.
   - Въпроси: Какво представлява класът `AutoModel`? Как се зарежда токенизатор?

2. **Работа с токенизация**
   - Фокус: Разбиране на процеса на токенизация и декодиране.
   - Задача: Токенизирайте примерен текст и го върнете обратно в естествен вид.
   - Въпроси: Какво е значението на `input_ids`? Защо е важно да се използва същият токенизатор при обучение и инференция?

3. **Използване на предварително обучени модели за инференция**
   - Фокус: Извличане на предсказания от модел.
   - Задача: Направете sentiment analysis на няколко изречения.
   - Въпроси: Как се интерпретират изходните логити? Как се преобразуват в вероятности?

4. **Фина настройка на модел**
   - Фокус: Подготовка на данни и обучение на модел върху нова задача.
   - Задача: Фино настройте модел за класификация на нов набор от данни.
   - Въпроси: Какви параметри са важни при фина настройка? Как се избира оптимизатор?

5. **Внедряване и споделяне**
   - Фокус: Запознаване с Hugging Face Hub и API.
   - Задача: Качете собствен модел в Hub и го използвайте чрез API.
   - Въпроси: Какви са предимствата на споделянето на модели? Как се осъществява достъп до модел през API?

## 5. Examples

### Пример 1: Зареждане на предварително обучен модел за sentiment analysis
```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("Това е страхотен ден!")
print(result)
```

### Пример 2: Токенизация и декодиране
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
text = "Hugging Face е страхотна платформа."
tokens = tokenizer.tokenize(text)
print(tokens)

encoded = tokenizer.encode(text)
print(encoded)

decoded = tokenizer.decode(encoded)
print(decoded)
```

### Пример 3: Фина настройка (схематично)
```python
from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
training_args = TrainingArguments(output_dir="./results", num_train_epochs=3, per_device_train_batch_size=16)

trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)
trainer.train()
```

## 6. Common Pitfalls
- **Несъответствие между токенизатор и модел** – Винаги използвайте токенизатора, съответстващ на модела, за да избегнете грешки при входните данни.
- **Прекомерна фина настройка** – Обучението с твърде малко данни или твърде дълго може да доведе до пренасищане (overfitting).
- **Игнориране на контекста на модела** – Моделите имат ограничена дължина на входа; подаването на твърде дълги текстове без подходяща обработка може да доведе до загуба на информация.
- **Недостатъчно внимание към изхода** – Не всички изходи са директно интерпретируеми; често е необходимо допълнително преобразуване (например softmax).
- **Пренебрегване на лицензите и етиката** – При използване и споделяне на модели е важно да се спазват лицензионните условия и етичните стандарти.

## 7. Short Retrieval Quiz
1. Какво представлява токенизацията и защо е важна?
2. Каква е ролята на предварително обучените модели?
3. Какво е фина настройка и кога се използва?
4. Какво представлява Hugging Face Hub?
5. Коя библиотека предоставя основните инструменти за работа с трансформъри?
6. Какво може да се случи при използване на несъвместим токенизатор и модел?
7. Защо е важно да се контролира дължината на входния текст?

## 8. Quick Recap
- Hugging Face е платформа и библиотека за лесна работа с трансформър модели.
- Токенизацията превръща текст в числови входове за моделите.
- Предварително обучените модели спестяват време и ресурси.
- Фина настройка адаптира модел към специфична задача.
- Hugging Face Hub улеснява споделянето и достъпа до модели.
- Вниманието към съвместимостта и параметрите е ключово за успешна работа.
- Етичните и лицензионни аспекти са важна част от използването на модели.

## 9. Spaced Review Plan

| Време след изучаване | Промпт за преговор                                      |
|----------------------|--------------------------------------------------------|
| 1 ден                | Обяснете какво е Hugging Face и какво представляват трансформър моделите. |
| 3 дни                | Опишете процеса на токенизация и защо е важен за NLP.  |
| 1 седмица            | Преговор на стъпките за фина настройка на модел.       |
| 1 месец              | Прегледайте общите грешки при работа с Hugging Face и как да ги избегнете. |