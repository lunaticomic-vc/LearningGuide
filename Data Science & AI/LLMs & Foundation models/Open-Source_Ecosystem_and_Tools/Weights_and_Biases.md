# Weights_and_Biases

## 1. Activate Prior Knowledge

- Какво представляват параметрите в изкуствените невронни мрежи и защо са важни?
- Как влияят стойностите на тежестите и пристрастията (biases) върху изхода на невронната мрежа?
- Можете ли да предвидите как промяната на тежестите и пристрастията ще промени поведението на модел за машинно обучение?

## 2. Overview

В контекста на изкуствените невронни мрежи, тежестите (weights) и пристрастията (biases) са основните параметри, които определят поведението на модела. Тежестите са коефициенти, които умножават входните данни, а пристрастията добавят константна стойност, позволявайки на модела да се адаптира по-гъвкаво към различни входове.

Тези параметри се настройват по време на обучението чрез алгоритми като обратното разпространение (backpropagation), за да минимизират грешката между предсказания и реалните резултати. Без правилно оптимизирани тежести и пристрастия, моделът няма да може да научи сложни зависимости в данните.

Разбирането на тежестите и пристрастията е фундаментално за проектиране, анализ и оптимизация на невронни мрежи, което ги прави ключов елемент в съвременните AI системи и софтуер за машинно обучение.

## 3. Key Concepts

- **Weights (Тежести)** – числови коефициенти, които умножават входните стойности, определяйки колко силно всеки вход влияе върху изхода. Можем да ги сравним с регулатори на силата на звука за всеки входен сигнал.
- **Biases (Пристрастия)** – допълнителни параметри, които добавят константна стойност към сумата на входовете, позволявайки на неврона да се "измести" и да научи по-гъвкави функции.
- **Activation Function (Функция на активация)** – нелинейна функция, която преобразува сумата от тежестите и пристрастията в изходна стойност. Тя позволява на мрежата да моделира сложни зависимости.
- **Backpropagation (Обратно разпространение)** – алгоритъм за оптимизация, който изчислява градиентите на тежестите и пристрастията спрямо грешката, за да ги актуализира и подобри модела.
- **Gradient Descent (Градиентен спуск)** – метод за минимизиране на грешката чрез итеративно коригиране на тежестите и пристрастията в посока, обратна на градиента.

## 4. Step-by-step Learning Path

1. **Разберете ролята на тежестите и пристрастията в един неврон**
   - Задача: Пресметнете изхода на един неврон с дадени тежести, пристрастия и входове.
   - Въпроси: Какво се случва, ако увеличим тежестта? Каква е ролята на пристрастията?

2. **Изучете функцията на активация и нейното значение**
   - Задача: Имплементирайте няколко функции на активация (ReLU, sigmoid) и визуализирайте изхода им.
   - Въпроси: Защо е нужна нелинейност? Какво ще стане без функция на активация?

3. **Разберете алгоритъма за обратно разпространение**
   - Задача: Ръчно изчислете градиентите на тежестите и пристрастията за малка мрежа.
   - Въпроси: Как се изчислява грешката? Как се актуализират параметрите?

4. **Практикувайте оптимизация с градиентен спуск**
   - Задача: Напишете прост код, който обучава неврон с градиентен спуск.
   - Въпроси: Как изборът на скорост на учене влияе на обучението? Как да разпознаем пренасищане?

5. **Изследвайте влиянието на тежестите и пристрастията върху цялостната мрежа**
   - Задача: Анализирайте как промяна на отделни тежести променя изхода на многослойна мрежа.
   - Въпроси: Как тежестите в различни слоеве влияят на крайния резултат? Как пристрастията подпомагат обучението?

## 5. Examples

### Пример 1: Изчисляване на изход на един неврон

```python
import numpy as np

inputs = np.array([1.0, 2.0, 3.0])
weights = np.array([0.2, 0.8, -0.5])
bias = 0.1

output = np.dot(inputs, weights) + bias
print("Output:", output)
```

### Пример 2: Имплементация на ReLU функция

```python
def relu(x):
    return max(0, x)

print(relu(-3))  # 0
print(relu(5))   # 5
```

### Пример 3: Обратно разпространение за един слой (псевдокод)

```python
# Грешка: e = y_pred - y_true
# Градиент за тежест w: dw = e * x
# Обновяване: w = w - learning_rate * dw
```

## 6. Common Pitfalls

- **Игнориране на пристрастията:** Без bias невроните не могат да се адаптират към някои функции, което ограничава представянето.
- **Неправилна инициализация на тежестите:** Ако тежестите са твърде големи или малки, обучението може да се забави или да не конвергира.
- **Прекалено голяма скорост на учене:** Водеща до нестабилно обучение и прескачане на оптимума.
- **Липса на нормализация:** Входните данни без нормализация могат да доведат до неефективно обучение.
- **Забравяне за нелинейна функция на активация:** Без нея мрежата се държи като линейна регресия, което ограничава възможностите ѝ.

## 7. Short Retrieval Quiz

1. Каква е ролята на тежестите в невронната мрежа?
2. Защо пристрастията са необходими?
3. Какво прави функцията на активация?
4. Какво представлява обратно разпространение?
5. Как градиентният спуск оптимизира тежестите?
6. Какво може да се случи при твърде голяма скорост на учене?
7. Защо не е добре тежестите да са инициализирани само с нули?

## 8. Quick Recap

- Тежестите и пристрастията са основните параметри, които определят поведението на невроните.
- Тежестите умножават входовете, а пристрастията добавят константна стойност.
- Функциите на активация въвеждат нелинейност, необходима за моделиране на сложни зависимости.
- Обратното разпространение изчислява градиентите за актуализация на параметрите.
- Градиентният спуск оптимизира тежестите и пристрастията чрез минимизиране на грешката.
- Правилната инициализация и избор на параметри са критични за успешно обучение.
- Пренебрегването на някой от тези елементи води до ограничена производителност на модела.

## 9. Spaced Review Plan

| Време след изучаване | Промпт за преговор                                   |
|----------------------|-----------------------------------------------------|
| 1 ден                | Обяснете ролята на тежестите и пристрастията.       |
| 3 дни                | Как функцията на активация влияе на изхода?         |
| 1 седмица            | Опишете процеса на обратно разпространение.         |
| 1 месец              | Дайте пример как промяна на тежестите променя изхода.|