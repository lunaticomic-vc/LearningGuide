# Mutual_information

## 1. Activate Prior Knowledge

- Какво представлява информацията в контекста на вероятностни модели и как се измерва?
- Как бихте оценили зависимостта между две случайни величини в софтуерна система?
- Защо е важно да разберем колко информация една променлива носи за друга при изграждането на AI модели?

## 2. Overview

Mutual information (MI) е мярка за взаимната зависимост между две случайни величини. Тя измерва колко информация за една променлива можем да научим, ако знаем стойността на другата. В контекста на изкуствения интелект и софтуерното инженерство, MI се използва за откриване на зависимости, оптимизация на модели и избор на характеристики (feature selection).

MI не изисква линейна връзка между променливите, за разлика от корелацията, което я прави мощен инструмент при анализ на сложни данни. Тя е основа за алгоритми в машинното обучение, като например при изграждането на Bayesian мрежи или при оценка на качеството на класификатори.

Разбирането на mutual information помага да се подобри интерпретируемостта на модели и да се избегне излишна информация, която може да доведе до пренасищане (overfitting). Това е ключово за създаване на по-ефективни и надеждни AI системи.

## 3. Key Concepts

- **Entropy (Ентропия)** – мярка за неопределеността или хаоса в една случайна променлива. Може да се представи като количеството "изненада" при наблюдение на резултат.
- **Joint Entropy (Съвместна ентропия)** – ентропията на две случайни променливи, измерва общата неопределеност при наблюдение на техните съвместни стойности.
- **Conditional Entropy (Условна ентропия)** – неопределеността на една променлива при условие, че стойността на друга е известна.
- **Mutual Information (Взаимна информация)** – количеството информация, което една променлива носи за друга; разликата между сумата на индивидуалните ентропии и съвместната ентропия.
- **Information Gain (Информационен прираст)** – специфичен случай на mutual information, използван при изграждане на дървета за вземане на решения.
- **Kullback-Leibler Divergence (KL дивергенция)** – мярка за разликата между две вероятностни разпределения, свързана с mutual information.

## 4. Step-by-step Learning Path

1. **Фокус:** Разберете ентропията като основна мярка за неопределеност.  
   **Задача:** Изчислете ентропията на монета с вероятност за глава 0.5 и 0.7.  
   **Въпроси:** Как се променя ентропията при различни вероятности? Защо ентропията е максимална при 0.5?

2. **Фокус:** Изучете съвместната и условната ентропия.  
   **Задача:** Създайте таблица с вероятности за две дискретни променливи и изчислете съвместната и условната ентропия.  
   **Въпроси:** Какво показва условната ентропия за зависимостта между променливите?

3. **Фокус:** Изследвайте формулата за mutual information и нейната интерпретация.  
   **Задача:** Изчислете mutual information за пример с две променливи от предходната задача.  
   **Въпроси:** Как mutual information се свързва с ентропията? Какво означава стойност 0?

4. **Фокус:** Прилагане на mutual information в избор на характеристики (feature selection).  
   **Задача:** Използвайте mutual information, за да изберете най-информативните характеристики от набор данни (например sklearn dataset).  
   **Въпроси:** Как mutual information помага при намаляване на размерността? Какво е предимството пред корелацията?

5. **Фокус:** Имплементирайте mutual information в Python с помощта на библиотеки.  
   **Задача:** Напишете код, който изчислява mutual information между две дискретни променливи.  
   **Въпроси:** Какви са ограниченията при изчисляване на MI с реални данни? Как да се справим с непрекъснати променливи?

## 5. Examples

**Пример 1: Изчисляване на mutual information между две дискретни променливи**

```python
from sklearn.metrics import mutual_info_score

x = [0, 0, 1, 1, 0, 1, 0, 1]
y = [1, 1, 0, 0, 1, 0, 1, 0]

mi = mutual_info_score(x, y)
print(f"Mutual Information: {mi:.4f}")
```

**Пример 2: Избор на характеристики с mutual information**

```python
from sklearn.feature_selection import mutual_info_classif
from sklearn.datasets import load_iris

data = load_iris()
X = data.data
y = data.target

mi_scores = mutual_info_classif(X, y)
for i, score in enumerate(mi_scores):
    print(f"Feature {i}: MI score = {score:.4f}")
```

**Пример 3: Визуализация на взаимната информация**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mutual_info_score

x = np.random.randint(0, 2, 1000)
y = np.random.randint(0, 2, 1000)

mi = mutual_info_score(x, y)
plt.bar(['Mutual Information'], [mi])
plt.title('Mutual Information between X and Y')
plt.show()
```

## 6. Common Pitfalls

- **Грешка:** Използване на mutual information за непрекъснати променливи без подходяща дискретизация.  
  **Как да избегнем:** Използвайте техники за дискретизация или специализирани методи за непрекъснати данни (например kNN-базирани оценки).

- **Грешка:** Бъркане на mutual information с корелация.  
  **Как да избегнем:** Помнете, че MI улавя всякакви зависимости, не само линейни.

- **Грешка:** Прекалено доверие в MI при малки размери на извадката.  
  **Как да избегнем:** Използвайте статистически тестове и валидирайте резултатите с по-големи набори от данни.

- **Грешка:** Неправилно интерпретиране на стойностите на MI като вероятности.  
  **Как да избегнем:** Разберете, че MI е мярка за информация в битове или натове, а не вероятност.

## 7. Short Retrieval Quiz

1. Какво измерва mutual information между две случайни променливи?  
2. Каква е връзката между ентропия и mutual information?  
3. Защо mutual information е по-гъвкава мярка от корелацията?  
4. Какво означава стойност на mutual information равна на 0?  
5. Какви са основните предизвикателства при изчисляване на MI за непрекъснати променливи?  
6. Как mutual information може да помогне при избор на характеристики?  
7. Какво е условна ентропия и как се използва в изчисляването на MI?

## 8. Quick Recap

- Mutual information измерва количеството информация, което една променлива носи за друга.  
- Тя се базира на концепциите за ентропия, съвместна и условна ентропия.  
- MI улавя всякакви зависимости, не само линейни, което я прави мощен инструмент.  
- Използва се широко в машинното обучение за избор на характеристики и моделиране на зависимости.  
- Изчисляването на MI изисква внимание при непрекъснати променливи и малки извадки.  
- Разбирането на MI подобрява интерпретируемостта и ефективността на AI системи.  
- Практическата имплементация често използва библиотеки като sklearn.

## 9. Spaced Review Plan

| Време след учене | Прегледна задача                                   |
|------------------|--------------------------------------------------|
| 1 ден            | Обяснете с прости думи какво е mutual information и защо е важно. |
| 3 дни            | Изчислете mutual information за малък пример с дискретни данни.  |
| 1 седмица        | Приложете mutual information за избор на характеристики в реален dataset. |
| 1 месец          | Обсъдете разликите между mutual information и корелация с примери. |