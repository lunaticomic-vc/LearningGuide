# Exploration_vs_exploitation

## 1. Activate Prior Knowledge
- Какво разбирате под термините "exploration" и "exploitation" в контекста на вземане на решения?
- Как бихте обяснили защо един AI агент трябва да балансира между опитване на нови действия и използване на вече известни успешни стратегии?
- Можете ли да дадете пример от живота или софтуерното инженерство, където изборът между изследване и експлоатация е критичен?

## 2. Overview
В контекста на изкуствения интелект и софтуерното инженерство, проблемът exploration vs. exploitation се отнася до дилемата дали да се изследват нови възможности (exploration) или да се използват вече познати, доказани решения (exploitation). Тази дилема е ключова при обучение на агенти, оптимизация и вземане на решения в условия на несигурност.

Exploration позволява на системата да открие по-добри решения, които не са били опитвани досега, докато exploitation се фокусира върху максимизиране на възвръщаемостта чрез използване на вече научени знания. Балансът между двете е критичен, защото прекаленото изследване може да забави постигането на добри резултати, а прекаленото експлоатиране може да доведе до застой в подоптимални решения.

Тази концепция е фундаментална в алгоритми като reinforcement learning, multi-armed bandits и други адаптивни системи. Разбирането и прилагането на правилния баланс между exploration и exploitation подобрява ефективността и устойчивостта на интелигентните системи.

## 3. Key Concepts
- **Exploration (Изследване)** – Процес на опитване на нови действия или стратегии, за да се събере повече информация за средата. Аналогия: пробване на нов ресторант, за да откриете дали е по-добър от любимия.
- **Exploitation (Експлоатация)** – Използване на вече научени знания или стратегии, които са доказали своята ефективност. Аналогия: посещаване на любим ресторант, защото знаете, че храната там е вкусна.
- **Trade-off (Компромис)** – Балансът между exploration и exploitation, който трябва да бъде намерен, за да се оптимизира дългосрочната възвръщаемост.
- **Multi-armed bandit problem (Проблем на многоръкия бандит)** – Класически модел, който формализира дилемата между exploration и exploitation чрез избор между няколко опции с неизвестни награди.
- **Reinforcement Learning (Обучение чрез подсилване)** – Метод за обучение на агенти, които вземат решения, базирани на награди и наказания, където exploration vs exploitation е централна тема.
- **Epsilon-greedy strategy (Eпсилон-жадна стратегия)** – Проста техника за управление на trade-off, при която с малка вероятност (ε) се изследва, а с голяма вероятност се експлоатира.

## 4. Step-by-step Learning Path
1. **Фокус:** Разберете основите на exploration и exploitation.  
   **Задача:** Прочетете и обяснете с ваши думи какво означава всяко от двете понятия.  
   **Въпроси за припомняне:** Какво е exploration? Какво е exploitation?

2. **Фокус:** Изучете multi-armed bandit problem като модел за дилемата.  
   **Задача:** Имплементирайте прост multi-armed bandit симулатор с 3 "ръце".  
   **Въпроси за припомняне:** Какво представлява multi-armed bandit? Каква е целта при този проблем?

3. **Фокус:** Запознайте се с epsilon-greedy стратегията.  
   **Задача:** Добавете epsilon-greedy политика към вашия bandit симулатор и експериментирайте с различни стойности на ε.  
   **Въпроси за припомняне:** Как работи epsilon-greedy? Как ε влияе на баланса между exploration и exploitation?

4. **Фокус:** Изучете други стратегии за баланс като UCB (Upper Confidence Bound) и Thompson Sampling.  
   **Задача:** Имплементирайте UCB алгоритъм и сравнете резултатите с epsilon-greedy.  
   **Въпроси за припомняне:** Как UCB различава exploration от exploitation? Кога е по-добър от epsilon-greedy?

5. **Фокус:** Прилагане на концепцията в reinforcement learning.  
   **Задача:** Използвайте exploration vs exploitation стратегия в прост reinforcement learning агент (напр. Q-learning).  
   **Въпроси за припомняне:** Защо exploration е критично в reinforcement learning? Как експлоатацията помага на агента?

## 5. Examples
### Пример 1: Epsilon-greedy в multi-armed bandit
```python
import numpy as np

class EpsilonGreedyBandit:
    def __init__(self, arms, epsilon):
        self.arms = arms
        self.epsilon = epsilon
        self.counts = np.zeros(arms)
        self.values = np.zeros(arms)

    def select_arm(self):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.arms)  # Exploration
        else:
            return np.argmax(self.values)  # Exploitation

    def update(self, chosen_arm, reward):
        self.counts[chosen_arm] += 1
        n = self.counts[chosen_arm]
        value = self.values[chosen_arm]
        # Обновяване на средната стойност на наградата
        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward
```

### Пример 2: Exploration vs exploitation в Q-learning
```python
import numpy as np

class QLearningAgent:
    def __init__(self, n_states, n_actions, epsilon=0.1, alpha=0.5, gamma=0.9):
        self.Q = np.zeros((n_states, n_actions))
        self.epsilon = epsilon
        self.alpha = alpha
        self.gamma = gamma

    def choose_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.Q.shape[1])  # Exploration
        else:
            return np.argmax(self.Q[state])  # Exploitation

    def update(self, state, action, reward, next_state):
        best_next = np.max(self.Q[next_state])
        self.Q[state, action] += self.alpha * (reward + self.gamma * best_next - self.Q[state, action])
```

## 6. Common Pitfalls
- **Прекомерно exploration:** Агентът прекарва твърде много време в опити на нови действия и не използва наученото, което забавя постигането на добри резултати.  
  *Как да избегнем:* Намаляване на exploration с времето (decay на ε).
- **Прекомерно exploitation:** Агентът се фиксира върху първоначално успешни действия и пропуска по-добри възможности.  
  *Как да избегнем:* Въвеждане на минимално ниво на exploration.
- **Липса на адаптивност:** Фиксирани параметри (напр. ε) не позволяват на агента да се адаптира към променящи се условия.  
  *Как да избегнем:* Използване на адаптивни или контекстуални стратегии.
- **Неправилно обновяване на стойности:** Грешки при изчисляване на средните награди или Q-стойности могат да доведат до неправилно поведение.  
  *Как да избегнем:* Внимателно тестване и верификация на формулите.

## 7. Short Retrieval Quiz
1. Какво означава exploration в контекста на AI агенти?  
2. Каква е ролята на exploitation?  
3. Какво представлява epsilon-greedy стратегията?  
4. Защо е важен балансът между exploration и exploitation?  
5. Какво е multi-armed bandit problem?  
6. Как може да се намали exploration с времето?  
7. Коя е основната цел на reinforcement learning агента във връзка с exploration и exploitation?

## 8. Quick Recap
- Exploration означава опитване на нови действия за събиране на информация.  
- Exploitation използва вече научени знания за максимизиране на наградата.  
- Балансът между двете е ключов за ефективността на интелигентните системи.  
- Multi-armed bandit е класически модел за изучаване на тази дилема.  
- Epsilon-greedy е проста и популярна стратегия за управление на trade-off.  
- Прекомерното изследване или експлоатиране води до подоптимални резултати.  
- В reinforcement learning exploration е критично за откриване на най-добрите политики.

## 9. Spaced Review Plan

| Време след учене | Промпт за преговор                                  |
|-------------------|----------------------------------------------------|
| 1 ден             | Обяснете с ваши думи какво е exploration и exploitation. |
| 3 дни             | Опишете multi-armed bandit проблема и защо е важен.      |
| 1 седмица         | Сравнете epsilon-greedy и UCB стратегии.                  |
| 1 месец           | Дайте пример от reinforcement learning, където exploration е ключово. |