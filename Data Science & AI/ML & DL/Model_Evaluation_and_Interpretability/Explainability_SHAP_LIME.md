# Explainability_SHAP_LIME

## 1. Activate Prior Knowledge
- Какво означава "обяснимост" (explainability) в контекста на изкуствения интелект и защо е важна?
- Как бихте обяснили на неспециалист защо една AI система е взела конкретно решение?
- Какви методи или техники познавате за интерпретиране на модели в машинното обучение?

## 2. Overview
Обяснимостта на модели в машинното обучение и изкуствения интелект се отнася до способността да се разбере и интерпретира как и защо даден модел е взел определено решение. Това е особено важно в критични приложения като медицина, финанси и право, където решенията трябва да бъдат прозрачни и доверени.

SHAP (SHapley Additive exPlanations) и LIME (Local Interpretable Model-agnostic Explanations) са две водещи техники за обяснимост, които предоставят локални обяснения за отделни предсказания на сложни модели. Те помагат да се разбере влиянието на отделните входни характеристики върху резултата.

Тези методи се използват като част от процеса на верификация, отстраняване на грешки и подобряване на модела, както и за повишаване на доверието на потребителите и регулаторите в AI системите. Те са приложими към всякакъв тип модели, включително черни кутии като дълбоки невронни мрежи и ансамбли.

## 3. Key Concepts
- **Explainability (Обяснимост)** – Способността да се разбере и интерпретира поведението и решенията на AI модел. Може да се сравни с разказване на история за това как моделът е стигнал до даден извод.
- **Local Explanation (Локално обяснение)** – Обяснение, което се фокусира върху конкретно предсказание, а не върху поведението на целия модел. Представете си, че обяснявате защо един конкретен пациент е диагностициран по определен начин.
- **SHAP Values (SHAP стойности)** – Измерват приноса на всяка характеристика към предсказанието, базирано на теорията на игрите (Shapley values). Мислете за това като за справедливо разпределяне на "заслугите" между играчите.
- **LIME (Local Interpretable Model-agnostic Explanations)** – Метод, който създава прост локален модел (например линейна регресия) около точката на интерес, за да обясни сложното предсказание. Подобно на използване на увеличително стъкло, за да видите детайлите на конкретна част от картината.
- **Model-agnostic (Независим от модела)** – Подход, който може да се прилага към всякакъв тип модели, без да се изисква вътрешна информация за тяхната структура.
- **Feature Importance (Важност на характеристиките)** – Мярка за това колко всяка входна променлива влияе върху изхода на модела.

## 4. Step-by-step Learning Path
1. **Разберете нуждата от обяснимост в AI**
   - Фокус: Защо обяснимостта е критична в реални приложения.
   - Задача: Изследвайте казус, в който липсата на обяснимост е довела до проблем.
   - Въпроси: Какви рискове крие черната кутия? Кога обяснимостта е задължителна?

2. **Запознайте се с основите на SHAP и LIME**
   - Фокус: Теория и принципи на двата метода.
   - Задача: Прочетете и обобщете основните идеи зад Shapley values и локалните модели.
   - Въпроси: Как SHAP използва теорията на игрите? Как LIME създава локален модел?

3. **Практическа имплементация на LIME**
   - Фокус: Използване на LIME върху прост модел (напр. логистична регресия).
   - Задача: Създайте локално обяснение за конкретно предсказание с LIME (Python, sklearn + lime).
   - Въпроси: Какво показва локалният модел? Как се интерпретират резултатите?

4. **Практическа имплементация на SHAP**
   - Фокус: Изчисляване и визуализация на SHAP стойности.
   - Задача: Използвайте SHAP за обяснение на предсказания от дървесен ансамбъл (напр. XGBoost).
   - Въпроси: Какви са най-влиятелните характеристики? Как SHAP стойностите се визуализират?

5. **Сравнение и избор на подход**
   - Фокус: Кога да използваме SHAP или LIME.
   - Задача: Анализирайте предимствата и ограниченията на двата метода.
   - Въпроси: Кой метод е по-стабилен? Кой е по-интуитивен за крайния потребител?

## 5. Examples

### Пример 1: LIME за логистична регресия
```python
import lime
import lime.lime_tabular
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

data = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)
model = LogisticRegression(max_iter=1000).fit(X_train, y_train)

explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=data.feature_names, class_names=['malignant', 'benign'], discretize_continuous=True)
i = 0
exp = explainer.explain_instance(X_test[i], model.predict_proba, num_features=5)
exp.show_in_notebook(show_table=True)
```

### Пример 2: SHAP за XGBoost
```python
import xgboost
import shap
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

boston = load_boston()
X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=7)
model = xgboost.XGBRegressor().fit(X_train, y_train)

explainer = shap.Explainer(model)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, feature_names=boston.feature_names)
```

## 6. Common Pitfalls
- **Интерпретиране на обясненията като абсолютна истина** – SHAP и LIME дават приближени обяснения, не гарантират пълна точност.
- **Игнориране на локалния характер на обясненията** – Локалните методи обясняват само конкретни предсказания, не цялата логика на модела.
- **Използване без разбиране на модела** – Обясненията трябва да се разглеждат заедно с познания за модела и данните.
- **Прекомерна зависимост от визуализации без критичен анализ** – Визуализациите са полезни, но трябва да се комбинират с количествена оценка.
- **Пренебрегване на времето за изчисление** – SHAP може да бъде изчислително скъп, особено за големи модели и набори от данни.

## 7. Short Retrieval Quiz
1. Какво представлява локално обяснение?
2. Как SHAP стойностите разпределят влиянието на характеристиките?
3. Каква е основната идея зад LIME?
4. Защо обяснимостта е важна в AI системите?
5. Какво означава, че SHAP и LIME са model-agnostic?
6. Кой метод използва теорията на игрите?
7. Какви са рисковете при неправилна интерпретация на обясненията?

## 8. Quick Recap
- Обяснимостта помага да разберем решенията на AI модели, повишавайки доверие и прозрачност.
- SHAP и LIME са водещи техники за локални обяснения, приложими към всякакви модели.
- SHAP базира обясненията си на теорията на игрите, разпределяйки "заслугите" между характеристики.
- LIME създава прост локален модел, който приближено обяснява сложното предсказание.
- И двата метода имат предимства и ограничения, които трябва да се вземат предвид при избор.
- Практическата им употреба изисква разбиране на модела и контекста на данните.
- Внимателната интерпретация и критичният анализ са ключови за ефективна обяснимост.

## 9. Spaced Review Plan

| Време след учене | Промпт за преговор                                   |
|------------------|-----------------------------------------------------|
| 1 ден            | Обяснете с прости думи какво представляват SHAP и LIME. |
| 3 дни            | Опишете разликите между локални и глобални обяснения. |
| 1 седмица        | Демонстрирайте с пример как се използва LIME за конкретно предсказание. |
| 1 месец          | Анализирайте предимствата и ограниченията на SHAP спрямо LIME. |