# Calibration_and_uncertainty

## 1. Activate Prior Knowledge

- Какво разбирате под „калибриране“ в контекста на измервателни уреди или модели за машинно обучение?
- Защо е важно да знаем колко сме сигурни в резултатите на един AI модел или софтуерна система?
- Как бихте оценили надеждността на прогноза, ако нямате директен достъп до истинските стойности?

## 2. Overview

Калибрирането и неопределеността са фундаментални понятия в инженерството, статистиката и изкуствения интелект. Калибрирането се отнася до процеса на настройка на измервателни уреди или модели, така че техните изходи да отговарят точно на реалните стойности. В контекста на AI системи, калибрирането гарантира, че вероятностните прогнози са коректно съпоставими с действителните честоти на събитията.

Неопределеността описва степента на несигурност в измерванията или прогнозите. Тя е неизбежна поради ограничената информация, шум в данните и моделиращите предположения. Разбирането и количественото оценяване на неопределеността е ключово за вземане на информирани решения и за повишаване на надеждността на системите.

В по-широк контекст, калибрирането и управлението на неопределеността са критични за системи, които трябва да работят в реално време, да вземат решения с висока степен на доверие и да минимизират риска от грешки. Те са основа за валидиране, тестване и подобряване на модели и софтуер.

## 3. Key Concepts

- **Calibration (Калибриране)** – Процес на корекция на изхода на модел или уред, така че прогнозите или измерванията да съответстват на реалните стойности. Може да се сравни с настройването на часовник, който трябва да показва точното време.
- **Uncertainty (Неопределеност)** – Мярка за липсата на сигурност в резултатите, произтичаща от шум, непълнота на данни или моделиращи грешки. Представете си я като „мъглата“ около една прогноза.
- **Confidence Interval (Доверителен интервал)** – Интервал, в който с определена вероятност се намира истинската стойност на параметъра. Аналогично на зоната около стрелата, където тя вероятно ще падне.
- **Overfitting (Прекалено нагласяне)** – Когато моделът е твърде добре пригоден към тренировъчните данни, но не е калибриран за нови данни, което води до лоша неопределеност.
- **Reliability Diagram (Диаграма на надеждност)** – Графично средство за оценка на калибрирането на вероятностни прогнози, показващо съответствието между предсказаните вероятности и реалните честоти.
- **Bayesian Uncertainty (Байесова неопределеност)** – Оценка на неопределеността, базирана на вероятностна интерпретация на параметрите на модела, която позволява формално изразяване на несигурността.

## 4. Step-by-step Learning Path

1. **Фокус:** Основи на калибрирането  
   **Задача:** Изследвайте как работи калибрирането на прост измервателен уред (например термометър). Сравнете измерванията с известна стандартна стойност.  
   **Въпроси:** Какво означава, че уредът е „калибриран“? Какво се случва, ако не е?

2. **Фокус:** Калибриране на вероятностни модели  
   **Задача:** Използвайте Python и sklearn, за да обучите логистична регресия и визуализирайте reliability diagram.  
   **Въпроси:** Какво показва диаграмата? Как да разберете дали моделът е добре калибриран?

3. **Фокус:** Измерване и интерпретация на неопределеност  
   **Задача:** Изчислете доверителни интервали за средна стойност на набор от данни.  
   **Въпроси:** Какво означава доверителният интервал? Как се променя с размера на извадката?

4. **Фокус:** Управление на неопределеност в AI системи  
   **Задача:** Имплементирайте прост Bayesian Neural Network или използвайте dropout като аппроксимация за неопределеност.  
   **Въпроси:** Как неопределеността помага при вземането на решения? Кога е полезно да се отчита?

5. **Фокус:** Практически аспекти и избягване на грешки  
   **Задача:** Анализирайте резултатите от модел с прекалено нагласяне и калибрирайте с Platt scaling или isotonic regression.  
   **Въпроси:** Как калибрирането подобрява модела? Какво се случва, ако не се калибрира?

## 5. Examples

### Пример 1: Калибриране на логистична регресия с sklearn

```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt

X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
model = LogisticRegression()
model.fit(X, y)
prob_pos = model.predict_proba(X)[:, 1]

fraction_of_positives, mean_predicted_value = calibration_curve(y, prob_pos, n_bins=10)

plt.plot(mean_predicted_value, fraction_of_positives, "s-", label="Logistic Regression")
plt.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
plt.xlabel("Средно предсказана вероятност")
plt.ylabel("Дял положителни")
plt.legend()
plt.show()
```

### Пример 2: Изчисляване на доверителен интервал

```python
import numpy as np
from scipy import stats

data = np.random.normal(loc=5, scale=2, size=100)
mean = np.mean(data)
conf_int = stats.norm.interval(0.95, loc=mean, scale=stats.sem(data))

print(f"Средна стойност: {mean:.2f}")
print(f"95% доверителен интервал: {conf_int}")
```

### Пример 3: Dropout като аппроксимация за неопределеност в PyTorch

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 50)
        self.dropout = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(50, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        return torch.sigmoid(self.fc2(x))

model = SimpleNN()
model.train()  # Dropout активен

# Многократно предсказване за оценка на неопределеност
inputs = torch.randn(1, 10)
predictions = [model(inputs).item() for _ in range(100)]
mean_pred = sum(predictions) / len(predictions)
uncertainty = (sum((p - mean_pred) ** 2 for p in predictions) / len(predictions)) ** 0.5

print(f"Средна прогноза: {mean_pred:.3f}, Неопределеност: {uncertainty:.3f}")
```

## 6. Common Pitfalls

- **Игнориране на калибрирането:** Много модели имат добри точности, но лошо калибрирани вероятности, което води до грешни решения.
- **Прекалено нагласяне (overfitting):** Води до подценяване на неопределеността и лъжлива увереност в прогнозите.
- **Липса на количествена оценка на неопределеност:** Без ясна метрика, неопределеността остава субективна и трудно приложима.
- **Неправилно използване на доверителни интервали:** Смесване на интерпретации или прилагане при неподходящи разпределения.
- **Пренебрегване на контекста:** Калибрирането и неопределеността трябва да се разглеждат в контекста на конкретната задача и данни.

## 7. Short Retrieval Quiz

1. Какво означава калибриране на модел?
2. Каква е ролята на доверителния интервал?
3. Какво е прекалено нагласяне и как влияе на неопределеността?
4. Как може да се визуализира калибрирането на вероятностен модел?
5. Как dropout може да помогне за оценка на неопределеност?
6. Защо е важно да отчитаме неопределеност при вземане на решения?
7. Каква е разликата между калибриране и точност?

## 8. Quick Recap

- Калибрирането осигурява съответствие между прогнозите и реалните стойности.
- Неопределеността измерва колко сме сигурни в дадена прогноза или измерване.
- Доверителните интервали и reliability diagrams са основни инструменти за оценка.
- Прекаленото нагласяне води до лъжлива увереност и лошо калибриране.
- Управлението на неопределеност е ключово за надеждни AI системи.
- Практическите техники включват Platt scaling, isotonic regression и Bayesian подходи.
- Винаги анализирайте и валидирайте калибрирането и неопределеността в контекста на конкретната задача.

## 9. Spaced Review Plan

| Време след учене | Прегледен въпрос                                      |
|------------------|-------------------------------------------------------|
| 1 ден            | Какво представлява калибрирането и защо е важно?      |
| 3 дни            | Как се използва доверителен интервал за неопределеност? |
| 1 седмица        | Как да разпознаем и коригираме лошо калибриране?      |
| 1 месец          | Какво е значението на неопределеността в AI системите? |