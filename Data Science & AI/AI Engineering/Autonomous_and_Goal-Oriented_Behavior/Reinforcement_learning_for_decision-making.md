# Reinforcement_learning_for_decision-making

## 1. Activate Prior Knowledge
- Какво представлява вземането на решения в контекста на изкуствения интелект и как се различава от традиционното програмиране?
- Как бихте описали разликата между обучение с учител и обучение чрез подсилване?
- Можете ли да дадете пример за система, която се учи чрез взаимодействие с околната среда?

## 2. Overview
Обучението чрез подсилване (Reinforcement Learning, RL) е метод за машинно обучение, при който агентът се учи да взема решения, като взаимодейства с околната среда и получава обратна връзка под формата на награди или наказания. Целта е да се максимизира кумулативната награда в дългосрочен план, което прави RL особено подходящ за задачи, където решенията имат последици във времето.

В по-широк контекст RL се използва за изграждане на автономни системи, които могат да се адаптират и подобряват без изрично програмиране на всяка възможна ситуация. Това го прави ключов компонент в области като роботика, игри, управление на ресурси и препоръчителни системи.

RL е важен, защото предлага рамка за решаване на сложни проблеми, където традиционните алгоритми не могат лесно да приложат оптимални стратегии. Той съчетава теорията на решенията, динамичното програмиране и статистическото обучение, за да моделира и оптимизира поведението на агенти в динамични и несигурни среди.

## 3. Key Concepts
- **Agent** – Това е „учещият се“ или вземащият решения компонент, който взаимодейства с околната среда. Можем да го сравним с играч в игра, който избира ходове.
- **Environment (Околна среда)** – Всичко, с което агентът взаимодейства. Представете си го като игралното поле, където се случват събитията.
- **State (Състояние)** – Описва текущата ситуация на агента в околната среда. Аналогично на позицията на играча в шахматната игра.
- **Action (Действие)** – Изборът, който агентът прави, за да промени състоянието. Като ход в игра.
- **Reward (Награда)** – Обратната връзка, която агентът получава след действие, за да оцени колко добро е било то. Може да се мисли като точки в игра.
- **Policy (Политика)** – Стратегията, която агентът използва, за да избира действия на базата на текущото състояние.
- **Value Function (Функция на стойността)** – Оценка колко добра е дадена позиция (състояние) или действие, с оглед на бъдещите награди.
- **Exploration vs Exploitation (Изследване срещу експлоатация)** – Балансът между опитване на нови действия и използване на вече научени успешни стратегии.

## 4. Step-by-step Learning Path
1. **Основи на RL и динамично програмиране**  
   - Фокус: Разберете основните компоненти на RL и концепцията за Марковски процеси.  
   - Задача: Прочетете и обяснете на свой език какво е Markov Decision Process (MDP).  
   - Въпроси: Какво представлява MDP? Защо Markov свойството е важно?

2. **Изучаване на политики и функции на стойността**  
   - Фокус: Научете разликата между политика, функция на стойността и функция на действие-стойност.  
   - Задача: Имплементирайте проста таблица за Q-стойности за малък MDP.  
   - Въпроси: Какво е Q-стойност? Как тя помага при вземането на решения?

3. **Основни алгоритми: Q-Learning и SARSA**  
   - Фокус: Разберете как работят Q-Learning и SARSA, и кога се използват.  
   - Задача: Напишете код за Q-Learning агент, който решава малка среда (например Gridworld).  
   - Въпроси: Каква е разликата между Q-Learning и SARSA? Как се обновяват Q-стойностите?

4. **Изследване и експлоатация**  
   - Фокус: Изучете стратегии за баланс между изследване и експлоатация (ε-greedy, softmax).  
   - Задача: Добавете ε-greedy стратегия към вашия Q-Learning агент.  
   - Въпроси: Какво е ε-greedy? Защо е важно да има изследване?

5. **Дълбоко обучение и RL (Deep RL)**  
   - Фокус: Запознайте се с използването на невронни мрежи за апроксимация на функции в RL.  
   - Задача: Проучете и обяснете основите на Deep Q-Network (DQN).  
   - Въпроси: Как невронните мрежи помагат в RL? Какво е DQN?

## 5. Examples
### Пример 1: Q-Learning за Gridworld
```python
import numpy as np

# Дефиниране на средата
states = 5
actions = 2  # наляво, надясно
Q = np.zeros((states, actions))
alpha = 0.1
gamma = 0.9
epsilon = 0.1

def choose_action(state):
    if np.random.rand() < epsilon:
        return np.random.choice(actions)
    else:
        return np.argmax(Q[state])

for episode in range(100):
    state = 0
    done = False
    while not done:
        action = choose_action(state)
        next_state = state + (1 if action == 1 else -1)
        next_state = max(0, min(states-1, next_state))
        reward = 1 if next_state == states-1 else 0
        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
        state = next_state
        if state == states-1:
            done = True
```

### Пример 2: Баланс между изследване и експлоатация (ε-greedy)
```python
def epsilon_greedy(Q, state, epsilon):
    if np.random.rand() < epsilon:
        return np.random.choice(len(Q[state]))
    else:
        return np.argmax(Q[state])
```

### Пример 3: Deep Q-Network (DQN) – концептуален код (псевдо)
```python
# Използване на невронна мрежа за апроксимация на Q-функцията
model = NeuralNetwork(input_size=state_dim, output_size=action_dim)

def train_dqn(state, action, reward, next_state, done):
    target = reward
    if not done:
        target += gamma * np.max(model.predict(next_state))
    current_q = model.predict(state)
    current_q[action] = target
    model.train(state, current_q)
```

## 6. Common Pitfalls
- **Прекалено ранно експлоатиране** – Ако агентът не изследва достатъчно, може да се заклещи в локален оптимум. Използвайте стратегии като ε-greedy.  
- **Неправилно дефиниране на наградите** – Наградите трябва да насочват към желаното поведение; неправилни награди водят до нежелани резултати.  
- **Игнориране на дисконтовия фактор (γ)** – Той контролира колко важни са бъдещите награди; неправилна стойност може да направи обучението нестабилно.  
- **Прекалено сложни модели без достатъчно данни** – В Deep RL невронните мрежи могат да се пренаситят или да не се обучат добре без достатъчно взаимодействия.  
- **Липса на стабилност при обновяване на Q-стойности** – Необходимо е подходящо избиране на скорост на обучение (α) и техники за стабилизиране.

## 7. Short Retrieval Quiz
1. Какво е основната цел на обучението чрез подсилване?  
2. Какво представлява политика в RL?  
3. Каква е ролята на наградата?  
4. Какво означава балансът между изследване и експлоатация?  
5. Какво е Q-стойност?  
6. Коя е основната разлика между Q-Learning и SARSA?  
7. Защо се използват невронни мрежи в Deep RL?

## 8. Quick Recap
- RL учи агент да взема решения чрез взаимодействие и обратна връзка от околната среда.  
- Основни компоненти са агент, среда, състояния, действия, награди и политика.  
- Целта е максимизиране на кумулативната награда чрез оптимална политика.  
- Q-Learning и SARSA са базови алгоритми за обучение на политика.  
- Балансът между изследване и експлоатация е ключов за ефективно обучение.  
- Deep RL използва невронни мрежи за апроксимация на функции в сложни среди.  
- Внимание към дефинирането на награди и параметрите на обучение е критично.

## 9. Spaced Review Plan

| Време след учене | Промпт за преговор                                         |
|------------------|------------------------------------------------------------|
| 1 ден            | Обяснете основните компоненти на RL и ролята на наградата. |
| 3 дни            | Опишете разликите между Q-Learning и SARSA.                |
| 1 седмица        | Какво е ε-greedy стратегия и защо е важна?                 |
| 1 месец          | Прегледайте как Deep RL разширява класическото RL.         |