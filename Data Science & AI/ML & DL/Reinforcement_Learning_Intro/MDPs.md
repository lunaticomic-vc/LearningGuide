# MDPs

## 1. Activate Prior Knowledge
- Какво представлява процесът на вземане на решения в условия на несигурност и как бихте го моделирали?
- Кои са основните компоненти на система за автоматично управление или интелигентен агент?
- Какво знаете за вероятностите и тяхната роля в моделирането на динамични системи?

## 2. Overview
Марковските процеси на вземане на решения (Markov Decision Processes, MDPs) са математически рамки за моделиране на ситуации, в които агент трябва да взема поредица от решения, като всяко решение влияе върху бъдещото състояние и възнаграждение. Те са фундаментални за области като изкуствения интелект, роботиката и оптимизацията на софтуерни системи.

MDP описва среда чрез състояния, действия и вероятности за преход между състоянията, както и функция за възнаграждение, която насърчава агента да избира определени действия. Този модел позволява формулиране и решаване на задачи за оптимално планиране и учене, особено когато средата е непълно предвидима.

В контекста на AI, MDPs са основата на методи за обучение с подсилване (reinforcement learning), където агентът се учи да максимизира дългосрочното си възнаграждение чрез взаимодействие със средата. Те са също така полезни в софтуерното инженерство за моделиране на системи с вероятностни състояния и решения.

## 3. Key Concepts
- **State (Състояние)** – Описва текущата ситуация или конфигурация на системата. Мислете за него като за „момента в играта“, в който се намирате.
- **Action (Действие)** – Изборът, който агентът може да направи в дадено състояние. Аналогично на ход в шахматна партия.
- **Transition Probability (Вероятност за преход)** – Вероятността да се премине от едно състояние в друго след изпълнението на действие. Това е като вероятността да хвърлите зар и да получите определен резултат.
- **Reward (Възнаграждение)** – Числова стойност, която агентът получава след извършване на действие в дадено състояние. Това е „точките“, които печелите за добър ход.
- **Policy (Политика)** – Правило или функция, която определя кое действие да се избере в дадено състояние. Може да се мисли като стратегия за игра.
- **Value Function (Функция на стойността)** – Оценка на очакваното бъдещо възнаграждение, започвайки от дадено състояние и следвайки определена политика.
- **Markov Property (Марковско свойство)** – Условие, че бъдещето зависи само от настоящето, а не от миналото. Представете си, че всичко важно е „записано“ в текущото състояние.

## 4. Step-by-step Learning Path
1. **Запознайте се с основните компоненти на MDP**
   - Фокус: Разберете какво са състояния, действия, преходи и възнаграждения.
   - Задача: Опишете MDP за прост проблем – например, навигация в лабиринт.
   - Въпроси: Какво представлява състоянието? Каква е ролята на действието?

2. **Изучете Марковското свойство и неговото значение**
   - Фокус: Защо бъдещето зависи само от настоящето?
   - Задача: Докажете с пример защо информацията от миналото не влияе на следващото състояние.
   - Въпроси: Какво е Марковско свойство? Какво би се случило, ако то не важи?

3. **Разберете политиките и функцията на стойността**
   - Фокус: Как да оценим и изберем най-добрата политика?
   - Задача: Изчислете стойността на няколко състояния при фиксирана политика.
   - Въпроси: Какво измерва функцията на стойността? Какво е политика?

4. **Практикувайте алгоритми за решаване на MDP**
   - Фокус: Научете алгоритми като value iteration и policy iteration.
   - Задача: Имплементирайте value iteration за малък MDP.
   - Въпроси: Как работи value iteration? Каква е разликата между value и policy iteration?

5. **Приложете MDP в реален софтуерен проект**
   - Фокус: Интегрирайте MDP за вземане на решения в симулация или игра.
   - Задача: Създайте агент, който използва MDP за навигация или оптимизация.
   - Въпроси: Как MDP подобри поведението на агента? Как се справя с несигурността?

## 5. Examples

### Пример 1: Навигация в лабиринт
Агентът се намира в клетка на мрежа и може да се движи нагоре, надолу, наляво или надясно. Всяко движение има вероятност да се провали и агентът да остане на място. Целта е да достигне изхода с минимален брой ходове.

```python
states = [(x, y) for x in range(5) for y in range(5)]
actions = ['up', 'down', 'left', 'right']
transition_prob = 0.8  # 80% успех при движение
reward = -1  # наказание за всяко движение
goal_state = (4, 4)
```

### Пример 2: Управление на енергия в умна сграда
Система избира кога да включи или изключи отоплението, за да минимизира разходите и поддържа комфортна температура. Състоянията са температурни диапазони, действията – включване/изключване.

### Пример 3: Обучение с подсилване (Reinforcement Learning)
MDP се използва за формализиране на средата, в която агентът учи чрез проба-грешка, като максимизира кумулативното възнаграждение.

## 6. Common Pitfalls
- **Игнориране на Марковското свойство** – Опит да се моделира зависимост от цялата история, което усложнява решението.
- **Неправилно дефиниране на състояния** – Прекалено общи или твърде специфични състояния водят до лошо обучение.
- **Пренебрегване на вероятностите за преход** – Третиране на системата като детерминистична, когато тя не е.
- **Лошо дефинирани функции за възнаграждение** – Възнагражденията не отразяват целите на агента, което води до нежелано поведение.
- **Прекомерно усложняване на политиката** – Използване на твърде сложни модели без нужда, което затруднява обучението и интерпретацията.

## 7. Short Retrieval Quiz
1. Какво представлява състоянието в MDP?
2. Каква е ролята на функцията за възнаграждение?
3. Какво означава Марковско свойство?
4. Каква е разликата между политика и функция на стойността?
5. Какво прави алгоритъмът value iteration?
6. Защо е важно да се дефинират правилно преходните вероятности?
7. Как MDP се използва в reinforcement learning?

## 8. Quick Recap
- MDP е модел за вземане на решения в условия на несигурност, базиран на състояния, действия, преходи и възнаграждения.
- Марковското свойство гарантира, че бъдещето зависи само от настоящето състояние.
- Политиката определя какви действия да се предприемат в дадено състояние.
- Функцията на стойността оценява очакваното бъдещо възнаграждение.
- Алгоритмите value iteration и policy iteration помагат за намиране на оптимални политики.
- Правилното дефиниране на състояния и възнаграждения е ключово за успешното моделиране.
- MDPs са основа за обучение с подсилване и много приложения в AI и софтуерното инженерство.

## 9. Spaced Review Plan

| Време след учене | Прегледен въпрос                                   |
|------------------|---------------------------------------------------|
| 1 ден            | Какво е Марковско свойство и защо е важно?        |
| 3 дни            | Опишете основните компоненти на MDP.              |
| 1 седмица        | Как работи алгоритъмът value iteration?           |
| 1 месец          | Как MDP се прилага в reinforcement learning?      |