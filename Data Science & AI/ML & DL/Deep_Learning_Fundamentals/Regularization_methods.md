# Regularization_methods

## 1. Activate Prior Knowledge
- Какво представлява пренасищането (overfitting) в контекста на машинното обучение и защо е проблем?
- Какви техники знаете, които помагат за подобряване на обобщаващата способност на моделите?
- Как бихте обяснили връзката между сложността на модела и качеството на неговото предсказване?

## 2. Overview
Регуляризацията е набор от техники, използвани за предотвратяване на пренасищането на модели в машинното обучение. Когато моделът се научи твърде добре на тренировъчните данни, той губи способността си да обобщава върху нови, невиждани данни. Регуляризацията въвежда допълнителна информация или ограничения, които "наказват" прекомерната сложност на модела.

Тези методи са ключови за изграждането на надеждни и стабилни AI системи, особено при работа с големи и шумни набори от данни. В софтуерното инженерство регуляризацията се прилага като част от процеса на оптимизация и валидация, за да се гарантира, че моделите не само пасват добре на тренировъчните примери, но и са устойчиви на вариации.

Регуляризацията може да се разглежда като баланс между bias (пристрастие) и variance (вариация) – тя помага да се намери "златната среда", където моделът е достатъчно гъвкав, но не прекалено сложен.

## 3. Key Concepts
- **Overfitting (Пренасищане)** – Ситуация, при която моделът се научава твърде детайлно на тренировъчните данни, включително шум и случайни вариации, което намалява неговата способност да предсказва нови данни. Аналогия: учиш се наизуст на отговорите, без да разбираш материала.
- **Underfitting (Недообучаване)** – Когато моделът е твърде прост и не успява да улови важните зависимости в данните. Аналогия: опитваш се да решиш сложна задача с твърде опростени инструменти.
- **L1 Regularization (Lasso)** – Добавя към функцията на загуба сумата от абсолютните стойности на параметрите, което води до изчистване на някои от тях до нула и създава по-опростен модел.
- **L2 Regularization (Ridge)** – Добавя към функцията на загуба сумата от квадратите на параметрите, което кара параметрите да бъдат по-малки, но не ги занулява напълно.
- **Dropout** – Техника, при която по време на обучението случайно се "изключват" някои неврони, за да се предотврати прекомерна зависимост между тях.
- **Early Stopping** – Спиране на обучението преди пълното минимизиране на грешката върху тренировъчните данни, когато грешката върху валидационния набор започне да се увеличава.
- **Bias-Variance Tradeoff** – Балансът между склонността на модела да прави системни грешки (bias) и чувствителността му към вариации в тренировъчните данни (variance).

## 4. Step-by-step Learning Path
1. **Фокус:** Разбиране на проблема с пренасищането и недообучаването.  
   **Практическа задача:** Обучете прост линейнен модел върху малък набор от данни и визуализирайте резултатите.  
   **Въпроси:** Как разпознавате пренасищане? Какво се случва при недообучаване?

2. **Фокус:** Изучаване на L1 и L2 регуляризация.  
   **Практическа задача:** Имплементирайте L1 и L2 регуляризация върху линейна регресия и сравнете резултатите.  
   **Въпроси:** Как L1 и L2 влияят на параметрите? Кога бихте предпочели L1 пред L2?

3. **Фокус:** Запознаване с Dropout и Early Stopping в невронни мрежи.  
   **Практическа задача:** Добавете Dropout към проста невронна мрежа и приложете Early Stopping при обучението.  
   **Въпроси:** Как Dropout намалява пренасищането? Какво е ролята на Early Stopping?

4. **Фокус:** Анализ на bias-variance tradeoff.  
   **Практическа задача:** Изследвайте как промяната на сложността на модела влияе на bias и variance чрез експерименти с различни архитектури.  
   **Въпроси:** Как се променят bias и variance при увеличаване на сложността? Как регуляризацията влияе на този баланс?

5. **Фокус:** Приложение на регуляризация в реални проекти.  
   **Практическа задача:** Изберете реален набор от данни и приложете различни регуляризационни техники, за да оптимизирате модела.  
   **Въпроси:** Коя техника даде най-добър резултат и защо? Какви компромиси направихте?

## 5. Examples
### Пример 1: L2 регуляризация върху линейна регресия (Python, scikit-learn)
```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston

data = load_boston()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)

model = Ridge(alpha=1.0)  # L2 регуляризация с параметър alpha
model.fit(X_train, y_train)
print("R^2 на теста:", model.score(X_test, y_test))
```

### Пример 2: Dropout в Keras
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential([
    Dense(64, activation='relu', input_shape=(100,)),
    Dropout(0.5),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')
```

### Пример 3: Early Stopping с Keras
```python
from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='val_loss', patience=5)
model.fit(X_train, y_train, validation_split=0.2, epochs=100, callbacks=[early_stop])
```

## 6. Common Pitfalls
- **Прекомерна регуляризация:** Прекаляването с регуляризация води до недообучаване и лоши резултати. Винаги тествайте различни стойности на параметрите.  
- **Игнориране на валидационен набор:** Без отделен валидационен набор не може да се прецени ефективността на регуляризацията.  
- **Неправилно прилагане на Dropout:** Прилагането на Dropout по време на тест или използване на твърде висока стойност може да влоши резултатите.  
- **Прекалено ранно спиране:** Early Stopping, ако се приложи твърде рано, може да не позволи на модела да достигне оптимума.  
- **Липса на разбиране за bias-variance tradeoff:** Регуляризацията не е универсално решение – трябва да се разбере кога и как да се използва.

## 7. Short Retrieval Quiz
1. Какво е основната цел на регуляризацията?  
2. Как се различават L1 и L2 регуляризацията?  
3. Какво представлява Dropout и как помага срещу пренасищане?  
4. Какво е bias-variance tradeoff?  
5. Защо е важно да имаме валидационен набор при прилагане на регуляризация?  
6. Какво може да се случи, ако прекалим с регуляризацията?  
7. Как Early Stopping подобрява обучението на модел?

## 8. Quick Recap
- Регуляризацията предотвратява пренасищането, като ограничава сложността на модела.  
- L1 регуляризация кара някои параметри да станат нула, създавайки по-опростен модел.  
- L2 регуляризация намалява стойностите на параметрите, без да ги занулява.  
- Dropout случайно изключва неврони по време на обучение, за да намали зависимостите.  
- Early Stopping спира обучението, когато валидационната грешка започне да се увеличава.  
- Балансът между bias и variance е ключов за доброто обобщение на модела.  
- Винаги използвайте валидационен набор, за да настроите параметрите на регуляризацията.

## 9. Spaced Review Plan

| Време след учене | Промпт за преглед                                   |
|------------------|----------------------------------------------------|
| 1 ден            | Обяснете с прости думи какво е регуляризация.      |
| 3 дни            | Сравнете L1 и L2 регуляризация с примери.          |
| 1 седмица        | Опишете как Dropout и Early Stopping помагат срещу пренасищане. |
| 1 месец          | Прегледайте bias-variance tradeoff и приложението на регуляризация в реален проект. |