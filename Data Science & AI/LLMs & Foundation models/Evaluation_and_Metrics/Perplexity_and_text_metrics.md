# Perplexity_and_text_metrics

## 1. Activate Prior Knowledge
- Какво представлява вероятността и как се използва в модели за обработка на естествен език?
- Как бихте оценили качеството на текст, генериран от езиков модел?
- Какви метрики познавате, които измерват колко "добър" е един текст спрямо даден езиков модел?

## 2. Overview
Перплексията (Perplexity) е ключова метрика за оценка на езикови модели, особено в контекста на вероятностни модели и невронни мрежи за обработка на естествен език (NLP). Тя измерва колко добре моделът предсказва даден текст, като по-ниска стойност означава по-добро съвпадение между модела и реалните данни.

В по-широк контекст, перплексията служи като индикатор за качеството на езиковия модел и помага при избора и оптимизацията му в системи като машинен превод, автоматично генериране на текст и разпознаване на реч. Тя е част от група текстови метрики, които позволяват количествена оценка на модели и алгоритми.

Разбирането на перплексията и други текстови метрики е фундаментално за професионалисти в AI и софтуерното инженерство, тъй като те осигуряват обективни критерии за подобряване на модели и системи, които работят с естествен език.

## 3. Key Concepts
- **Perplexity (Перплексия)** – Мярка за това колко "изненадан" е езиковият модел при среща с нов текст. Може да се разглежда като степен на несигурност или объркване на модела. По-ниска перплексия означава по-добро предсказване.
- **Language Model (Езиков модел)** – Модел, който оценява вероятността на последователност от думи. Аналогия: езиковият модел е като предсказател на следващата дума в изречение.
- **Cross-Entropy (Крос-ентропия)** – Мярка за разликата между истинското разпределение на данните и това, което моделът предсказва. Перплексията е експоненциална функция на крос-ентропията.
- **N-gram Model (N-грам модел)** – Прост езиков модел, който предсказва дума въз основа на предходните N-1 думи. Ментален модел: като да се опитваш да предскажеш следващата дума, гледайки само последните няколко.
- **Text Metrics (Текстови метрики)** – Комплект от количествени показатели, които измерват качеството, разнообразието и точността на текст, генериран или анализиран от модели.

## 4. Step-by-step Learning Path
1. **Фокус:** Разберете дефиницията и математическата формула на перплексията.  
   **Задача:** Изчислете перплексията на прост езиков модел върху малък текстов корпус.  
   **Въпроси:** Какво означава по-ниска перплексия? Как се свързва с вероятността на текста?

2. **Фокус:** Изучете връзката между крос-ентропия и перплексия.  
   **Задача:** Напишете кратък скрипт, който изчислява крос-ентропия и перплексия за даден текст и модел.  
   **Въпроси:** Каква е ролята на крос-ентропията при обучението на езикови модели?

3. **Фокус:** Анализирайте перплексията при различни типове езикови модели (n-грам, RNN, Transformer).  
   **Задача:** Сравнете перплексията на два различни езикови модела върху един и същ тестов набор.  
   **Въпроси:** Как архитектурата на модела влияе на перплексията?

4. **Фокус:** Изследвайте други текстови метрики (BLEU, ROUGE, METEOR) и тяхната роля.  
   **Задача:** Измерете и сравнете перплексия и BLEU за генериран текст от езиков модел.  
   **Въпроси:** Кога перплексията е по-подходяща от BLEU и обратното?

5. **Фокус:** Прилагане на перплексия в реални AI системи.  
   **Задача:** Използвайте перплексия за избор на най-добър модел в проект за машинен превод.  
   **Въпроси:** Как перплексията влияе на избора на модел в продукционна среда?

## 5. Examples
**Пример 1: Изчисляване на перплексия за прост n-грам модел (Python)**  
```python
import math

def perplexity(probabilities):
    N = len(probabilities)
    cross_entropy = -sum(math.log2(p) for p in probabilities) / N
    return 2 ** cross_entropy

# Примерни вероятности на думи в изречение
probs = [0.1, 0.05, 0.2, 0.15, 0.5]
print(f"Perplexity: {perplexity(probs):.2f}")
```

**Пример 2: Сравнение на перплексия между два езикови модела**  
- Модел A: перплексия 30  
- Модел B: перплексия 45  
Извод: Модел A по-добре предсказва тестовия текст.

**Пример 3: Перплексия и крос-ентропия**  
Ако крос-ентропията е 2.0 (бита на дума), тогава перплексията е 2^2 = 4, което означава, че моделът има средна несигурност равна на избор между 4 възможности за всяка дума.

## 6. Common Pitfalls
- **Интерпретиране на висока перплексия като "лошо" без контекст** – Високата перплексия може да е резултат от сложен или неочакван текст, а не непременно от лош модел.
- **Сравняване на перплексия между различни корпуси** – Перплексията е чувствителна към домейна и дължината на текста, затова сравненията трябва да са на еднакви данни.
- **Игнориране на други метрики** – Перплексията не измерва семантичната или стилистичната точност, затова трябва да се комбинира с други оценки.
- **Грешки при изчисляване на вероятности** – Неправилно нормализирани вероятности водят до грешни стойности на перплексия.

## 7. Short Retrieval Quiz
1. Какво измерва перплексията в езиков модел?  
2. Каква е връзката между крос-ентропия и перплексия?  
3. Защо по-ниската перплексия е желателна?  
4. Какво представлява n-грам моделът?  
5. Кога не е подходящо да сравняваме перплексия между два модела?  
6. Какво означава перплексия от 1?  
7. Кои други текстови метрики могат да се използват за оценка на модели?

## 8. Quick Recap
- Перплексията измерва колко добре езиковият модел предсказва текст, като по-ниска стойност е по-добра.  
- Тя е експоненциална функция на крос-ентропията, която измерва средната логаритмична загуба.  
- Перплексията е полезна при сравняване и оптимизиране на езикови модели, но трябва да се използва внимателно.  
- N-грам моделите са основен пример за езикови модели, при които перплексията може лесно да се изчисли.  
- Перплексията не измерва семантичната точност, затова се комбинира с други метрики като BLEU и ROUGE.  
- Високата перплексия не винаги означава лош модел – контекстът и данните са важни.  
- Практическото изчисляване на перплексия изисква правилно изчислени вероятности на думите.

## 9. Spaced Review Plan

| Време       | Промпт за преговор                                  |
|-------------|----------------------------------------------------|
| 1 ден       | Какво е перплексия и как се изчислява?             |
| 3 дни       | Обяснете връзката между перплексия и крос-ентропия.|
| 1 седмица   | Кога и защо използваме перплексия при оценка на модели?|
| 1 месец     | Сравнете перплексия с други текстови метрики и тяхното приложение.|