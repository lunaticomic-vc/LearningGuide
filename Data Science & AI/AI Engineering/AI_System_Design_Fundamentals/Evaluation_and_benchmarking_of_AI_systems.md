# Evaluation_and_benchmarking_of_AI_systems

## 1. Activate Prior Knowledge
- Какво означава „оценка“ на софтуерна система и как тя се различава от „бенчмаркинг“?
- Кои метрики бихте използвали, за да измерите ефективността на AI модел?
- Как оценката на AI системи влияе върху избора и внедряването им в реални приложения?

## 2. Overview
Оценката и бенчмаркингът на AI системи са критични процеси, които позволяват да се измери и сравни качеството и производителността на различни модели и алгоритми. Те осигуряват обективна основа за вземане на решения при избора на най-подходящия AI инструмент за конкретна задача.

В контекста на софтуерното инженерство, оценката се интегрира в цикъла на разработка, като помага да се открият слабости, да се подобри точността и да се гарантира надеждността на системата. Бенчмаркингът, от своя страна, поставя системите в конкурентна среда, позволявайки сравнение чрез стандартизирани тестове и метрики.

Този процес е особено важен, защото AI системите често работят с големи обеми данни и вземат решения, които имат реално въздействие върху потребителите и бизнеса. Без систематична оценка, рискуваме да внедрим модели с непредвидими или нежелани резултати.

## 3. Key Concepts
- **Evaluation Metrics** – Количествени показатели, които измерват качеството на AI модела, като точност, прецизност, припомняне и F1-скор. Аналогия: това са „оценките“ на модела, подобно на оценките в училище.
- **Benchmarking** – Процес на сравняване на AI системи спрямо стандартизирани тестове или други модели, за да се определи кой е по-добър в определена задача. Може да се разглежда като състезание между модели.
- **Overfitting** – Ситуация, в която моделът се представя отлично на тренировъчните данни, но зле на нови, невиждани данни. Моделът „запомня“ вместо да „учи“.
- **Cross-validation** – Техника за оценка, която разделя данните на няколко части, за да се тества моделът върху различни подмножества и да се избегне overfitting.
- **Baseline Model** – Прост модел, който служи като отправна точка за сравнение. Ако новият модел не превъзхожда baseline, той вероятно не е полезен.
- **Latency and Throughput** – Времето за отговор на AI системата и броят на обработените заявки за единица време. Важни показатели за производителността на системата.

## 4. Step-by-step Learning Path
1. **Фокус:** Разберете основните метрики за оценка на AI модели.  
   **Задача:** Изберете публичен набор от данни и изчислете точност, прецизност и припомняне за прост класификатор.  
   **Въпроси:** Какво измерва прецизността? Защо припомнянето е важно?

2. **Фокус:** Научете се да прилагате cross-validation.  
   **Задача:** Реализирайте k-fold cross-validation върху същия набор от данни и сравнете резултатите с еднократна оценка.  
   **Въпроси:** Как cross-validation помага за избягване на overfitting? Какво е k в k-fold?

3. **Фокус:** Изследвайте benchmarking с публични AI платформи.  
   **Задача:** Изберете два модела от платформа като Hugging Face и ги сравнете по стандартен benchmark.  
   **Въпроси:** Какви са предимствата на стандартизираните benchmarks? Какво показва по-високият резултат?

4. **Фокус:** Анализирайте производителността – latency и throughput.  
   **Задача:** Измерете времето за отговор на AI модел при различни натоварвания.  
   **Въпроси:** Как latency влияе върху потребителското изживяване? Какво е trade-off между latency и throughput?

5. **Фокус:** Практикувайте интерпретируемост и обяснимост на резултатите.  
   **Задача:** Използвайте SHAP или LIME, за да обясните решенията на AI модел.  
   **Въпроси:** Защо е важно да разбираме как моделът взема решения? Как това помага при оценката?

## 5. Examples
### Пример 1: Оценка на класификатор за спам имейли
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score
y_true = [0, 1, 0, 1, 0]
y_pred = [0, 1, 0, 0, 0]

accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)

print(f"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}")
```

### Пример 2: Cross-validation с sklearn
```python
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

X = np.random.rand(100, 10)
y = np.random.randint(0, 2, 100)
model = RandomForestClassifier()

scores = cross_val_score(model, X, y, cv=5)
print(f"Cross-validation scores: {scores}")
print(f"Average score: {scores.mean()}")
```

### Пример 3: Benchmarking на NLP модели с Hugging Face
```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("This is a great example of benchmarking!")
print(result)
```

## 6. Common Pitfalls
- **Използване на неподходящи метрики** – Например, точността може да е подвеждаща при несбалансирани класове. Използвайте метрики, подходящи за задачата.
- **Overfitting при оценка** – Оценявайте модела на отделен тестов набор, различен от тренировъчния.
- **Игнориране на производителността** – Високата точност не означава, че моделът е бърз или мащабируем.
- **Липса на стандартизиран benchmark** – Без сравнение с други модели, трудно се преценява реалната стойност на системата.
- **Пренебрегване на интерпретируемостта** – Черните кутии могат да доведат до недоверие и грешки в реални приложения.

## 7. Short Retrieval Quiz
1. Каква е разликата между оценка (evaluation) и бенчмаркинг (benchmarking)?
2. Коя метрика е по-подходяща при силно несбалансирани класове – точност или припомняне?
3. Какво представлява overfitting и как може да се избегне?
4. Какво е cross-validation и защо е полезно?
5. Защо е важно да измерваме latency и throughput на AI системите?
6. Какво е baseline модел и каква е неговата роля?
7. Какви са рисковете при използване само на една метрика за оценка?

## 8. Quick Recap
- Оценката и бенчмаркингът са ключови за измерване и сравнение на AI системи.
- Метриките трябва да се избират спрямо задачата и данните.
- Cross-validation помага да се избегне overfitting и да се получи по-реалистична оценка.
- Benchmarking осигурява стандартизирана среда за сравнение на модели.
- Производителността (latency и throughput) е важна за реални приложения.
- Интерпретируемостта повишава доверието и качеството на AI системите.
- Често срещани грешки могат да бъдат избегнати с правилна методология и внимание.

## 9. Spaced Review Plan

| Време след изучаване | Промпт за преглед                                  |
|----------------------|---------------------------------------------------|
| 1 ден                | Какви са основните метрики за оценка на AI модели?|
| 3 дни                | Обяснете ролята на cross-validation и benchmarking.|
| 1 седмица            | Какво е overfitting и как се предотвратява?       |
| 1 месец              | Кои са ключовите аспекти при оценка на производителността на AI системи? |